{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Translation Project\n",
    "\n",
    "\n",
    "The goal of the project is to compare the strength of the following recurrent models:\n",
    "\n",
    "1. Embedded GRU\n",
    "2. Embedded Bidirectional GRU\n",
    "3. Embedded GRU encoder-decoder model\n",
    "4. Embedded GRU encoder-decoder model with Multiplicative Attention\n",
    "\n",
    "The models implemented in Tensorflow 2.0 with Keras as a high-level API. Models are trained and analyzed based on [TedHrlrTranslate dataset](https://www.tensorflow.org/datasets/datasets#ted_hrlr_translate)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow==2.0.0-alpha0 in /Users/aponamarev/env/translation-tf2/lib/python3.6/site-packages (2.0.0a0)\n",
      "Requirement already satisfied: google-pasta>=0.1.2 in /Users/aponamarev/env/translation-tf2/lib/python3.6/site-packages (from tensorflow==2.0.0-alpha0) (0.1.5)\n",
      "Requirement already satisfied: gast>=0.2.0 in /Users/aponamarev/env/translation-tf2/lib/python3.6/site-packages (from tensorflow==2.0.0-alpha0) (0.2.2)\n",
      "Requirement already satisfied: astor>=0.6.0 in /Users/aponamarev/env/translation-tf2/lib/python3.6/site-packages (from tensorflow==2.0.0-alpha0) (0.7.1)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /Users/aponamarev/env/translation-tf2/lib/python3.6/site-packages (from tensorflow==2.0.0-alpha0) (1.0.9)\n",
      "Requirement already satisfied: numpy<2.0,>=1.14.5 in /Users/aponamarev/env/translation-tf2/lib/python3.6/site-packages (from tensorflow==2.0.0-alpha0) (1.16.3)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /Users/aponamarev/env/translation-tf2/lib/python3.6/site-packages (from tensorflow==2.0.0-alpha0) (0.7.1)\n",
      "Requirement already satisfied: tf-estimator-nightly<1.14.0.dev2019030116,>=1.14.0.dev2019030115 in /Users/aponamarev/env/translation-tf2/lib/python3.6/site-packages (from tensorflow==2.0.0-alpha0) (1.14.0.dev2019030115)\n",
      "Requirement already satisfied: wheel>=0.26 in /Users/aponamarev/env/translation-tf2/lib/python3.6/site-packages (from tensorflow==2.0.0-alpha0) (0.33.1)\n",
      "Requirement already satisfied: tb-nightly<1.14.0a20190302,>=1.14.0a20190301 in /Users/aponamarev/env/translation-tf2/lib/python3.6/site-packages (from tensorflow==2.0.0-alpha0) (1.14.0a20190301)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /Users/aponamarev/env/translation-tf2/lib/python3.6/site-packages (from tensorflow==2.0.0-alpha0) (1.0.7)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in /Users/aponamarev/env/translation-tf2/lib/python3.6/site-packages (from tensorflow==2.0.0-alpha0) (3.7.1)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Users/aponamarev/env/translation-tf2/lib/python3.6/site-packages (from tensorflow==2.0.0-alpha0) (1.1.0)\n",
      "Requirement already satisfied: six>=1.10.0 in /Users/aponamarev/env/translation-tf2/lib/python3.6/site-packages (from tensorflow==2.0.0-alpha0) (1.12.0)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /Users/aponamarev/env/translation-tf2/lib/python3.6/site-packages (from tensorflow==2.0.0-alpha0) (1.20.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /Users/aponamarev/env/translation-tf2/lib/python3.6/site-packages (from tb-nightly<1.14.0a20190302,>=1.14.0a20190301->tensorflow==2.0.0-alpha0) (0.15.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/aponamarev/env/translation-tf2/lib/python3.6/site-packages (from tb-nightly<1.14.0a20190302,>=1.14.0a20190301->tensorflow==2.0.0-alpha0) (3.1)\n",
      "Requirement already satisfied: h5py in /Users/aponamarev/env/translation-tf2/lib/python3.6/site-packages (from keras-applications>=1.0.6->tensorflow==2.0.0-alpha0) (2.9.0)\n",
      "Requirement already satisfied: setuptools in /Users/aponamarev/env/translation-tf2/lib/python3.6/site-packages (from protobuf>=3.6.1->tensorflow==2.0.0-alpha0) (40.6.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow==2.0.0-alpha0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import re\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data ETL\n",
    "\n",
    "The data load, extraction, and transformation is done with data_etl() method. This method returns a dictionary containing source data stored under 'x' label. Target data is stored under 'y' label. In addition to the source and target data, the dictionary contains x and y tockenizers (stored as 'x_tk' and 'y_tk'):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_etl(download_dir: str = \".\", file_name: str = \"en_ru.tgz\", n_lines: int = 3000,\n",
    "             lenght_lim_lower: int = 4, length_lim_upper: int = 500, num_words: int = 5000) -> dict:\n",
    "    path = os.path.join(download_dir, file_name)\n",
    "\n",
    "    print(\"Start data ETL\")\n",
    "\n",
    "    if os.path.isfile(path):\n",
    "        print(\"Reuse pre-downloaded \" + path)\n",
    "    else:\n",
    "        # download a dataset\n",
    "        print(\"Start downloading\")\n",
    "        subprocess.run(\n",
    "                [\"curl\", \"--output\",\n",
    "                 \"https://s3.amazonaws.com/web-language-models/paracrawl/release1/paracrawl-release1.en-ru.zipporah0-dedup-clean.tgz\",\n",
    "                 path]\n",
    "        )\n",
    "        # extract\n",
    "        subprocess.run(\"tar --extract --file\".split() + [path])\n",
    "        print(\"Data downloaded and extracted\")\n",
    "\n",
    "    # read data\n",
    "    en = read_lines(\"paracrawl-release1.en-ru.zipporah0-dedup-clean.en\", n_lines)\n",
    "    ru = read_lines(\"paracrawl-release1.en-ru.zipporah0-dedup-clean.ru\", n_lines)\n",
    "\n",
    "    print(\"Extracted successfully\")\n",
    "\n",
    "    # filter out small samples\n",
    "    indices = list(filter(lambda idx: length_lim_upper > len(en[idx].split()) >=lenght_lim_lower, range(len(en))))\n",
    "    en = [en[idx] for idx in indices]\n",
    "    ru = [ru[idx] for idx in indices]\n",
    "\n",
    "    # Tockenize\n",
    "    en = [re.sub(\"[0-9]\", \" \\g<0>\", s) for s in en]\n",
    "    ru = [re.sub(\"[0-9]\", \" \\g<0>\", s) for s in ru]\n",
    "\n",
    "    x, x_tk = tokenize(en, num_words=num_words, filters_regex=None)\n",
    "    y, y_tk = tokenize(ru, num_words=num_words * 3, filters_regex=None)\n",
    "\n",
    "    x = pad(x)\n",
    "    y = pad(y)\n",
    "\n",
    "    print(\"Transformed successfully\")\n",
    "\n",
    "    return {'x': x, 'y': y, 'x_tk': x_tk, 'y_tk': y_tk}\n",
    "\n",
    "\n",
    "def read_lines(from_file: str, n_lines: int) -> list:\n",
    "    if not os.path.isfile(from_file):\n",
    "        raise FileNotFoundError(from_file)\n",
    "\n",
    "    lines: list = []\n",
    "    with open(from_file, \"r\") as f:\n",
    "        for idx, line in enumerate(f):\n",
    "            if idx < n_lines:\n",
    "                lines.append(line)\n",
    "            else:\n",
    "                break\n",
    "\n",
    "    return lines\n",
    "\n",
    "\n",
    "def tokenize(x: list, num_words: int = 5000, filters_regex: str = None):\n",
    "    \"\"\"\n",
    "    Tokenize x\n",
    "    :param x: List of sentences/strings to be tokenized\n",
    "    :param num_words: maximum number of words in the vocabulary - rare words will be replaced with a [rare] token\n",
    "    :param filters_regex: regular expression filter string such as [^a-zA-Z0-9/-]\n",
    "    :return: Tuple of (tokenized x data, tokenizer used to tokenize x)\n",
    "    \"\"\"\n",
    "    if filters_regex is not None:\n",
    "        x_tk = keras.preprocessing.text.Tokenizer(num_words=num_words, filters=filters_regex, oov_token=\"[rare]\")\n",
    "    else:\n",
    "        x_tk = keras.preprocessing.text.Tokenizer(num_words=num_words, oov_token=\"[rare]\")\n",
    "\n",
    "    x_tk.fit_on_texts(x)\n",
    "    return x_tk.texts_to_sequences(x), x_tk\n",
    "\n",
    "\n",
    "def pad(x, length=None) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Pad x\n",
    "    :param x: List of sequences.\n",
    "    :param length: Length to pad the sequence to.  If None, use length of longest sequence in x.\n",
    "    :return: Padded numpy array of sequences\n",
    "    \"\"\"\n",
    "    if length is None:\n",
    "        length = max([len(sentence) for sentence in x])\n",
    "\n",
    "    return keras.preprocessing.sequence.pad_sequences(x, maxlen=length, padding='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dataset = data_etl()\n",
    "\n",
    "{\n",
    "    'x': np.ndarray,\n",
    "    'y': np.ndarray,\n",
    "    'x_tk': keras.preprocessing.text.Tokenizer,\n",
    "    'y_tk': keras.preprocessing.text.Tokenizer\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start data ETL\n",
      "Reuse pre-downloaded ./en_ru.tgz\n",
      "Extracted successfully\n",
      "Transformed successfully\n"
     ]
    }
   ],
   "source": [
    "dataset = data_etl(download_dir=\".\", n_lines=5000, num_words=3000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions\n",
    "\n",
    "In addition to the data ETL, the code below provides two additional functions for converting logits into word indicies and converting word indicies into text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logits_to_id(logits):\n",
    "    \"\"\"\n",
    "    Turns logits into word ids\n",
    "    :param logits: Logits from a neural network\n",
    "    \"\"\"\n",
    "    return [prediction for prediction in np.argmax(logits, 1)]\n",
    "\n",
    "def id_to_text(idx, tokenizer):\n",
    "    \"\"\"\n",
    "    Turns id into text using the tokenizer\n",
    "    :param idx: word id\n",
    "    :param tokenizer: Keras Tokenizer fit on the labels\n",
    "    :return: String that represents the text of the logits\n",
    "    \"\"\"\n",
    "    index_to_words = {id: word for word, id in tokenizer.word_index.items()}\n",
    "    index_to_words[0] = '<PAD>'\n",
    "\n",
    "    return ' '.join([index_to_words[prediction] for prediction in idx]).replace(\" <PAD>\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is an example for a samples number 1:\n",
      "Source('en') example: the company has been registered with the [rare] court in prague in section b file 1 4 8 5 7\n",
      "Target('ru') example: фирма зарегистрирована в городском суде в г праге раздел б вкладыш 1 4 8 5 7\n",
      " \n",
      "Samples number 2:\n",
      "Source('en') example: since january 2 0 0 9 new office premises in the street v [rare] 3 prague 1 have been opened these premises are situated in a close [rare] of all our partner banks\n",
      "Target('ru') example: с января месяца 2 0 0 9 г открыты новые офисные помещения на улице v kolkovně 3 в колковне д 3 прага 1 данные помещения находятся поблизости всех банков партнеров\n",
      "source vocabulary size: 2999\n",
      "target vocabulary size: 8999\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'[rare]': 1,\n",
       " 'в': 2,\n",
       " 'и': 3,\n",
       " '1': 4,\n",
       " '0': 5,\n",
       " '2': 6,\n",
       " 'на': 7,\n",
       " 'с': 8,\n",
       " '3': 9,\n",
       " '5': 10,\n",
       " '9': 11,\n",
       " '4': 12,\n",
       " 'для': 13,\n",
       " '7': 14,\n",
       " 'не': 15,\n",
       " '8': 16,\n",
       " 'по': 17,\n",
       " '6': 18,\n",
       " 'или': 19,\n",
       " 'что': 20,\n",
       " 'the': 21,\n",
       " 'о': 22,\n",
       " 'мы': 23,\n",
       " 'от': 24,\n",
       " 'из': 25,\n",
       " 'как': 26,\n",
       " 'к': 27,\n",
       " 'во': 28,\n",
       " '–': 29,\n",
       " 'также': 30,\n",
       " 'а': 31,\n",
       " 'радио': 32,\n",
       " 'все': 33,\n",
       " 'г': 34,\n",
       " 'за': 35,\n",
       " 'франции': 36,\n",
       " 'до': 37,\n",
       " 'это': 38,\n",
       " 'при': 39,\n",
       " 'вы': 40,\n",
       " 'года': 41,\n",
       " 'его': 42,\n",
       " 'документов': 43,\n",
       " 'будет': 44,\n",
       " 'он': 45,\n",
       " 'http': 46,\n",
       " 'of': 47,\n",
       " 'чтобы': 48,\n",
       " 'так': 49,\n",
       " 'время': 50,\n",
       " 'визы': 51,\n",
       " 'то': 52,\n",
       " 'начало': 53,\n",
       " 'того': 54,\n",
       " 'внимание': 55,\n",
       " 'нам': 56,\n",
       " 'документы': 57,\n",
       " 'быть': 58,\n",
       " 'которые': 59,\n",
       " 'но': 60,\n",
       " 'лет': 61,\n",
       " 'если': 62,\n",
       " 'нас': 63,\n",
       " 'более': 64,\n",
       " 'является': 65,\n",
       " 'пожалуйста': 66,\n",
       " 'to': 67,\n",
       " 'всех': 68,\n",
       " 'со': 69,\n",
       " 'должны': 70,\n",
       " 'this': 71,\n",
       " 'визу': 72,\n",
       " 'после': 73,\n",
       " 'конец': 74,\n",
       " 'центр': 75,\n",
       " 'архиепископии': 76,\n",
       " 'онлайн': 77,\n",
       " 'работы': 78,\n",
       " 'on': 79,\n",
       " 'только': 80,\n",
       " 'их': 81,\n",
       " 'году': 82,\n",
       " 'июня': 83,\n",
       " 'and': 84,\n",
       " 'может': 85,\n",
       " 'вам': 86,\n",
       " 'обратите': 87,\n",
       " 'подачи': 88,\n",
       " 'они': 89,\n",
       " 'можно': 90,\n",
       " 'всем': 91,\n",
       " 'где': 92,\n",
       " 'у': 93,\n",
       " 'случае': 94,\n",
       " 'not': 95,\n",
       " 'жизни': 96,\n",
       " 'ru': 97,\n",
       " 'сентября': 98,\n",
       " 'info': 99,\n",
       " 'визовый': 100,\n",
       " 'a': 101,\n",
       " 'искусства': 102,\n",
       " 'переводом': 103,\n",
       " 'данных': 104,\n",
       " 'который': 105,\n",
       " 'этого': 106,\n",
       " 'родителей': 107,\n",
       " 'server': 108,\n",
       " 'php': 109,\n",
       " 'есть': 110,\n",
       " 'found': 111,\n",
       " 'можете': 112,\n",
       " 'я': 113,\n",
       " 'будут': 114,\n",
       " 'октября': 115,\n",
       " 'кроме': 116,\n",
       " 'requested': 117,\n",
       " 'url': 118,\n",
       " 'was': 119,\n",
       " 'вас': 120,\n",
       " 'all': 121,\n",
       " 'необходимо': 122,\n",
       " 'же': 123,\n",
       " 'был': 124,\n",
       " 'слушать': 125,\n",
       " 'москва': 126,\n",
       " 'церкви': 127,\n",
       " 'когда': 128,\n",
       " 'forum': 129,\n",
       " 'паспорт': 130,\n",
       " 'этот': 131,\n",
       " 'через': 132,\n",
       " 'здесь': 133,\n",
       " 'уже': 134,\n",
       " 'всего': 135,\n",
       " 'know': 136,\n",
       " 'м': 137,\n",
       " 'мире': 138,\n",
       " 'центре': 139,\n",
       " 'add': 140,\n",
       " 'французский': 141,\n",
       " 'других': 142,\n",
       " 'этой': 143,\n",
       " 'we': 144,\n",
       " 'roerich': 145,\n",
       " 'that’s': 146,\n",
       " 'заявителей': 147,\n",
       " 'могут': 148,\n",
       " 'без': 149,\n",
       " 'было': 150,\n",
       " 'страниц': 151,\n",
       " 'feedurl': 152,\n",
       " 'external': 153,\n",
       " 'lastpost': 154,\n",
       " 'forumids': 155,\n",
       " 'mir': 156,\n",
       " 'svadbi': 157,\n",
       " 'нашей': 158,\n",
       " 'были': 159,\n",
       " 'день': 160,\n",
       " 'записи': 161,\n",
       " 'копия': 162,\n",
       " 'английский': 163,\n",
       " 'данные': 164,\n",
       " 'проект': 165,\n",
       " 'мира': 166,\n",
       " 'жизнь': 167,\n",
       " 'лично': 168,\n",
       " 'газа': 169,\n",
       " 'ее': 170,\n",
       " '»': 171,\n",
       " 'несовершеннолетних': 172,\n",
       " 'центра': 173,\n",
       " 'июля': 174,\n",
       " 'посольство': 175,\n",
       " 'заявители': 176,\n",
       " 'визового': 177,\n",
       " 'должен': 178,\n",
       " 'св': 179,\n",
       " 'data': 180,\n",
       " 'языки': 181,\n",
       " 'она': 182,\n",
       " 'вместе': 183,\n",
       " 'нужно': 184,\n",
       " 'in': 185,\n",
       " 'москве': 186,\n",
       " 'анкеты': 187,\n",
       " 'проекта': 188,\n",
       " 'нами': 189,\n",
       " 'которая': 190,\n",
       " 'еще': 191,\n",
       " 'апреля': 192,\n",
       " 'информации': 193,\n",
       " 'свидетельство': 194,\n",
       " 'об': 195,\n",
       " 'предоставить': 196,\n",
       " 'этом': 197,\n",
       " 'возможность': 198,\n",
       " 'нет': 199,\n",
       " 'дней': 200,\n",
       " 'целью': 201,\n",
       " 'подавать': 202,\n",
       " 'справка': 203,\n",
       " 'вфс': 204,\n",
       " 'глобал': 205,\n",
       " 'области': 206,\n",
       " 'наших': 207,\n",
       " 'города': 208,\n",
       " 'посетителей': 209,\n",
       " 'под': 210,\n",
       " 'количество': 211,\n",
       " 'паспорта': 212,\n",
       " 'информацию': 213,\n",
       " 'наши': 214,\n",
       " 'совет': 215,\n",
       " 'смерти': 216,\n",
       " 'языке': 217,\n",
       " 'id': 218,\n",
       " 'минимум': 219,\n",
       " 'том': 220,\n",
       " 'мая': 221,\n",
       " 'событие': 222,\n",
       " 'продолжительность': 223,\n",
       " 'заявитель': 224,\n",
       " 'французском': 225,\n",
       " 'чтв': 226,\n",
       " 'фестиваль': 227,\n",
       " 'которых': 228,\n",
       " 'себя': 229,\n",
       " 'рамках': 230,\n",
       " 'свидетельства': 231,\n",
       " 'ни': 232,\n",
       " 'любви': 233,\n",
       " 'свои': 234,\n",
       " 'radio': 235,\n",
       " 'визовом': 236,\n",
       " 'паспортов': 237,\n",
       " 'персональных': 238,\n",
       " 'между': 239,\n",
       " 'месяцев': 240,\n",
       " 'украине': 241,\n",
       " 'им': 242,\n",
       " '«': 243,\n",
       " 'ли': 244,\n",
       " 'земли': 245,\n",
       " 'тел': 246,\n",
       " 'доставки': 247,\n",
       " 'виза': 248,\n",
       " 'перевод': 249,\n",
       " 'птн': 250,\n",
       " 'имеет': 251,\n",
       " 'один': 252,\n",
       " 'назад': 253,\n",
       " 'своей': 254,\n",
       " 'была': 255,\n",
       " 'да': 256,\n",
       " 'февраля': 257,\n",
       " 'информация': 258,\n",
       " 'которой': 259,\n",
       " 'больше': 260,\n",
       " 'августа': 261,\n",
       " 'нефти': 262,\n",
       " 'заявления': 263,\n",
       " 'предварительной': 264,\n",
       " 'современного': 265,\n",
       " 'австрийского': 266,\n",
       " 'австрии': 267,\n",
       " 'стороны': 268,\n",
       " 'получить': 269,\n",
       " 'com': 270,\n",
       " 'год': 271,\n",
       " '—': 272,\n",
       " 'b': 273,\n",
       " 'угля': 274,\n",
       " 'анкета': 275,\n",
       " 'далее': 276,\n",
       " 'участие': 277,\n",
       " 'своих': 278,\n",
       " 'всеми': 279,\n",
       " 'несколько': 280,\n",
       " 'россии': 281,\n",
       " 'открытие': 282,\n",
       " 'for': 283,\n",
       " 'соли': 284,\n",
       " 'руды': 285,\n",
       " 'by': 286,\n",
       " 'рождении': 287,\n",
       " 'вск': 288,\n",
       " 'января': 289,\n",
       " 'v': 290,\n",
       " 'услуги': 291,\n",
       " 'истории': 292,\n",
       " 'стран': 293,\n",
       " 'другие': 294,\n",
       " 'природы': 295,\n",
       " 'дня': 296,\n",
       " 'котором': 297,\n",
       " 'нашего': 298,\n",
       " 'каждый': 299,\n",
       " 'места': 300,\n",
       " 'детей': 301,\n",
       " 'site': 302,\n",
       " 'санкт': 303,\n",
       " 'посольства': 304,\n",
       " 'английском': 305,\n",
       " 'rent': 306,\n",
       " 'float': 307,\n",
       " 'одним': 308,\n",
       " 'начиная': 309,\n",
       " 'период': 310,\n",
       " 'мне': 311,\n",
       " 'тем': 312,\n",
       " 'христа': 313,\n",
       " 'ноября': 314,\n",
       " 'миром': 315,\n",
       " 'родился': 316,\n",
       " 'выставки': 317,\n",
       " 'всему': 318,\n",
       " 'номер': 319,\n",
       " '°с': 320,\n",
       " 'заявителем': 321,\n",
       " 'курьерской': 322,\n",
       " 'долгосрочную': 323,\n",
       " 'которое': 324,\n",
       " 'вашего': 325,\n",
       " 'пребывания': 326,\n",
       " 'поездки': 327,\n",
       " 'странах': 328,\n",
       " 'господь': 329,\n",
       " 'жить': 330,\n",
       " 'христос': 331,\n",
       " 'гавриил': 332,\n",
       " 'т': 333,\n",
       " 'дереву': 334,\n",
       " 'культуры': 335,\n",
       " 'получения': 336,\n",
       " 'веб': 337,\n",
       " 'соглашение': 338,\n",
       " 'ребенка': 339,\n",
       " 'виз': 340,\n",
       " 'колл': 341,\n",
       " 'краткосрочную': 342,\n",
       " 'перед': 343,\n",
       " 'московской': 344,\n",
       " 'согласно': 345,\n",
       " 'европе': 346,\n",
       " 'чем': 347,\n",
       " 'архиепископа': 348,\n",
       " 'марта': 349,\n",
       " 'братья': 350,\n",
       " 'раз': 351,\n",
       " 'очень': 352,\n",
       " 'место': 353,\n",
       " 'поддержке': 354,\n",
       " 'адрес': 355,\n",
       " 'дети': 356,\n",
       " 'непосредственно': 357,\n",
       " 'песка': 358,\n",
       " 'слушателей': 359,\n",
       " 'средняя': 360,\n",
       " 'прослушивания': 361,\n",
       " 'получение': 362,\n",
       " 'отв': 363,\n",
       " 'копии': 364,\n",
       " 'вис': 365,\n",
       " 'австрийской': 366,\n",
       " 'различных': 367,\n",
       " 'эта': 368,\n",
       " 'совета': 369,\n",
       " 'вселенского': 370,\n",
       " 'гавриила': 371,\n",
       " 'be': 372,\n",
       " 'грунта': 373,\n",
       " 'гравия': 374,\n",
       " 'камня': 375,\n",
       " 'or': 376,\n",
       " 'подать': 377,\n",
       " 'письмо': 378,\n",
       " 'форме': 379,\n",
       " 'браке': 380,\n",
       " 'пустых': 381,\n",
       " 'указанием': 382,\n",
       " 'нотариально': 383,\n",
       " 'ул': 384,\n",
       " 'собой': 385,\n",
       " 'компании': 386,\n",
       " 'например': 387,\n",
       " 'образом': 388,\n",
       " 'условия': 389,\n",
       " 'видом': 390,\n",
       " 'кто': 391,\n",
       " 'великобритании': 392,\n",
       " 'патриархата': 393,\n",
       " 'соответствии': 394,\n",
       " 'господа': 395,\n",
       " 'заседание': 396,\n",
       " 'собора': 397,\n",
       " 'должна': 398,\n",
       " 'членов': 399,\n",
       " 'www': 400,\n",
       " 'cookies': 401,\n",
       " 'пара': 402,\n",
       " 'категории': 403,\n",
       " 'обработку': 404,\n",
       " 'подаче': 405,\n",
       " 'сбт': 406,\n",
       " 'срд': 407,\n",
       " 'наш': 408,\n",
       " 'эти': 409,\n",
       " 'над': 410,\n",
       " 'сегодня': 411,\n",
       " 'одна': 412,\n",
       " 'около': 413,\n",
       " 'последние': 414,\n",
       " 'х': 415,\n",
       " 'русской': 416,\n",
       " 'любовь': 417,\n",
       " 'поэтому': 418,\n",
       " 'надо': 419,\n",
       " 'православных': 420,\n",
       " 'мир': 421,\n",
       " 'однако': 422,\n",
       " 'press': 423,\n",
       " 'срок': 424,\n",
       " 'два': 425,\n",
       " 'предприятия': 426,\n",
       " 'тип': 427,\n",
       " 'заявителя': 428,\n",
       " 'биометрических': 429,\n",
       " 'копиями': 430,\n",
       " 'разрешение': 431,\n",
       " 'разных': 432,\n",
       " 'организации': 433,\n",
       " 'всегда': 434,\n",
       " 'человек': 435,\n",
       " 'свою': 436,\n",
       " 'права': 437,\n",
       " 'сестры': 438,\n",
       " 'которую': 439,\n",
       " 'своего': 440,\n",
       " 'вопросы': 441,\n",
       " 'члены': 442,\n",
       " 'i': 443,\n",
       " 'группы': 444,\n",
       " 'работает': 445,\n",
       " 'хранения': 446,\n",
       " 'имеют': 447,\n",
       " 'заявителю': 448,\n",
       " 'шенгенской': 449,\n",
       " 'французского': 450,\n",
       " 'gekakonus': 451,\n",
       " 'данный': 452,\n",
       " 'связи': 453,\n",
       " 'одной': 454,\n",
       " 'времени': 455,\n",
       " 'тех': 456,\n",
       " 'иисуса': 457,\n",
       " 'менее': 458,\n",
       " 'эту': 459,\n",
       " 'самым': 460,\n",
       " 'декабря': 461,\n",
       " 'течение': 462,\n",
       " 'миру': 463,\n",
       " 'действия': 464,\n",
       " 'прекратить': 465,\n",
       " 'международный': 466,\n",
       " 'семьи': 467,\n",
       " 'петербург': 468,\n",
       " 'выбор': 469,\n",
       " 'воды': 470,\n",
       " 'is': 471,\n",
       " 'user': 472,\n",
       " 'заявителям': 473,\n",
       " 'биометрические': 474,\n",
       " 'выезд': 475,\n",
       " 'национальный': 476,\n",
       " 'апостилем': 477,\n",
       " 'культурного': 478,\n",
       " 'понтонных': 479,\n",
       " 'проектов': 480,\n",
       " 'благодаря': 481,\n",
       " 'этому': 482,\n",
       " 'номера': 483,\n",
       " 'другими': 484,\n",
       " 'составляет': 485,\n",
       " 'западной': 486,\n",
       " 'этих': 487,\n",
       " 'париже': 488,\n",
       " 'смерть': 489,\n",
       " 'первое': 490,\n",
       " 'людей': 491,\n",
       " 'разрешения': 492,\n",
       " 'навсегда': 493,\n",
       " 'либо': 494,\n",
       " 'копией': 495,\n",
       " 'n': 496,\n",
       " 'выставках': 497,\n",
       " 'художников': 498,\n",
       " 'добычу': 499,\n",
       " 'матушки': 500,\n",
       " 'земля': 501,\n",
       " 'страницы': 502,\n",
       " 'university': 503,\n",
       " 'гражданина': 504,\n",
       " 'шенгенскую': 505,\n",
       " 'оригинал': 506,\n",
       " 'доказательства': 507,\n",
       " 'граждане': 508,\n",
       " 'свяжитесь': 509,\n",
       " 'ucm': 510,\n",
       " 'gmbh': 511,\n",
       " 'предлагает': 512,\n",
       " 'стоит': 513,\n",
       " 'доступ': 514,\n",
       " 'представляет': 515,\n",
       " 'ваше': 516,\n",
       " 'самых': 517,\n",
       " 'три': 518,\n",
       " 'воспользоваться': 519,\n",
       " 'союза': 520,\n",
       " 'деятельности': 521,\n",
       " 'православной': 522,\n",
       " 'александра': 523,\n",
       " 'собрание': 524,\n",
       " 'христе': 525,\n",
       " 'этим': 526,\n",
       " 'воскресение': 527,\n",
       " 'тогда': 528,\n",
       " 'являются': 529,\n",
       " 'владыка': 530,\n",
       " 'свой': 531,\n",
       " 'новых': 532,\n",
       " 'стал': 533,\n",
       " 'de': 534,\n",
       " 'литвы': 535,\n",
       " 'киев': 536,\n",
       " 'языка': 537,\n",
       " 'двух': 538,\n",
       " 'возвращения': 539,\n",
       " 'никто': 540,\n",
       " 'поздно': 541,\n",
       " 'воде': 542,\n",
       " 'p': 543,\n",
       " 'airlines': 544,\n",
       " 'другое': 545,\n",
       " 'service': 546,\n",
       " 'граждан': 547,\n",
       " 'подачу': 548,\n",
       " 'долгосрочная': 549,\n",
       " 'подтверждение': 550,\n",
       " 'отдел': 551,\n",
       " 'втр': 552,\n",
       " 'форума': 553,\n",
       " 'музыкальных': 554,\n",
       " 'плавучих': 555,\n",
       " 'новости': 556,\n",
       " 'именно': 557,\n",
       " 'евро': 558,\n",
       " 'таким': 559,\n",
       " 'всей': 560,\n",
       " 'данной': 561,\n",
       " 'даже': 562,\n",
       " 'отдыха': 563,\n",
       " 'интернет': 564,\n",
       " 'протяжении': 565,\n",
       " 'находится': 566,\n",
       " 'н': 567,\n",
       " 'своим': 568,\n",
       " 'себе': 569,\n",
       " 'полностью': 570,\n",
       " 'нового': 571,\n",
       " 'наше': 572,\n",
       " 'см': 573,\n",
       " 'получении': 574,\n",
       " 'дорогие': 575,\n",
       " 'среди': 576,\n",
       " 'председательством': 577,\n",
       " 'территории': 578,\n",
       " 'ознакомиться': 579,\n",
       " 'божественной': 580,\n",
       " 'такие': 581,\n",
       " 'сделать': 582,\n",
       " 'вопросам': 583,\n",
       " 'решение': 584,\n",
       " 'книги': 585,\n",
       " 'найдете': 586,\n",
       " 'дерева': 587,\n",
       " 'р': 588,\n",
       " 'представить': 589,\n",
       " 'многое': 590,\n",
       " 'таких': 591,\n",
       " 'лица': 592,\n",
       " 'миль': 593,\n",
       " 'законам': 594,\n",
       " 'fm': 595,\n",
       " 'анкету': 596,\n",
       " 'принадлежит': 597,\n",
       " 'датой': 598,\n",
       " 'фильмов': 599,\n",
       " 'удовольствием': 600,\n",
       " 'свое': 601,\n",
       " 'лишь': 602,\n",
       " 'включая': 603,\n",
       " 'городе': 604,\n",
       " 'различные': 605,\n",
       " 'любой': 606,\n",
       " 'экзархата': 607,\n",
       " '№': 608,\n",
       " 'список': 609,\n",
       " 'ибо': 610,\n",
       " 'человека': 611,\n",
       " 'ему': 612,\n",
       " 'следующие': 613,\n",
       " 'имя': 614,\n",
       " 'го': 615,\n",
       " 'новый': 616,\n",
       " 'ко': 617,\n",
       " 'необходимости': 618,\n",
       " 'часто': 619,\n",
       " 'заявление': 620,\n",
       " 'первых': 621,\n",
       " 'аэропорт': 622,\n",
       " 'часть': 623,\n",
       " 'дома': 624,\n",
       " 'учился': 625,\n",
       " 'латвии': 626,\n",
       " 'производство': 627,\n",
       " 'выставка': 628,\n",
       " 'имени': 629,\n",
       " 'месяц': 630,\n",
       " 'услуг': 631,\n",
       " 'бронирования': 632,\n",
       " 'наличии': 633,\n",
       " 'нажмите': 634,\n",
       " 'with': 635,\n",
       " 'are': 636,\n",
       " 'качества': 637,\n",
       " 'наличие': 638,\n",
       " 'жительство': 639,\n",
       " 'дату': 640,\n",
       " 'ти': 641,\n",
       " 'действительный': 642,\n",
       " 'direccte': 643,\n",
       " 'использование': 644,\n",
       " 'вене': 645,\n",
       " 'биеннале': 646,\n",
       " '®': 647,\n",
       " 'давление': 648,\n",
       " 'понтонные': 649,\n",
       " 'д': 650,\n",
       " 'говорит': 651,\n",
       " 'отель': 652,\n",
       " 'проведения': 653,\n",
       " 'новой': 654,\n",
       " 'найти': 655,\n",
       " 'начал': 656,\n",
       " 'института': 657,\n",
       " 'собственности': 658,\n",
       " 'церковью': 659,\n",
       " 'приходов': 660,\n",
       " 'друг': 661,\n",
       " 'париж': 662,\n",
       " 'святого': 663,\n",
       " 'невского': 664,\n",
       " 'воскресения': 665,\n",
       " 'вами': 666,\n",
       " 'нем': 667,\n",
       " 'прихода': 668,\n",
       " 'ставит': 669,\n",
       " 'собор': 670,\n",
       " 'программа': 671,\n",
       " 'сбор': 672,\n",
       " 'право': 673,\n",
       " 'французской': 674,\n",
       " 'постоянно': 675,\n",
       " 'приглашение': 676,\n",
       " 'море': 677,\n",
       " 'рокишкис': 678,\n",
       " 'скульптуры': 679,\n",
       " 'австрия': 680,\n",
       " 'работ': 681,\n",
       " 'произведения': 682,\n",
       " 'зао': 683,\n",
       " 'л': 684,\n",
       " 'парк': 685,\n",
       " 'стоимость': 686,\n",
       " 'страны': 687,\n",
       " 'услуга': 688,\n",
       " 'online': 689,\n",
       " 'одного': 690,\n",
       " 'пространства': 691,\n",
       " 'information': 692,\n",
       " 'its': 693,\n",
       " 'украину': 694,\n",
       " 'позвонив': 695,\n",
       " 'подача': 696,\n",
       " 'электронный': 697,\n",
       " 'почте': 698,\n",
       " 'ответственности': 699,\n",
       " 'свободной': 700,\n",
       " 'странице': 701,\n",
       " 'плавучие': 702,\n",
       " 'jetfloat®': 703,\n",
       " 'опыт': 704,\n",
       " 'messinian': 705,\n",
       " 'bay': 706,\n",
       " 'hotel': 707,\n",
       " 'нашем': 708,\n",
       " 'путь': 709,\n",
       " 'него': 710,\n",
       " 'местной': 711,\n",
       " 'которого': 712,\n",
       " 'продукции': 713,\n",
       " 'программы': 714,\n",
       " 'некоторые': 715,\n",
       " 'управления': 716,\n",
       " 'момент': 717,\n",
       " 'патриархом': 718,\n",
       " 'следует': 719,\n",
       " 'соборе': 720,\n",
       " 'церковь': 721,\n",
       " 'бы': 722,\n",
       " 'тому': 723,\n",
       " 'бога': 724,\n",
       " 'отца': 725,\n",
       " 'помощи': 726,\n",
       " 'встречи': 727,\n",
       " 'церквей': 728,\n",
       " 'должно': 729,\n",
       " 'пятницу': 730,\n",
       " 'образования': 731,\n",
       " 'ним': 732,\n",
       " 'такой': 733,\n",
       " 'литургии': 734,\n",
       " 'праздник': 735,\n",
       " 'каждому': 736,\n",
       " 'создания': 737,\n",
       " 'совершенно': 738,\n",
       " 'фонда': 739,\n",
       " 'украины': 740,\n",
       " 'киеве': 741,\n",
       " 'решения': 742,\n",
       " 'пустые': 743,\n",
       " 'обязательно': 744,\n",
       " 'русский': 745,\n",
       " 'землю': 746,\n",
       " 'сотрудников': 747,\n",
       " 'оплаты': 748,\n",
       " 'пока': 749,\n",
       " 'зависимости': 750,\n",
       " 'производства': 751,\n",
       " 'наиболее': 752,\n",
       " 'семья': 753,\n",
       " 'должности': 754,\n",
       " 'билеты': 755,\n",
       " 'content': 756,\n",
       " 'download': 757,\n",
       " 'живое': 758,\n",
       " 'последствия': 759,\n",
       " 'лицо': 760,\n",
       " 'хит': 761,\n",
       " 'ua': 762,\n",
       " 'as': 763,\n",
       " 'google': 764,\n",
       " 'температуре': 765,\n",
       " 'приходить': 766,\n",
       " 'долгосрочных': 767,\n",
       " 'имеющие': 768,\n",
       " 'типу': 769,\n",
       " 'лиц': 770,\n",
       " 'страховка': 771,\n",
       " 'студентов': 772,\n",
       " 'предназначен': 773,\n",
       " 'пальцев': 774,\n",
       " 'гражданства': 775,\n",
       " 'совместный': 776,\n",
       " 'музыки': 777,\n",
       " 'венский': 778,\n",
       " 'немецкого': 779,\n",
       " 'кино': 780,\n",
       " 'эксплуатации': 781,\n",
       " 'месяца': 782,\n",
       " 'новые': 783,\n",
       " 'качестве': 784,\n",
       " 'регистрации': 785,\n",
       " 'затем': 786,\n",
       " 'имеется': 787,\n",
       " 'части': 788,\n",
       " 'кондиционером': 789,\n",
       " 'безопасности': 790,\n",
       " 'сможете': 791,\n",
       " 'км': 792,\n",
       " 'такси': 793,\n",
       " 'периода': 794,\n",
       " 'университета': 795,\n",
       " 'город': 796,\n",
       " 'возможно': 797,\n",
       " 'прежде': 798,\n",
       " 'университете': 799,\n",
       " 'международных': 800,\n",
       " 'нашим': 801,\n",
       " 'сможем': 802,\n",
       " 'радость': 803,\n",
       " 'много': 804,\n",
       " 'архиепископ': 805,\n",
       " 'московского': 806,\n",
       " 'бог': 807,\n",
       " 'крест': 808,\n",
       " 'друга': 809,\n",
       " 'будем': 810,\n",
       " 'сами': 811,\n",
       " 'которым': 812,\n",
       " 'традиции': 813,\n",
       " 'создает': 814,\n",
       " 'каждой': 815,\n",
       " 'европы': 816,\n",
       " 'проблемы': 817,\n",
       " 'них': 818,\n",
       " 'членом': 819,\n",
       " 'призваны': 820,\n",
       " 'п': 821,\n",
       " 'ч': 822,\n",
       " 'сша': 823,\n",
       " 'средств': 824,\n",
       " 'своем': 825,\n",
       " 'занятости': 826,\n",
       " 'относительно': 827,\n",
       " 'первой': 828,\n",
       " 'понедельника': 829,\n",
       " 'вопросов': 830,\n",
       " 'работу': 831,\n",
       " 'природу': 832,\n",
       " 'окружающей': 833,\n",
       " 'will': 834,\n",
       " 'класса': 835,\n",
       " 'ваши': 836,\n",
       " 'you': 837,\n",
       " 'can': 838,\n",
       " 'народного': 839,\n",
       " 'музей': 840,\n",
       " 'даты': 841,\n",
       " 'музея': 842,\n",
       " 'it': 843,\n",
       " 'time': 844,\n",
       " 'воду': 845,\n",
       " 'дополнительные': 846,\n",
       " 'билетов': 847,\n",
       " 'бронирование': 848,\n",
       " 'd': 849,\n",
       " 'аэропорта': 850,\n",
       " 'страницу': 851,\n",
       " 'добычи': 852,\n",
       " 'youtube': 853,\n",
       " 'use': 854,\n",
       " 'рейтинг': 855,\n",
       " 'москвы': 856,\n",
       " 'at': 857,\n",
       " 'принимаются': 858,\n",
       " 'францию': 859,\n",
       " 'оформление': 860,\n",
       " 'подаваться': 861,\n",
       " 'рабочий': 862,\n",
       " 'сбора': 863,\n",
       " 'francevac': 864,\n",
       " 'школьников': 865,\n",
       " 'копию': 866,\n",
       " 'сдачи': 867,\n",
       " 'языках': 868,\n",
       " 'подробную': 869,\n",
       " 'явиться': 870,\n",
       " 'документ': 871,\n",
       " 'заграничный': 872,\n",
       " 'язык': 873,\n",
       " 'подписанная': 874,\n",
       " 'подпись': 875,\n",
       " 'заверенное': 876,\n",
       " 'turnov': 877,\n",
       " 'feed': 878,\n",
       " 'котлов': 879,\n",
       " 'оснащение': 880,\n",
       " 'проконсультируем': 881,\n",
       " 'строительства': 882,\n",
       " 'момента': 883,\n",
       " 'создание': 884,\n",
       " 'проекты': 885,\n",
       " 'ваш': 886,\n",
       " 'бар': 887,\n",
       " 'мероприятий': 888,\n",
       " 'поскольку': 889,\n",
       " 'многие': 890,\n",
       " 'телевизором': 891,\n",
       " 'ванной': 892,\n",
       " 'вид': 893,\n",
       " 'часа': 894,\n",
       " 'существует': 895,\n",
       " 'недель': 896,\n",
       " 'азии': 897,\n",
       " 'праги': 898,\n",
       " 'там': 899,\n",
       " 'означает': 900,\n",
       " 'собрания': 901,\n",
       " 'каждого': 902,\n",
       " 'можем': 903,\n",
       " 'невозможно': 904,\n",
       " 'высокопреосвященного': 905,\n",
       " 'архиепископом': 906,\n",
       " 'гавриилом': 907,\n",
       " 'отцы': 908,\n",
       " 'служить': 909,\n",
       " 'епископов': 910,\n",
       " 'одно': 911,\n",
       " 'потому': 912,\n",
       " 'вечер': 913,\n",
       " 'разные': 914,\n",
       " 'процессе': 915,\n",
       " 'самые': 916,\n",
       " 'числе': 917,\n",
       " 'российской': 918,\n",
       " 'россию': 919,\n",
       " 'узнать': 920,\n",
       " 'ответственность': 921,\n",
       " 'развития': 922,\n",
       " 'подробная': 923,\n",
       " 'желающих': 924,\n",
       " 'дабы': 925,\n",
       " 'обращаем': 926,\n",
       " 'принимает': 927,\n",
       " 'окончания': 928,\n",
       " 'рассмотрения': 929,\n",
       " 'правилами': 930,\n",
       " 'опытом': 931,\n",
       " 'сервис': 932,\n",
       " 'литве': 933,\n",
       " 'мастеров': 934,\n",
       " 'скульптур': 935,\n",
       " 'государственный': 936,\n",
       " 'животные': 937,\n",
       " 'фестиваля': 938,\n",
       " 'искусств': 939,\n",
       " 'первые': 940,\n",
       " 'дата': 941,\n",
       " 'растениями': 942,\n",
       " 'бед': 943,\n",
       " 'watch': 944,\n",
       " 'sns': 945,\n",
       " 'одну': 946,\n",
       " 'форум': 947,\n",
       " 'e': 948,\n",
       " 'радиостанции': 949,\n",
       " 'wrn': 950,\n",
       " 'содержание': 951,\n",
       " 'мероприятия': 952,\n",
       " 'may': 953,\n",
       " 'processing': 954,\n",
       " 'фотографий': 955,\n",
       " '«химпром»': 956,\n",
       " 'контроль': 957,\n",
       " 'объектов': 958,\n",
       " 'рабочих': 959,\n",
       " 'сотрудники': 960,\n",
       " 'посольством': 961,\n",
       " 'медицинского': 962,\n",
       " 'электронной': 963,\n",
       " 'приглашения': 964,\n",
       " 'банковского': 965,\n",
       " 'пакет': 966,\n",
       " 'доверенности': 967,\n",
       " 'конкуренции': 968,\n",
       " 'заполненная': 969,\n",
       " 'гарантией': 970,\n",
       " 'неофициальный': 971,\n",
       " 'заверять': 972,\n",
       " 'счета': 973,\n",
       " 'справки': 974,\n",
       " 'фотографии': 975,\n",
       " 'любого': 976,\n",
       " 'персональные': 977,\n",
       " 'остаться': 978,\n",
       " 'олимпос': 979,\n",
       " 'германии': 980,\n",
       " 'австрийский': 981,\n",
       " 'австрийских': 982,\n",
       " 'пациентов': 983,\n",
       " 'подробно': 984,\n",
       " 'прага': 985,\n",
       " 'цель': 986,\n",
       " 'наша': 987,\n",
       " 'республики': 988,\n",
       " 'использовать': 989,\n",
       " 'бумаги': 990,\n",
       " 'сейчас': 991,\n",
       " 'необходимые': 992,\n",
       " 'самого': 993,\n",
       " 'конечно': 994,\n",
       " 'театр': 995,\n",
       " 'достаточно': 996,\n",
       " 'месте': 997,\n",
       " 'гг': 998,\n",
       " 'названия': 999,\n",
       " 'пор': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Here is an example for a samples number 1:\")\n",
    "print(\"Source('en') example:\", id_to_text(dataset['x'][0], dataset['x_tk']))\n",
    "print(\"Target('ru') example:\", id_to_text(dataset['y'][0], dataset['y_tk']))\n",
    "print(\" \")\n",
    "print(\"Samples number 2:\")\n",
    "print(\"Source('en') example:\", id_to_text(dataset['x'][1], dataset['x_tk']))\n",
    "print(\"Target('ru') example:\", id_to_text(dataset['y'][1], dataset['y_tk']))\n",
    "print(\"source vocabulary size:\", dataset['x'].max())\n",
    "print(\"target vocabulary size:\", dataset['y'].max())\n",
    "dataset['y_tk'].word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models\n",
    "\n",
    "The models are implemented with a similar set of parameters. The main idea is to keep models as small and simple as possible to quickly train them and validate the difference the primarely derived from model architectures. The summary of main hyper parameters presented below:\n",
    "\n",
    "* Mapping:\n",
    "    - Embeddings - word indices will be mapped into a 16-dimentional space\n",
    "    - Dense mapping - recurrence outputs mapped into the target-language space, represented with OHE, via Dense mapping\n",
    "* Layers:\n",
    "    - GRU - number of units 128\n",
    "    - Bidirectional GRU - number of untis set up to 64 in order to keep the total number of units the same (128)\n",
    "    - Batch Normalization - To speed up the training batch normalization is inserted after embeddings and before dense mapping\n",
    "* Optimization:\n",
    "    - Adam - all models trained with Adam optimizer and the same learning rate (1e-3)\n",
    "* Loss function:\n",
    "    - sparse_categorical_crossentropy_from_logits - keras.losses.sparse_categorical_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "embeddings_units = 16\n",
    "gru_units = 128\n",
    "epochs = 2\n",
    "validation_split = 0.05\n",
    "batch_size = 512\n",
    "sparse_categorical_crossentropy_from_logits = partial(keras.losses.sparse_categorical_crossentropy, from_logits=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model list:**\n",
    "\n",
    "1. Embedded GRU\n",
    "2. Embedded Bidirectional GRU\n",
    "3. Embedded GRU encoder-decoder model\n",
    "4. Embedded GRU encoder-decoder model with Multiplicative Attention\n",
    "\n",
    "#### Model 1 - Embedded GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model summary:\n",
      "Model: \"model_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_9 (InputLayer)         [(None, 113)]             0         \n",
      "_________________________________________________________________\n",
      "embedding_7 (Embedding)      (None, 113, 16)           816640    \n",
      "_________________________________________________________________\n",
      "unified_gru_7 (UnifiedGRU)   (None, 113, 128)          56064     \n",
      "_________________________________________________________________\n",
      "time_distributed_6 (TimeDist (None, 113, 155245)       20026605  \n",
      "=================================================================\n",
      "Total params: 20,899,309\n",
      "Trainable params: 20,899,309\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 58964 samples, validate on 6552 samples\n",
      "Epoch 1/10\n",
      "  128/58964 [..............................] - ETA: 4:28:25 - loss: 5.6606 - accuracy: 0.6659   "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-124-a8cff5b8db55>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Model summary:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0membed_rnn_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0membed_rnn_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'x'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZeroPadding1D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'x'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'y'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'y'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;31m# Print prediction(s)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits_to_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membed_rnn_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'x'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'y_tk'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/env/translation-tf2/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    871\u001b[0m           \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m           \u001b[0mvalidation_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m           steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m    874\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/env/translation-tf2/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/env/translation-tf2/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3215\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3216\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3217\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3218\u001b[0m     return nest.pack_sequence_as(self._outputs_structure,\n\u001b[1;32m   3219\u001b[0m                                  [x.numpy() for x in outputs])\n",
      "\u001b[0;32m~/env/translation-tf2/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    556\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[1;32m    557\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[0;32m--> 558\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    559\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/env/translation-tf2/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    625\u001b[0m     \u001b[0;31m# Only need to override the gradient in graph mode and when we have outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 627\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    628\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_register_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/env/translation-tf2/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args)\u001b[0m\n\u001b[1;32m    413\u001b[0m             attrs=(\"executor_type\", executor_type,\n\u001b[1;32m    414\u001b[0m                    \"config_proto\", config),\n\u001b[0;32m--> 415\u001b[0;31m             ctx=ctx)\n\u001b[0m\u001b[1;32m    416\u001b[0m       \u001b[0;31m# Replace empty list with None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/env/translation-tf2/lib/python3.6/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     59\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def embedded_gru_model(input_shape, output_sequence_length, source_vocab_size, target_vocab_size):\n",
    "    \"\"\"\n",
    "    Build and train a RNN model using word embedding on x and y\n",
    "    :param input_shape: Tuple of input shape\n",
    "    :param output_sequence_length: Length of output sequence\n",
    "    :param english_vocab_size: Number of unique English words in the dataset\n",
    "    :param french_vocab_size: Number of unique French words in the dataset\n",
    "    :return: Keras model built, but not trained\n",
    "    \"\"\"\n",
    "    input_seq = keras.Input(input_shape[1:])\n",
    "    embedded_seq = keras.layers.Embedding(source_vocab_size, embeddings_units, input_length=output_sequence_length)(input_seq)\n",
    "    rnn = keras.layers.GRU(gru_units, return_sequences=True)(embedded_seq)\n",
    "    logits = keras.layers.TimeDistributed(keras.layers.Dense(target_vocab_size))(rnn)\n",
    "    model = keras.Model(input_seq, logits)\n",
    "    model.compile(loss=keras.losses.sparse_categorical_crossentropy,\n",
    "                  optimizer=keras.optimizers.Adam(learning_rate),\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Train the neural network\n",
    "embed_rnn_model = embedded_gru_model(\n",
    "    dataset['x'].shape,\n",
    "    dataset['y'].shape[1],\n",
    "    len(dataset['x_tk'].word_index)+1,\n",
    "    len(dataset['y_tk'].word_index)+1)\n",
    "print(\"Model summary:\")\n",
    "embed_rnn_model.summary()\n",
    "embed_rnn_model.fit(dataset['x'], \n",
    "                    keras.layers.ZeroPadding1D((0, dataset['x'].shape[1]-dataset['y'].shape[1]))(dataset['y'][:,:,None]), \n",
    "                    batch_size=batch_size, \n",
    "                    epochs=epochs, \n",
    "                    validation_split=validation_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print prediction(s)\n",
    "sentense_id = -1\n",
    "print(\"Source('en') example:\", id_to_text(dataset['x'][sentense_id], dataset['x_tk']))\n",
    "print(\"Target('ru') example:\", id_to_text(dataset['y'][sentense_id], dataset['y_tk']))\n",
    "print(\"Translation(en_ru) example:\", logits_to_text(logits_to_id(embed_rnn_model.predict(dataset['x'][sentense_id][None, :])[0]), dataset['y_tk']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
