{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Translation Project\n",
    "\n",
    "\n",
    "The goal of the project is to compare the strength of the following recurrent models:\n",
    "\n",
    "1. Embedded GRU\n",
    "2. Embedded Bidirectional GRU\n",
    "3. Embedded GRU encoder-decoder model\n",
    "4. Embedded GRU encoder-decoder model with Multiplicative Attention\n",
    "\n",
    "The models implemented in Tensorflow 2.0 with Keras as a high-level API. Models are trained and analyzed based on [TedHrlrTranslate dataset](https://www.tensorflow.org/datasets/datasets#ted_hrlr_translate)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm, tqdm_notebook\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow_datasets.translate.ted_hrlr import TedHrlrTranslate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data load, extraction, and transformation is done with data_etl() method. This method returns a dictionary containing source data stored under 'x' label. Target data is stored under 'y' label. In addition to the source and target data, the dictionary contains x and y tockenizers (stored as 'x_tk' and 'y_tk') and source and target maximum sequence length ('x_length' and 'y_length'):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_etl(lang_pairs: str = 'ru_to_en', download_dir: str = \".\") -> dict:\n",
    "    print(\"Start data ETL\")\n",
    "    # Download a language data-set specified by :param language_pairs\n",
    "    builder = TedHrlrTranslate(data_dir=download_dir, config=lang_pairs)\n",
    "    builder.download_and_prepare()\n",
    "    datasets = builder.as_dataset()\n",
    "    print(\"Downloaded successfully\")\n",
    "\n",
    "    # extract data\n",
    "    target, source = [], []\n",
    "    for dataset_name in ['train', 'test', 'validation']:\n",
    "        # extract dataset\n",
    "        dataset = datasets[dataset_name]\n",
    "        # convert into numpy\n",
    "        dataset = tfds.as_numpy(dataset)\n",
    "        # convert to string\n",
    "        dataset = list(map(lambda features: (features['ru'].decode(\"utf-8\"), features['en'].decode(\"utf-8\")), dataset))\n",
    "        source.extend([t[1] for t in dataset])\n",
    "        target.extend([t[0] for t in dataset])\n",
    "\n",
    "    print(\"Extracted successfully\")\n",
    "\n",
    "    # Tockenize\n",
    "    x, x_tk = tokenize(source)\n",
    "    y, y_tk = tokenize(target)\n",
    "\n",
    "    x, x_length = pad(x)\n",
    "    y, y_length = pad(y)\n",
    "\n",
    "    print(\"Transformed successfully\")\n",
    "\n",
    "    return {'x': x, 'y': y, 'x_tk': x_tk, 'y_tk': y_tk, 'x_length': x_length, 'y_length': y_length}\n",
    "\n",
    "def tokenize(x):\n",
    "    \"\"\"\n",
    "    Tokenize x\n",
    "    :param x: List of sentences/strings to be tokenized\n",
    "    :return: Tuple of (tokenized x data, tokenizer used to tokenize x)\n",
    "    \"\"\"\n",
    "    x_tk = keras.preprocessing.text.Tokenizer()\n",
    "    x_tk.fit_on_texts(x)\n",
    "    return x_tk.texts_to_sequences(x), x_tk\n",
    "\n",
    "def pad(x, length=None) -> tuple:\n",
    "    \"\"\"\n",
    "    Pad x\n",
    "    :param x: List of sequences.\n",
    "    :param length: Length to pad the sequence to.  If None, use length of longest sequence in x.\n",
    "    :return: Padded numpy array of sequences\n",
    "    \"\"\"\n",
    "    if length is None:\n",
    "        length = max([len(sentence) for sentence in x])\n",
    "\n",
    "    return keras.preprocessing.sequence.pad_sequences(x, maxlen=length, padding='post'), length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dataset = data_etl()\n",
    "\n",
    "{\n",
    "'x': np.ndarray,\n",
    "'y': np.ndarray,\n",
    "'x_tk': keras.preprocessing.text.Tokenizer,\n",
    "'y_tk': keras.preprocessing.text.Tokenizer,\n",
    "'x_length': int,\n",
    "'y_length': int\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start data ETL\n",
      "Downloaded successfully\n",
      "Extracted successfully\n",
      "Transformed successfully\n"
     ]
    }
   ],
   "source": [
    "dataset = data_etl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
