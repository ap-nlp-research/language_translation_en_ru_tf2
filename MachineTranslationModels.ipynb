{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Translation Project\n",
    "\n",
    "\n",
    "The goal of the project is to compare the strength of the following recurrent models:\n",
    "\n",
    "1. Embedded GRU\n",
    "2. Embedded Bidirectional GRU\n",
    "3. Embedded GRU encoder-decoder model\n",
    "4. Embedded GRU encoder-decoder model with Multiplicative Attention\n",
    "\n",
    "The models implemented in Tensorflow 2.0 with Keras as a high-level API. Models are trained and analyzed based on [TedHrlrTranslate dataset](https://www.tensorflow.org/datasets/datasets#ted_hrlr_translate)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow_datasets.translate.ted_hrlr import TedHrlrTranslate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data ETL\n",
    "\n",
    "The data load, extraction, and transformation is done with data_etl() method. This method returns a dictionary containing source data stored under 'x' label. Target data is stored under 'y' label. In addition to the source and target data, the dictionary contains x and y tockenizers (stored as 'x_tk' and 'y_tk') and source and target maximum sequence length ('x_length' and 'y_length'):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_etl(lang_pairs: str = 'ru_to_en', download_dir: str = \".\") -> dict:\n",
    "    print(\"Start data ETL\")\n",
    "    # Download a language data-set specified by :param language_pairs\n",
    "    builder = TedHrlrTranslate(data_dir=download_dir, config=lang_pairs)\n",
    "    builder.download_and_prepare()\n",
    "    datasets = builder.as_dataset()\n",
    "    print(\"Downloaded successfully\")\n",
    "\n",
    "    # extract data\n",
    "    target, source = [], []\n",
    "    for dataset_name in ['train', 'test', 'validation']:\n",
    "        # extract dataset\n",
    "        dataset = datasets[dataset_name]\n",
    "        # convert into numpy\n",
    "        dataset = tfds.as_numpy(dataset)\n",
    "        # convert to string\n",
    "        dataset = list(map(lambda features: (features['ru'].decode(\"utf-8\"), features['en'].decode(\"utf-8\")), dataset))\n",
    "        source.extend([t[1] for t in dataset])\n",
    "        target.extend([t[0] for t in dataset])\n",
    "\n",
    "    print(\"Extracted successfully\")\n",
    "\n",
    "    # Tockenize\n",
    "    x, x_tk = tokenize(source)\n",
    "    y, y_tk = tokenize(target)\n",
    "\n",
    "    x, x_length = pad(x)\n",
    "    y, y_length = pad(y)\n",
    "\n",
    "    print(\"Transformed successfully\")\n",
    "\n",
    "    return {'x': x, 'y': y, 'x_tk': x_tk, 'y_tk': y_tk, 'x_length': x_length, 'y_length': y_length}\n",
    "\n",
    "def tokenize(x):\n",
    "    \"\"\"\n",
    "    Tokenize x\n",
    "    :param x: List of sentences/strings to be tokenized\n",
    "    :return: Tuple of (tokenized x data, tokenizer used to tokenize x)\n",
    "    \"\"\"\n",
    "    x_tk = keras.preprocessing.text.Tokenizer()\n",
    "    x_tk.fit_on_texts(x)\n",
    "    return x_tk.texts_to_sequences(x), x_tk\n",
    "\n",
    "def pad(x, length=None) -> tuple:\n",
    "    \"\"\"\n",
    "    Pad x\n",
    "    :param x: List of sequences.\n",
    "    :param length: Length to pad the sequence to.  If None, use length of longest sequence in x.\n",
    "    :return: Padded numpy array of sequences\n",
    "    \"\"\"\n",
    "    if length is None:\n",
    "        length = max([len(sentence) for sentence in x])\n",
    "\n",
    "    return keras.preprocessing.sequence.pad_sequences(x, maxlen=length, padding='post'), length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dataset = data_etl()\n",
    "\n",
    "{\n",
    "'x': np.ndarray,\n",
    "'y': np.ndarray,\n",
    "'x_tk': keras.preprocessing.text.Tokenizer,\n",
    "'y_tk': keras.preprocessing.text.Tokenizer,\n",
    "'x_length': int,\n",
    "'y_length': int\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start data ETL\n",
      "Downloaded successfully\n",
      "Extracted successfully\n",
      "Transformed successfully\n"
     ]
    }
   ],
   "source": [
    "dataset = data_etl()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions\n",
    "\n",
    "In addition to the data ETL, the code below provides two additional functions for converting logits into word indicies and converting word indicies into text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logits_to_id(logits):\n",
    "    \"\"\"\n",
    "    Turns logits into word ids\n",
    "    :param logits: Logits from a neural network\n",
    "    \"\"\"\n",
    "    return [prediction for prediction in np.argmax(logits, 1)]\n",
    "\n",
    "def id_to_text(idx, tokenizer):\n",
    "    \"\"\"\n",
    "    Turns id into text using the tokenizer\n",
    "    :param idx: word id\n",
    "    :param tokenizer: Keras Tokenizer fit on the labels\n",
    "    :return: String that represents the text of the logits\n",
    "    \"\"\"\n",
    "    index_to_words = {id: word for word, id in tokenizer.word_index.items()}\n",
    "    index_to_words[0] = '<PAD>'\n",
    "\n",
    "    return ' '.join([index_to_words[prediction] for prediction in idx]).replace(\" <PAD>\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is an example for a samples number 1:\n",
      "Source('en') example: visceral is subconscious you 're unaware of it\n",
      "Target('ru') example: интуитивность подсознательна — вы не осознаете её\n",
      " \n",
      "Samples number 2:\n",
      "Source('en') example: and i 'd like to tell you the story in three acts and if i have time still an epilogue\n",
      "Target('ru') example: и я хотел бы рассказать вам эту историю в трех актах а если останется время и эпилог\n"
     ]
    }
   ],
   "source": [
    "print(\"Here is an example for a samples number 1:\")\n",
    "print(\"Source('en') example:\", id_to_text(dataset['x'][0], dataset['x_tk']))\n",
    "print(\"Target('ru') example:\", id_to_text(dataset['y'][0], dataset['y_tk']))\n",
    "print(\" \")\n",
    "print(\"Samples number 2:\")\n",
    "print(\"Source('en') example:\", id_to_text(dataset['x'][1], dataset['x_tk']))\n",
    "print(\"Target('ru') example:\", id_to_text(dataset['y'][1], dataset['y_tk']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models\n",
    "\n",
    "The models are implemented with a similar set of parameters. The main idea is to keep models as small and simple as possible to quickly train them and validate the difference the primarely derived from model architectures. The summary of main hyper parameters presented below:\n",
    "\n",
    "* Mapping:\n",
    "    - Embeddings - word indices will be mapped into a 16-dimentional space\n",
    "    - Dense mapping - recurrence outputs mapped into the target-language space, represented with OHE, via Dense mapping\n",
    "* Layers:\n",
    "    - GRU - number of units 128\n",
    "    - Bidirectional GRU - number of untis set up to 64 in order to keep the total number of units the same (128)\n",
    "    - Batch Normalization - To speed up the training batch normalization is inserted after embeddings and before dense mapping\n",
    "* Optimization:\n",
    "    - Adam - all models trained with Adam optimizer and the same learning rate (1e-3)\n",
    "\n",
    "**Model list:**\n",
    "\n",
    "1. Embedded GRU\n",
    "2. Embedded Bidirectional GRU\n",
    "3. Embedded GRU encoder-decoder model\n",
    "4. Embedded GRU encoder-decoder model with Multiplicative Attention"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
