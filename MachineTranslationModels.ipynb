{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MachineTranslationModels.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ap-nlp-research/language_translation_en_ru_tf2/blob/master/MachineTranslationModels.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "17OD4xTkRxeL"
      },
      "source": [
        "# Machine Translation Project\n",
        "\n",
        "\n",
        "The goal of the project is to compare the strength of the following recurrent models:\n",
        "\n",
        "1. Embedded GRU\n",
        "2. Embedded Bidirectional GRU\n",
        "3. Embedded GRU encoder-decoder model\n",
        "4. Embedded GRU encoder-decoder model with Multiplicative Attention\n",
        "\n",
        "The models implemented in Tensorflow 2.0 with Keras as a high-level API. Models are trained and analyzed based on EN-RU [wmt19_translate dataset](https://www.tensorflow.org/datasets/datasets#wmt19_translate) ([ACL 2019 FOURTH CONFERENCE ON MACHINE TRANSLATION (WMT19)](http://www.statmt.org/wmt19/translation-task.html))."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "j3gfCUyDRxeN",
        "outputId": "958452d0-a7db-4e0a-965e-df56c594cf4a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        }
      },
      "source": [
        "!pip install --force https://github.com/chengs/tqdm/archive/colab.zip\n",
        "!pip install tensorflow-gpu==2.0.0-alpha0\n",
        "!git clone https://github.com/ap-nlp-research/language_translation_en_ru_tf2.git translation_en_ru"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting https://github.com/chengs/tqdm/archive/colab.zip\n",
            "\u001b[?25l  Downloading https://github.com/chengs/tqdm/archive/colab.zip\n",
            "\u001b[K     | 604kB 3.2MB/s\n",
            "\u001b[?25hBuilding wheels for collected packages: tqdm\n",
            "  Building wheel for tqdm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-4qr3zmaj/wheels/41/18/ee/d5dd158441b27965855b1bbae03fa2d8a91fe645c01b419896\n",
            "Successfully built tqdm\n",
            "Installing collected packages: tqdm\n",
            "  Found existing installation: tqdm 4.28.1\n",
            "    Uninstalling tqdm-4.28.1:\n",
            "      Successfully uninstalled tqdm-4.28.1\n",
            "Successfully installed tqdm-4.28.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tqdm"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ecoqx4K1RxeT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 115
        },
        "outputId": "4bfdf02c-6986-4071-d222-f7c2d6376f00"
      },
      "source": [
        "import os\n",
        "import pickle as pk\n",
        "import subprocess\n",
        "import re\n",
        "import numpy as np\n",
        "#from tqdm import tqdm, tqdm_notebook\n",
        "from tqdm.autonotebook import tqdm\n",
        "from typing import Callable\n",
        "from functools import partial\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.python.keras import backend as K"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tqdm/_tqdm_notebook.py:88: TqdmExperimentalWarning: Detect Google Colab 0.0.1a2 and thus load dummy ipywidgets package. Note that UI is different from that in Jupyter. See https://github.com/tqdm/tqdm/pull/640\n",
            "  \" See https://github.com/tqdm/tqdm/pull/640\".format(colab.__version__), TqdmExperimentalWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/tqdm/autonotebook/__init__.py:14: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
            "  \" (e.g. in jupyter console)\", TqdmExperimentalWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "myYPpU6bRxeX"
      },
      "source": [
        "## Data ETL\n",
        "\n",
        "The data load, extraction, and transformation was done beforehand with [create_dataset_en_ru.py](./create_dataset_en_ru.py) script. This script stores a dictionary containing source data under 'x' label. Target data is stored under 'y' label. In addition to the source and target data, the dictionary contains x and y tockenizers (stored as 'x_tk' and 'y_tk'):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "s6-C7QMFRxea"
      },
      "source": [
        "dataset: dict\n",
        "\n",
        "{\n",
        "    'x': np.ndarray,\n",
        "    'y': np.ndarray,\n",
        "    'x_tk': keras.preprocessing.text.Tokenizer,\n",
        "    'y_tk': keras.preprocessing.text.Tokenizer\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hc1RXHOURxeb",
        "colab": {}
      },
      "source": [
        "with open(\"./translation_en_ru/data/wlm_en_ru.pkl\", 'rb') as file:\n",
        "    dataset = pk.load(file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tj5lJy93Rxeg"
      },
      "source": [
        "## Utility Functions\n",
        "\n",
        "In addition to the data ETL, the code below provides two additional functions for converting logits into word indicies and converting word indicies into text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tjrUMAOPRxeh",
        "colab": {}
      },
      "source": [
        "def logits_to_id(logits):\n",
        "    \"\"\"\n",
        "    Turns logits into word ids\n",
        "    :param logits: Logits from a neural network\n",
        "    \"\"\"\n",
        "    return np.argmax(logits, 1)\n",
        "\n",
        "def id_to_text(idx, tokenizer):\n",
        "    \"\"\"\n",
        "    Turns id into text using the tokenizer\n",
        "    :param idx: word id\n",
        "    :param tokenizer: Keras Tokenizer fit on the labels\n",
        "    :return: String that represents the text of the logits\n",
        "    \"\"\"\n",
        "    index_to_words = {id: word for word, id in tokenizer.word_index.items()}\n",
        "    index_to_words[0] = '<PAD>'\n",
        "\n",
        "    return ' '.join([index_to_words[prediction] for prediction in idx]).replace(\" <PAD>\", \"\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VE45jscXRxek",
        "outputId": "83744aba-06cf-4c42-a32f-c256fd500650",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        }
      },
      "source": [
        "print(\"Here is an example for a sample number 1:\")\n",
        "print(\"Source('en') example:\", id_to_text(dataset['x'][0], dataset['x_tk']))\n",
        "print(\"Target('ru') example:\", id_to_text(dataset['y'][0], dataset['y_tk']))\n",
        "print(\" \")\n",
        "print(\"A sample number 2:\")\n",
        "print(\"Source('en') example:\", id_to_text(dataset['x'][-10], dataset['x_tk']))\n",
        "print(\"Target('ru') example:\", id_to_text(dataset['y'][-10], dataset['y_tk']))\n",
        "print(\"source vocabulary size:\", dataset['x'].max())\n",
        "print(\"target vocabulary size:\", dataset['y'].max())\n",
        "print(\"Source shape:\", dataset['x'].shape)\n",
        "print(\"Target shape:\", dataset['y'].shape)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Here is an example for a sample number 1:\n",
            "Source('en') example: the company has been registered with the municipal court in prague in section b file 1 4 8 5 7\n",
            "Target('ru') example: фирма зарегистрирована в городском суде в г праге раздел б [rare] 1 4 8 5 7\n",
            " \n",
            "A sample number 2:\n",
            "Source('en') example: six years ago i had a surgery and l 4 l 5 [rare] were [rare] now l 5 s 1 [rare] [rare] and i had a second surgery that went well\n",
            "Target('ru') example: шесть лет назад мне сделали операцию и на дисках l 4 l 5 сейчас [rare] [rare] диски l 5 s 1 и было необходимо второе хирургическое вмешательство которое произошло вчера и прошло хорошо\n",
            "source vocabulary size: 3499\n",
            "target vocabulary size: 14999\n",
            "Source shape: (14751, 148)\n",
            "Target shape: (14751, 148)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "g2b8qgl5Rxen"
      },
      "source": [
        "## Models\n",
        "\n",
        "The models are implemented with a similar set of parameters. The main idea is to keep models as small and simple as possible to quickly train them and validate the difference the primarely derived from model architectures. The summary of main hyper parameters presented below:\n",
        "\n",
        "* Mapping:\n",
        "    - Embeddings - word indices will be mapped into a 16-dimentional space\n",
        "    - Dense mapping - recurrence outputs mapped into the target-language space, represented with OHE, via Dense mapping\n",
        "* Layers:\n",
        "    - GRU - number of units 256\n",
        "    - Bidirectional GRU - number of untis set up to 128 in order to keep the total number of units the same (256)\n",
        "    - Batch Normalization - To speed up the training batch normalization is inserted after embeddings and before dense mapping\n",
        "* Optimization:\n",
        "    - Adam - all models trained with Adam optimizer and the same learning rate (1e-3)\n",
        "* Loss function:\n",
        "    - sparse_categorical_crossentropy_from_logits - keras.losses.sparse_categorical_crossentropy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6ZKvPDCFRxen",
        "colab": {}
      },
      "source": [
        "learning_rate = 1e-3\n",
        "embeddings_units = 16\n",
        "gru_units = 256\n",
        "epochs = 10\n",
        "validation_split = 0.1\n",
        "batch_size = 64\n",
        "\n",
        "def focal_loss(gamma=2., alpha=.25):\n",
        "\n",
        "    def call(y_true, y_pred):\n",
        "        y_true = tf.squeeze(tf.cast(y_true, tf.int32))\n",
        "        vocab_size = y_pred.shape[-1]\n",
        "\n",
        "        y_pred = tf.clip_by_value(y_pred, K.epsilon(), 1 - K.epsilon())\n",
        "        truth = tf.one_hot(y_true, depth=vocab_size, dtype=tf.float32)\n",
        "\n",
        "        p_1 = -alpha * tf.reduce_sum(truth * tf.pow(1. - y_pred, gamma) * tf.math.log(y_pred), axis=-1)\n",
        "        p_0 = - (1 - alpha) * tf.reduce_sum((1 - truth) * tf.pow(y_pred, gamma) * tf.math.log(1. - y_pred), axis=-1)\n",
        "\n",
        "        cost = p_1 + p_0\n",
        "\n",
        "        return cost\n",
        "\n",
        "    return call\n",
        "\n",
        "\n",
        "#loss = keras.losses.sparse_categorical_crossentropy\n",
        "loss = focal_loss(gamma=2., alpha=.25)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1d7e-xH1Rxeq"
      },
      "source": [
        "**Model list:**\n",
        "\n",
        "1. Embedded GRU - 2 stacked GRU cells 256 units each\n",
        "2. Embedded Bidirectional GRU - 2 stacked bi-directional GRU cells 256 units each\n",
        "3. Embedded GRU encoder-decoder model - 1 GRU cell as an encoder and 1 GRU cell as a decoder (256 units each)\n",
        "4. Embedded GRU encoder-decoder model with Multiplicative Attention\n",
        "\n",
        "#### Model 1 - Embedded GRU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "l4QhEHNaRxer",
        "outputId": "30809a74-057b-4bb7-8775-7a62b1be0f1f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510
        }
      },
      "source": [
        "def embedded_gru_model(input_shape, output_sequence_length, source_vocab_size, target_vocab_size):\n",
        "    \"\"\"\n",
        "    Build and train a RNN model using word embedding on x and y\n",
        "    :param input_shape: Tuple of input shape\n",
        "    :param output_sequence_length: Length of output sequence\n",
        "    :param english_vocab_size: Number of unique English words in the dataset\n",
        "    :param french_vocab_size: Number of unique French words in the dataset\n",
        "    :return: Keras model built, but not trained\n",
        "    \"\"\"\n",
        "    input_seq = keras.Input(input_shape[1:])\n",
        "    if output_sequence_length>input_shape[1]:\n",
        "        expanded_seq = keras.backend.squeeze(\n",
        "            keras.layers.ZeroPadding1D((0, output_sequence_length-input_shape[1]))(\n",
        "                keras.layers.Reshape((input_shape[1], 1))(input_seq)\n",
        "            ),\n",
        "            axis = -1\n",
        "        )\n",
        "    else:\n",
        "        expanded_seq = input_seq\n",
        "        \n",
        "        \n",
        "    embedded_seq = keras.layers.TimeDistributed(keras.layers.BatchNormalization(axis=-1))(\n",
        "        keras.layers.Embedding(source_vocab_size, embeddings_units, input_length=output_sequence_length)(expanded_seq)\n",
        "    )\n",
        "    rnn = keras.layers.TimeDistributed(keras.layers.BatchNormalization(axis=-1))(\n",
        "        keras.layers.GRU(gru_units, return_sequences=True)(embedded_seq)\n",
        "    )\n",
        "    rnn = keras.layers.TimeDistributed(keras.layers.BatchNormalization(axis=-1))(\n",
        "        keras.layers.GRU(gru_units, return_sequences=True)(rnn)\n",
        "    )\n",
        "    logits = keras.layers.TimeDistributed(keras.layers.BatchNormalization(axis=-1))(\n",
        "        keras.layers.TimeDistributed(keras.layers.Dense(4*gru_units, activation='elu'))(rnn)\n",
        "    )\n",
        "    probabilities = keras.layers.TimeDistributed(keras.layers.Dense(target_vocab_size, activation='softmax'))(logits)\n",
        "    \n",
        "    model = keras.Model(input_seq, probabilities)\n",
        "    \n",
        "    model.compile(loss=loss,\n",
        "                  optimizer=keras.optimizers.Adam(learning_rate, clipnorm=3.0),\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "  \n",
        "# Train the neural network\n",
        "keras.backend.clear_session()\n",
        "embed_rnn_model = embedded_gru_model(\n",
        "    dataset['x'].shape,\n",
        "    dataset['y'].shape[1],\n",
        "    dataset['x'].max()+1,\n",
        "    dataset['y'].max()+1)\n",
        "\n",
        "\n",
        "\n",
        "print(\"Model summary:\")\n",
        "embed_rnn_model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model summary:\n",
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 148)]             0         \n",
            "_________________________________________________________________\n",
            "embedding (Embedding)        (None, 148, 16)           56000     \n",
            "_________________________________________________________________\n",
            "time_distributed (TimeDistri (None, 148, 16)           64        \n",
            "_________________________________________________________________\n",
            "unified_gru (UnifiedGRU)     (None, 148, 256)          210432    \n",
            "_________________________________________________________________\n",
            "time_distributed_1 (TimeDist (None, 148, 256)          1024      \n",
            "_________________________________________________________________\n",
            "unified_gru_1 (UnifiedGRU)   (None, 148, 256)          394752    \n",
            "_________________________________________________________________\n",
            "time_distributed_2 (TimeDist (None, 148, 256)          1024      \n",
            "_________________________________________________________________\n",
            "time_distributed_4 (TimeDist (None, 148, 1024)         263168    \n",
            "_________________________________________________________________\n",
            "time_distributed_3 (TimeDist (None, 148, 1024)         4096      \n",
            "_________________________________________________________________\n",
            "time_distributed_5 (TimeDist (None, 148, 15000)        15375000  \n",
            "=================================================================\n",
            "Total params: 16,305,560\n",
            "Trainable params: 16,302,456\n",
            "Non-trainable params: 3,104\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "48rVQw-ch2ek",
        "outputId": "6d79c05e-87ba-4418-81ee-699904ae9acc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "embed_rnn_model.fit(\n",
        "    dataset['x'], \n",
        "    dataset['y'][:,:, None], \n",
        "    batch_size=batch_size, \n",
        "    epochs=epochs, \n",
        "    validation_split=validation_split)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 13275 samples, validate on 1476 samples\n",
            "Epoch 1/5\n",
            "13275/13275 [==============================] - 122s 9ms/sample - loss: 0.4150 - accuracy: 0.8573 - val_loss: 0.7007 - val_accuracy: 0.7890\n",
            "Epoch 2/5\n",
            "13275/13275 [==============================] - 121s 9ms/sample - loss: 0.3016 - accuracy: 0.9147 - val_loss: 1.0893 - val_accuracy: 0.9853\n",
            "Epoch 3/5\n",
            "13275/13275 [==============================] - 122s 9ms/sample - loss: 0.2275 - accuracy: 0.9279 - val_loss: 0.3747 - val_accuracy: 0.9630\n",
            "Epoch 4/5\n",
            "13275/13275 [==============================] - 122s 9ms/sample - loss: 0.2180 - accuracy: 0.9249 - val_loss: 0.2289 - val_accuracy: 0.9456\n",
            "Epoch 5/5\n",
            "13275/13275 [==============================] - 122s 9ms/sample - loss: 0.2096 - accuracy: 0.9226 - val_loss: 0.2265 - val_accuracy: 0.9363\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fb5ca71d8d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "u1Y2tPv5Rxev",
        "outputId": "edee6a02-3084-48ec-b56f-82a5e158ebb9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# Print prediction(s)\n",
        "sentense_id = 2\n",
        "x_sample = dataset['x'][sentense_id]\n",
        "y_sample = dataset['y'][sentense_id]\n",
        "print(\"Source('en') example:\", id_to_text( x_sample, dataset['x_tk'] ))\n",
        "print(\"Source('ru') example:\", id_to_text( y_sample, dataset['y_tk'] ))\n",
        "prediction = embed_rnn_model.predict(x_sample[None, :], verbose=1).squeeze()\n",
        "print(\"Translation(en_ru) example:\", id_to_text( logits_to_id(prediction), dataset['y_tk'] ))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Source('en') example: our team consists of highly experienced professionals who have already successfully implemented several [rare]\n",
            "Source('ru') example: наша команда состоит из [rare] специалистов которые уже реализовали ряд успешных проектов\n",
            "1/1 [==============================] - 0s 105ms/sample\n",
            "Translation(en_ru) example: наша [rare] [rare] [rare] [rare] [rare] [rare] [rare] [rare]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ORh1vXotA9t8"
      },
      "source": [
        "#### Model 2 - BiDirectional GRU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZTraW66N41zP",
        "outputId": "9a764e70-f69f-4b97-c99d-9a927dcef066",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510
        }
      },
      "source": [
        "def bd_gru_model(input_shape, output_sequence_length, source_vocab_size, target_vocab_size):\n",
        "    \"\"\"\n",
        "    Build and train a RNN model using word embedding on x and y\n",
        "    :param input_shape: Tuple of input shape\n",
        "    :param output_sequence_length: Length of output sequence\n",
        "    :param english_vocab_size: Number of unique English words in the dataset\n",
        "    :param french_vocab_size: Number of unique French words in the dataset\n",
        "    :return: Keras model built, but not trained\n",
        "    \"\"\"\n",
        "    input_seq = keras.Input(input_shape[1:])\n",
        "    if output_sequence_length>input_shape[1]:\n",
        "        expanded_seq = keras.backend.squeeze(\n",
        "            keras.layers.ZeroPadding1D((0, output_sequence_length-input_shape[1]))(\n",
        "                keras.layers.Reshape((input_shape[1], 1))(input_seq)\n",
        "            ),\n",
        "            axis = -1\n",
        "        )\n",
        "    else:\n",
        "        expanded_seq = input_seq\n",
        "    embedded_seq = keras.layers.TimeDistributed(keras.layers.BatchNormalization(axis=-1))(\n",
        "        keras.layers.Embedding(source_vocab_size, embeddings_units, input_length=output_sequence_length)(expanded_seq)\n",
        "    )\n",
        "    rnn = keras.layers.TimeDistributed(keras.layers.BatchNormalization(axis=-1))(\n",
        "        keras.layers.Bidirectional(keras.layers.GRU(int(gru_units/2), return_sequences=True))(embedded_seq)\n",
        "    )\n",
        "    rnn = keras.layers.TimeDistributed(keras.layers.BatchNormalization(axis=-1))(\n",
        "        keras.layers.Bidirectional(keras.layers.GRU(int(gru_units/2), return_sequences=True))(rnn)\n",
        "    )\n",
        "    logits = keras.layers.TimeDistributed(keras.layers.BatchNormalization(axis=-1))(\n",
        "        keras.layers.TimeDistributed(keras.layers.Dense(4*gru_units, activation='elu'))(rnn)\n",
        "    )\n",
        "    probabilities = keras.layers.TimeDistributed(keras.layers.Dense(target_vocab_size, activation='softmax'))(logits)\n",
        "    \n",
        "    model = keras.Model(input_seq, probabilities)\n",
        "    \n",
        "    model.compile(loss=loss,\n",
        "                  optimizer=keras.optimizers.Adam(learning_rate, clipnorm=3.0),\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "  \n",
        "# Train the neural network\n",
        "keras.backend.clear_session()\n",
        "bd_rnn_model = embedded_gru_model(\n",
        "    dataset['x'].shape,\n",
        "    dataset['y'].shape[1],\n",
        "    dataset['x'].max()+1,\n",
        "    dataset['y'].max()+1)\n",
        "\n",
        "\n",
        "\n",
        "print(\"Model summary:\")\n",
        "bd_rnn_model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model summary:\n",
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 148)]             0         \n",
            "_________________________________________________________________\n",
            "embedding (Embedding)        (None, 148, 16)           56000     \n",
            "_________________________________________________________________\n",
            "time_distributed (TimeDistri (None, 148, 16)           64        \n",
            "_________________________________________________________________\n",
            "unified_gru (UnifiedGRU)     (None, 148, 256)          210432    \n",
            "_________________________________________________________________\n",
            "time_distributed_1 (TimeDist (None, 148, 256)          1024      \n",
            "_________________________________________________________________\n",
            "unified_gru_1 (UnifiedGRU)   (None, 148, 256)          394752    \n",
            "_________________________________________________________________\n",
            "time_distributed_2 (TimeDist (None, 148, 256)          1024      \n",
            "_________________________________________________________________\n",
            "time_distributed_4 (TimeDist (None, 148, 1024)         263168    \n",
            "_________________________________________________________________\n",
            "time_distributed_3 (TimeDist (None, 148, 1024)         4096      \n",
            "_________________________________________________________________\n",
            "time_distributed_5 (TimeDist (None, 148, 15000)        15375000  \n",
            "=================================================================\n",
            "Total params: 16,305,560\n",
            "Trainable params: 16,302,456\n",
            "Non-trainable params: 3,104\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hFKX2UJHBmvh",
        "outputId": "ed5a2c61-e547-47a5-cca8-399c5f853084",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "source": [
        "bd_rnn_model.fit(\n",
        "    dataset['x'], \n",
        "    dataset['y'][:,:, None], \n",
        "    batch_size=batch_size, \n",
        "    epochs=epochs, \n",
        "    validation_split=validation_split\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 13275 samples, validate on 1476 samples\n",
            "Epoch 1/10\n",
            "13275/13275 [==============================] - 122s 9ms/sample - loss: 0.4260 - accuracy: 0.8531 - val_loss: 1.1436 - val_accuracy: 0.7106\n",
            "Epoch 2/10\n",
            "13275/13275 [==============================] - 122s 9ms/sample - loss: 0.2884 - accuracy: 0.9164 - val_loss: 0.8233 - val_accuracy: 0.9715\n",
            "Epoch 3/10\n",
            "13275/13275 [==============================] - 122s 9ms/sample - loss: 0.2151 - accuracy: 0.9253 - val_loss: 0.3492 - val_accuracy: 0.9687\n",
            "Epoch 4/10\n",
            "13275/13275 [==============================] - 122s 9ms/sample - loss: 0.2028 - accuracy: 0.9240 - val_loss: 0.2194 - val_accuracy: 0.9378\n",
            "Epoch 5/10\n",
            "13275/13275 [==============================] - 122s 9ms/sample - loss: 0.1924 - accuracy: 0.9215 - val_loss: 0.2194 - val_accuracy: 0.9251\n",
            "Epoch 6/10\n",
            "13275/13275 [==============================] - 122s 9ms/sample - loss: 0.1853 - accuracy: 0.9189 - val_loss: 0.2270 - val_accuracy: 0.9448\n",
            "Epoch 7/10\n",
            "13275/13275 [==============================] - 122s 9ms/sample - loss: 0.2101 - accuracy: 0.9167 - val_loss: 0.2539 - val_accuracy: 0.9217\n",
            "Epoch 8/10\n",
            "13275/13275 [==============================] - 122s 9ms/sample - loss: 0.2134 - accuracy: 0.9170 - val_loss: 0.2501 - val_accuracy: 0.8880\n",
            "Epoch 9/10\n",
            "13275/13275 [==============================] - 122s 9ms/sample - loss: 0.2021 - accuracy: 0.9161 - val_loss: 0.2504 - val_accuracy: 0.9118\n",
            "Epoch 10/10\n",
            "13275/13275 [==============================] - 122s 9ms/sample - loss: 0.1949 - accuracy: 0.9148 - val_loss: 0.2604 - val_accuracy: 0.8869\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fb54e6a3f60>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7OZpP5XBB6z4",
        "outputId": "c1bd5dd3-c218-42f7-b4d7-c045af3ab79e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# Print prediction(s)\n",
        "sentense_id = 2\n",
        "x_sample = dataset['x'][sentense_id]\n",
        "y_sample = dataset['y'][sentense_id]\n",
        "print(\"Source('en') example:\", id_to_text( x_sample, dataset['x_tk'] ))\n",
        "print(\"Source('ru') example:\", id_to_text( y_sample, dataset['y_tk'] ))\n",
        "prediction = bd_rnn_model.predict(x_sample[None, :], verbose=1).squeeze()\n",
        "print(\"Translation(en_ru) example:\", id_to_text( logits_to_id(prediction), dataset['y_tk'] ))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Source('en') example: our team consists of highly experienced professionals who have already successfully implemented several [rare]\n",
            "Source('ru') example: наша команда состоит из [rare] специалистов которые уже реализовали ряд успешных проектов\n",
            "1/1 [==============================] - 0s 96ms/sample\n",
            "Translation(en_ru) example: антибактериальная команда [rare] [rare] [rare] специалистов [rare] [rare] реализовали реализовали в [rare] [rare] [rare]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GgoYU9VgVd2H",
        "colab_type": "text"
      },
      "source": [
        "###Model 3 - Seuqnce 2 Sequence\n",
        "\n",
        "This model accumulates the hidden state during the encoder stage and then uses this state to make predictions during the decoder step."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Z2BMIkdWZms",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(keras.Model):\n",
        "\n",
        "\n",
        "    def __init__(self, vocab_size: int, gru_units: int, embeddings_units: int):\n",
        "\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self.embedding = keras.layers.Embedding(vocab_size, embeddings_units)\n",
        "        self.BN_embedding = keras.layers.TimeDistributed(keras.layers.BatchNormalization(axis=-1))\n",
        "        self.gru = keras.layers.GRU(gru_units, return_sequences=False, return_state=True)\n",
        "\n",
        "    def call(self, x):\n",
        "\n",
        "        embedded = self.embedding(x)\n",
        "        bn_embedding = self.BN_embedding(embedded)\n",
        "        h_s = self.gru(bn_embedding)\n",
        "\n",
        "        return h_s[0]\n",
        "\n",
        "\n",
        "class Decoder(keras.Model):\n",
        "\n",
        "\n",
        "    def __init__(self, vocab_size: int, gru_units: int, embeddings_units: int):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.hs_BN = keras.layers.BatchNormalization(axis=-1)\n",
        "        # the embedding layer needs to contain 1 additional token for the start of the sequence (dataset['y'].max()+1)\n",
        "        self.embedding = keras.layers.Embedding(vocab_size+1, embeddings_units)\n",
        "        self.embedding_BN = keras.layers.TimeDistributed(keras.layers.BatchNormalization(axis=-1))\n",
        "        self.gru = keras.layers.GRU(gru_units, return_sequences=True, return_state=True)\n",
        "        self.gru_BN = keras.layers.TimeDistributed(keras.layers.BatchNormalization(axis=-1))\n",
        "        self.dense = keras.layers.Dense(vocab_size, activation='softmax')\n",
        "\n",
        "\n",
        "    def call(self, x, hidden):\n",
        "\n",
        "        embedded = self.embedding_BN(\n",
        "                self.embedding(x)\n",
        "        )\n",
        "\n",
        "        seq, h_t = self.gru(embedded, initial_state = self.hs_BN(hidden))\n",
        "        p = self.dense(self.gru_BN(seq))\n",
        "\n",
        "        return p, h_t"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1JQOw9AWnGa",
        "colab_type": "text"
      },
      "source": [
        "Additional element of the Encoder-Decoder architecture is the teacher-student relationship between the training function and the model. The training function during the decoding phase, uses forcefeeding of the lables as an input into the decoder. This setup allows to simulate the prediction of the correct translation for each word and reusing that prediciton to influence the prediction for the next word."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vf1JOVp0Wn_G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def train_step(inputs, targets,\n",
        "               encoder, decoder,\n",
        "               loss_fn, optimizer,\n",
        "               start_word_index):\n",
        "\n",
        "    loss = 0.0\n",
        "    BATCH_SIZE = int(targets.shape[0])\n",
        "    N_STEPS = int(targets.shape[1])\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        h_s = encoder(inputs=inputs)\n",
        "\n",
        "        # Kick-off decoding with a start word\n",
        "        dec_input = tf.expand_dims([start_word_index] * BATCH_SIZE, 1)\n",
        "        # Teacher forcing - feeding the target as the next input\n",
        "        h_t = h_s\n",
        "        for t in range(N_STEPS):\n",
        "            p, h_t = decoder(dec_input, hidden=h_t)\n",
        "            # normalize loss across time dimension\n",
        "            loss += loss_fn(targets[:, t], p) / tf.cast(N_STEPS, p.dtype)\n",
        "            # using teacher forcing\n",
        "            dec_input = tf.expand_dims(targets[:, t], 1)\n",
        "\n",
        "    # obtain all trainable variables\n",
        "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "\n",
        "    # obtain gradient history across all steps with respect to trainable variables\n",
        "    gradients = tape.gradient(loss, variables)\n",
        "\n",
        "    optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "    return tf.reduce_mean(loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lET7IlF0W8Xf",
        "colab_type": "text"
      },
      "source": [
        "To allow a custom training step, we need to create our custom trianing loop function - fit:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dXIu4WJvXC2R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fit(inputs: np.ndarray, targets: np.ndarray, batch_size: int, epochs: int,\n",
        "        train_step: Callable[[np.ndarray, np.ndarray], float]):\n",
        "\n",
        "    steps = inputs.shape[0] // batch_size\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((inputs, targets))\n",
        "    dataset = dataset.batch(batch_size, drop_remainder=True)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "        pbar = tqdm(range(steps), desc=\"Epoch {}\".format(epoch))\n",
        "        minibatch = enumerate(dataset.take(steps))\n",
        "\n",
        "        for i in pbar:\n",
        "\n",
        "            _, (x, y) = next(minibatch)\n",
        "            loss = train_step(x, y)\n",
        "            pbar.set_postfix(ordered_dict={\"loss\": loss.numpy()})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7UC0zsN0XIJc",
        "colab_type": "text"
      },
      "source": [
        "Putting it all together"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VpJ_u4bxXKI3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "c6f5e4c3-7b31-47c0-a85d-71d58bd308a7"
      },
      "source": [
        "keras.backend.clear_session()\n",
        "# remember 1 token is reserved for unknown words\n",
        "encoder = Encoder(vocab_size=dataset['x'].max() + 1, gru_units=gru_units, embeddings_units=embeddings_units)\n",
        "# remember 1 token is reserved for unknown words\n",
        "decoder = Decoder(vocab_size=dataset['y'].max() + 1, gru_units=gru_units, embeddings_units=embeddings_units)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(lr=learning_rate)\n",
        "\n",
        "loss_fn = keras.losses.sparse_categorical_crossentropy\n",
        "# the decoder contains embedding with target vocabulary size + 1. Additional (+1) token is reserved for the start\n",
        "# token.\n",
        "train_step_fn = partial(train_step, encoder=encoder, decoder=decoder, loss_fn=loss_fn, optimizer=optimizer,\n",
        "                        start_word_index=dataset['y'].max()+1)\n",
        "\n",
        "fit(dataset['x'], dataset['y'], batch_size=batch_size, epochs=epochs, train_step=train_step_fn)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div style=\"display:flex;flex-direction:row;\"><span>Epoch 0</span><progress style='margin:2px 4px;description_width:initial;' max='230' value='230'></progress>100% 230/230 [04:37&lt;00:00,  1.87it/s, loss=0.896]</div>"
            ],
            "text/plain": [
              "<tqdm._fake_ipywidgets.HBox object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div style=\"display:flex;flex-direction:row;\"><span>Epoch 1</span><progress style='margin:2px 4px;description_width:initial;' max='230' value='230'></progress>100% 230/230 [02:03&lt;00:00,  1.89it/s, loss=0.885]</div>"
            ],
            "text/plain": [
              "<tqdm._fake_ipywidgets.HBox object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div style=\"display:flex;flex-direction:row;\"><span>Epoch 2</span><progress style='margin:2px 4px;description_width:initial;' max='230' value='209'></progress> 91% 209/230 [01:52&lt;00:11,  1.89it/s, loss=0.811]</div>"
            ],
            "text/plain": [
              "<tqdm._fake_ipywidgets.HBox object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}