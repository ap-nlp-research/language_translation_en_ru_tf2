{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Translation Project\n",
    "\n",
    "\n",
    "The goal of the project is to compare the strength of the following recurrent models:\n",
    "\n",
    "1. Embedded GRU\n",
    "2. Embedded Bidirectional GRU\n",
    "3. Embedded GRU encoder-decoder model\n",
    "4. Embedded GRU encoder-decoder model with Multiplicative Attention\n",
    "\n",
    "The models implemented in Tensorflow 2.0 with Keras as a high-level API. Models are trained and analyzed based on [TedHrlrTranslate dataset](https://www.tensorflow.org/datasets/datasets#ted_hrlr_translate)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow_datasets.translate.ted_hrlr import TedHrlrTranslate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data ETL\n",
    "\n",
    "The data load, extraction, and transformation is done with data_etl() method. This method returns a dictionary containing source data stored under 'x' label. Target data is stored under 'y' label. In addition to the source and target data, the dictionary contains x and y tockenizers (stored as 'x_tk' and 'y_tk'):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_etl(lang_pairs: str = 'ru_to_en', download_dir: str = \".\") -> dict:\n",
    "    print(\"Start data ETL\")\n",
    "    # Download a language data-set specified by :param language_pairs\n",
    "    builder = TedHrlrTranslate(data_dir=download_dir, config=lang_pairs)\n",
    "    builder.download_and_prepare()\n",
    "    datasets = builder.as_dataset()\n",
    "    print(\"Downloaded successfully\")\n",
    "\n",
    "    # extract data\n",
    "    target, source = [], []\n",
    "    for dataset_name in ['train', 'test', 'validation']:\n",
    "        # extract dataset\n",
    "        dataset = datasets[dataset_name]\n",
    "        # convert into numpy\n",
    "        dataset = tfds.as_numpy(dataset)\n",
    "        # convert to string\n",
    "        dataset = list(map(lambda features: (features['ru'].decode(\"utf-8\"), features['en'].decode(\"utf-8\")), dataset))\n",
    "        source.extend([t[1] for t in dataset])\n",
    "        target.extend([t[0] for t in dataset])\n",
    "\n",
    "    print(\"Extracted successfully\")\n",
    "    \n",
    "    source = [re.sub(\"[0-9]\", \" \\g<0>\", re.sub(\"[^a-zA-Z0-0/-]\", \" \", s.lower())) for s in source]\n",
    "    target = [re.sub(\"[0-9]\", \" \\g<0>\", re.sub(\"[^а-яА-ЯёЁ0-9/-]\", \" \", s.lower())) for s in target]\n",
    "\n",
    "    # Tockenize\n",
    "    x, x_tk = tokenize(source)\n",
    "    y, y_tk = tokenize(target)\n",
    "\n",
    "    x = pad(x)\n",
    "    y = pad(y)\n",
    "\n",
    "    print(\"Transformed successfully\")\n",
    "\n",
    "    return {'x': x, 'y': y, 'x_tk': x_tk, 'y_tk': y_tk}\n",
    "\n",
    "def tokenize(x, num_words=5000, filters_regex=None):\n",
    "    \"\"\"\n",
    "    Tokenize x\n",
    "    :param x: List of sentences/strings to be tokenized\n",
    "    :n_words: Limit of the number of words that will be kept\n",
    "    :filters_regex: Regular expression filtering out words\n",
    "    :return: Tuple of (tokenized x data, tokenizer used to tokenize x)\n",
    "    \"\"\"\n",
    "    if filters_regex:\n",
    "        x_tk = keras.preprocessing.text.Tokenizer(num_words=num_words, filters=filters_regex)\n",
    "    else:\n",
    "        x_tk = keras.preprocessing.text.Tokenizer(num_words=num_words)\n",
    "    x_tk.fit_on_texts(x)\n",
    "    return x_tk.texts_to_sequences(x), x_tk\n",
    "\n",
    "def pad(x, length=None):\n",
    "    \"\"\"\n",
    "    Pad x\n",
    "    :param x: List of sequences.\n",
    "    :param length: Length to pad the sequence to.  If None, use length of longest sequence in x.\n",
    "    :return: Padded numpy array of sequences\n",
    "    \"\"\"\n",
    "    if length is None:\n",
    "        length = max([len(sentence) for sentence in x])\n",
    "\n",
    "    return keras.preprocessing.sequence.pad_sequences(x, maxlen=length, padding='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dataset = data_etl()\n",
    "\n",
    "{\n",
    "    'x': np.ndarray,\n",
    "    'y': np.ndarray,\n",
    "    'x_tk': keras.preprocessing.text.Tokenizer,\n",
    "    'y_tk': keras.preprocessing.text.Tokenizer\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start data ETL\n",
      "Downloaded successfully\n",
      "Extracted successfully\n",
      "Transformed successfully\n"
     ]
    }
   ],
   "source": [
    "dataset = data_etl()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions\n",
    "\n",
    "In addition to the data ETL, the code below provides two additional functions for converting logits into word indicies and converting word indicies into text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logits_to_id(logits):\n",
    "    \"\"\"\n",
    "    Turns logits into word ids\n",
    "    :param logits: Logits from a neural network\n",
    "    \"\"\"\n",
    "    return [prediction for prediction in np.argmax(logits, 1)]\n",
    "\n",
    "def id_to_text(idx, tokenizer):\n",
    "    \"\"\"\n",
    "    Turns id into text using the tokenizer\n",
    "    :param idx: word id\n",
    "    :param tokenizer: Keras Tokenizer fit on the labels\n",
    "    :return: String that represents the text of the logits\n",
    "    \"\"\"\n",
    "    index_to_words = {id: word for word, id in tokenizer.word_index.items()}\n",
    "    index_to_words[0] = '<PAD>'\n",
    "\n",
    "    return ' '.join([index_to_words[prediction] for prediction in idx]).replace(\" <PAD>\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is an example for a samples number 1:\n",
      "Source('en') example: and i d like to tell you the story in three acts and if i have time still an\n",
      "Target('ru') example: и я хотел бы рассказать вам эту историю в трех а если останется время и\n",
      " \n",
      "Samples number 2:\n",
      "Source('en') example: is you re of it\n",
      "Target('ru') example: вы не её\n",
      "source vocabulary size: 4999\n",
      "target vocabulary size: 4999\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'и': 1,\n",
       " 'в': 2,\n",
       " 'что': 3,\n",
       " 'я': 4,\n",
       " 'это': 5,\n",
       " 'на': 6,\n",
       " 'не': 7,\n",
       " 'мы': 8,\n",
       " 'с': 9,\n",
       " 'как': 10,\n",
       " '0': 11,\n",
       " 'но': 12,\n",
       " 'то': 13,\n",
       " 'вы': 14,\n",
       " 'они': 15,\n",
       " 'из': 16,\n",
       " 'для': 17,\n",
       " 'а': 18,\n",
       " 'так': 19,\n",
       " 'у': 20,\n",
       " 'к': 21,\n",
       " 'о': 22,\n",
       " 'он': 23,\n",
       " 'по': 24,\n",
       " 'если': 25,\n",
       " 'когда': 26,\n",
       " '1': 27,\n",
       " 'чтобы': 28,\n",
       " 'за': 29,\n",
       " 'их': 30,\n",
       " 'бы': 31,\n",
       " 'или': 32,\n",
       " 'есть': 33,\n",
       " 'от': 34,\n",
       " 'было': 35,\n",
       " 'же': 36,\n",
       " 'очень': 37,\n",
       " 'все': 38,\n",
       " 'вот': 39,\n",
       " 'его': 40,\n",
       " '2': 41,\n",
       " 'мне': 42,\n",
       " 'которые': 43,\n",
       " 'она': 44,\n",
       " 'нас': 45,\n",
       " 'меня': 46,\n",
       " 'всё': 47,\n",
       " 'нам': 48,\n",
       " 'потому': 49,\n",
       " 'только': 50,\n",
       " 'смех': 51,\n",
       " 'был': 52,\n",
       " 'эти': 53,\n",
       " '5': 54,\n",
       " 'лет': 55,\n",
       " 'том': 56,\n",
       " 'вам': 57,\n",
       " 'чем': 58,\n",
       " 'может': 59,\n",
       " 'людей': 60,\n",
       " 'быть': 61,\n",
       " 'того': 62,\n",
       " 'до': 63,\n",
       " 'этого': 64,\n",
       " 'можно': 65,\n",
       " 'люди': 66,\n",
       " 'просто': 67,\n",
       " 'этот': 68,\n",
       " 'больше': 69,\n",
       " 'этом': 70,\n",
       " 'где': 71,\n",
       " 'были': 72,\n",
       " 'который': 73,\n",
       " 'ещё': 74,\n",
       " 'была': 75,\n",
       " '9': 76,\n",
       " 'время': 77,\n",
       " 'более': 78,\n",
       " 'нет': 79,\n",
       " 'вас': 80,\n",
       " 'во': 81,\n",
       " '3': 82,\n",
       " 'здесь': 83,\n",
       " 'сейчас': 84,\n",
       " 'её': 85,\n",
       " 'ли': 86,\n",
       " 'кто': 87,\n",
       " 'будет': 88,\n",
       " 'них': 89,\n",
       " 'даже': 90,\n",
       " 'аплодисменты': 91,\n",
       " 'этой': 92,\n",
       " 'нужно': 93,\n",
       " '4': 94,\n",
       " 'уже': 95,\n",
       " 'один': 96,\n",
       " 'сделать': 97,\n",
       " 'можем': 98,\n",
       " 'также': 99,\n",
       " 'всего': 100,\n",
       " 'жизни': 101,\n",
       " 'много': 102,\n",
       " 'об': 103,\n",
       " 'себя': 104,\n",
       " 'там': 105,\n",
       " 'со': 106,\n",
       " 'теперь': 107,\n",
       " 'этих': 108,\n",
       " 'поэтому': 109,\n",
       " 'почему': 110,\n",
       " '8': 111,\n",
       " 'которая': 112,\n",
       " 'эта': 113,\n",
       " 'ты': 114,\n",
       " 'спасибо': 115,\n",
       " 'могут': 116,\n",
       " '6': 117,\n",
       " 'должны': 118,\n",
       " 'думаю': 119,\n",
       " 'всех': 120,\n",
       " 'самом': 121,\n",
       " 'раз': 122,\n",
       " 'тем': 123,\n",
       " 'сегодня': 124,\n",
       " 'им': 125,\n",
       " 'хочу': 126,\n",
       " 'после': 127,\n",
       " 'через': 128,\n",
       " 'деле': 129,\n",
       " 'при': 130,\n",
       " 'эту': 131,\n",
       " '7': 132,\n",
       " 'назад': 133,\n",
       " 'можете': 134,\n",
       " 'итак': 135,\n",
       " 'да': 136,\n",
       " 'несколько': 137,\n",
       " 'каждый': 138,\n",
       " 'именно': 139,\n",
       " 'сказал': 140,\n",
       " 'жизнь': 141,\n",
       " 'возможно': 142,\n",
       " 'себе': 143,\n",
       " 'человек': 144,\n",
       " 'этим': 145,\n",
       " 'день': 146,\n",
       " 'под': 147,\n",
       " 'мире': 148,\n",
       " 'делать': 149,\n",
       " 'мой': 150,\n",
       " 'тогда': 151,\n",
       " 'ни': 152,\n",
       " 'действительно': 153,\n",
       " 'которых': 154,\n",
       " 'году': 155,\n",
       " 'хорошо': 156,\n",
       " 'мир': 157,\n",
       " 'например': 158,\n",
       " 'времени': 159,\n",
       " 'вопрос': 160,\n",
       " 'конечно': 161,\n",
       " 'затем': 162,\n",
       " 'между': 163,\n",
       " 'года': 164,\n",
       " 'происходит': 165,\n",
       " 'два': 166,\n",
       " 'одна': 167,\n",
       " 'образом': 168,\n",
       " 'оно': 169,\n",
       " 'наши': 170,\n",
       " 'сказать': 171,\n",
       " 'которой': 172,\n",
       " 'видите': 173,\n",
       " 'таким': 174,\n",
       " 'является': 175,\n",
       " 'потом': 176,\n",
       " 'без': 177,\n",
       " 'над': 178,\n",
       " 'никогда': 179,\n",
       " 'вместе': 180,\n",
       " 'человека': 181,\n",
       " 'тех': 182,\n",
       " 'знаете': 183,\n",
       " 'всегда': 184,\n",
       " 'которую': 185,\n",
       " 'других': 186,\n",
       " 'детей': 187,\n",
       " 'могли': 188,\n",
       " 'такие': 189,\n",
       " 'некоторые': 190,\n",
       " 'часть': 191,\n",
       " 'пока': 192,\n",
       " 'нашей': 193,\n",
       " 'такой': 194,\n",
       " 'лучше': 195,\n",
       " 'другой': 196,\n",
       " 'моя': 197,\n",
       " 'те': 198,\n",
       " 'либо': 199,\n",
       " 'которое': 200,\n",
       " 'будут': 201,\n",
       " 'три': 202,\n",
       " 'давайте': 203,\n",
       " 'своей': 204,\n",
       " 'него': 205,\n",
       " 'ничего': 206,\n",
       " 'свою': 207,\n",
       " 'могу': 208,\n",
       " 'тот': 209,\n",
       " 'многие': 210,\n",
       " 'лишь': 211,\n",
       " 'моей': 212,\n",
       " 'вещи': 213,\n",
       " 'использовать': 214,\n",
       " 'еще': 215,\n",
       " 'год': 216,\n",
       " 'немного': 217,\n",
       " 'наших': 218,\n",
       " 'около': 219,\n",
       " 'такое': 220,\n",
       " 'мира': 221,\n",
       " 'ему': 222,\n",
       " 'другие': 223,\n",
       " 'тоже': 224,\n",
       " 'сша': 225,\n",
       " 'знаю': 226,\n",
       " 'таких': 227,\n",
       " 'одной': 228,\n",
       " 'истории': 229,\n",
       " 'момент': 230,\n",
       " 'понять': 231,\n",
       " 'важно': 232,\n",
       " 'самое': 233,\n",
       " 'друг': 234,\n",
       " 'чего': 235,\n",
       " 'ведь': 236,\n",
       " 'свои': 237,\n",
       " 'проблема': 238,\n",
       " 'знаем': 239,\n",
       " 'перед': 240,\n",
       " 'снова': 241,\n",
       " 'долларов': 242,\n",
       " 'ну': 243,\n",
       " 'идея': 244,\n",
       " 'проблемы': 245,\n",
       " 'найти': 246,\n",
       " 'какой': 247,\n",
       " 'значит': 248,\n",
       " 'хотел': 249,\n",
       " 'тому': 250,\n",
       " 'помощью': 251,\n",
       " 'создать': 252,\n",
       " 'всем': 253,\n",
       " 'тут': 254,\n",
       " 'дело': 255,\n",
       " 'почти': 256,\n",
       " 'ее': 257,\n",
       " 'кажется': 258,\n",
       " 'надо': 259,\n",
       " 'возможность': 260,\n",
       " 'наш': 261,\n",
       " 'работы': 262,\n",
       " 'место': 263,\n",
       " 'первый': 264,\n",
       " 'должен': 265,\n",
       " 'данные': 266,\n",
       " 'видео': 267,\n",
       " 'нибудь': 268,\n",
       " 'случае': 269,\n",
       " 'технологии': 270,\n",
       " 'своих': 271,\n",
       " 'работать': 272,\n",
       " 'работает': 273,\n",
       " 'говорит': 274,\n",
       " 'миллионов': 275,\n",
       " 'этому': 276,\n",
       " 'увидеть': 277,\n",
       " 'работу': 278,\n",
       " 'однако': 279,\n",
       " 'существует': 280,\n",
       " 'мои': 281,\n",
       " 'достаточно': 282,\n",
       " 'мозга': 283,\n",
       " 'людям': 284,\n",
       " 'стали': 285,\n",
       " 'течение': 286,\n",
       " 'примерно': 287,\n",
       " 'часто': 288,\n",
       " 'одно': 289,\n",
       " 'иногда': 290,\n",
       " 'своего': 291,\n",
       " 'слишком': 292,\n",
       " 'мог': 293,\n",
       " 'ней': 294,\n",
       " 'хотя': 295,\n",
       " 'совсем': 296,\n",
       " 'большинство': 297,\n",
       " 'две': 298,\n",
       " 'большое': 299,\n",
       " 'довольно': 300,\n",
       " 'наша': 301,\n",
       " 'дети': 302,\n",
       " 'всему': 303,\n",
       " 'свой': 304,\n",
       " 'говорить': 305,\n",
       " 'имеет': 306,\n",
       " 'количество': 307,\n",
       " 'одного': 308,\n",
       " 'никто': 309,\n",
       " 'собой': 310,\n",
       " 'вокруг': 311,\n",
       " 'страны': 312,\n",
       " 'прямо': 313,\n",
       " 'начали': 314,\n",
       " 'совершенно': 315,\n",
       " 'сделали': 316,\n",
       " 'миру': 317,\n",
       " 'способ': 318,\n",
       " 'двух': 319,\n",
       " 'видеть': 320,\n",
       " 'пример': 321,\n",
       " 'изменить': 322,\n",
       " 'системы': 323,\n",
       " 'наше': 324,\n",
       " 'многих': 325,\n",
       " 'пор': 326,\n",
       " 'насколько': 327,\n",
       " 'говорят': 328,\n",
       " 'показать': 329,\n",
       " 'вместо': 330,\n",
       " 'новые': 331,\n",
       " 'части': 332,\n",
       " 'внутри': 333,\n",
       " 'внимание': 334,\n",
       " 'меньше': 335,\n",
       " 'информации': 336,\n",
       " 'женщин': 337,\n",
       " 'благодаря': 338,\n",
       " 'конце': 339,\n",
       " 'котором': 340,\n",
       " 'видим': 341,\n",
       " 'стал': 342,\n",
       " 'необходимо': 343,\n",
       " 'вами': 344,\n",
       " 'сколько': 345,\n",
       " 'хотим': 346,\n",
       " 'мозг': 347,\n",
       " 'нашего': 348,\n",
       " 'быстро': 349,\n",
       " 'стороны': 350,\n",
       " 'делают': 351,\n",
       " 'стоит': 352,\n",
       " 'система': 353,\n",
       " 'ей': 354,\n",
       " 'стать': 355,\n",
       " 'история': 356,\n",
       " 'настолько': 357,\n",
       " 'вроде': 358,\n",
       " 'вообще': 359,\n",
       " 'большой': 360,\n",
       " 'самых': 361,\n",
       " 'людьми': 362,\n",
       " 'означает': 363,\n",
       " 'делает': 364,\n",
       " 'процесс': 365,\n",
       " 'получить': 366,\n",
       " 'рассказать': 367,\n",
       " 'нами': 368,\n",
       " 'всю': 369,\n",
       " 'компании': 370,\n",
       " 'историю': 371,\n",
       " 'женщины': 372,\n",
       " 'последние': 373,\n",
       " 'энергии': 374,\n",
       " 'ним': 375,\n",
       " 'одну': 376,\n",
       " 'ответ': 377,\n",
       " 'разных': 378,\n",
       " 'данных': 379,\n",
       " 'проект': 380,\n",
       " 'кого': 381,\n",
       " 'тысяч': 382,\n",
       " 'работа': 383,\n",
       " 'какие': 384,\n",
       " 'моего': 385,\n",
       " 'становится': 386,\n",
       " 'чём': 387,\n",
       " 'моих': 388,\n",
       " 'дома': 389,\n",
       " 'х': 390,\n",
       " 'которым': 391,\n",
       " 'самый': 392,\n",
       " 'менее': 393,\n",
       " 'деньги': 394,\n",
       " 'такого': 395,\n",
       " 'всей': 396,\n",
       " 'неё': 397,\n",
       " 'раньше': 398,\n",
       " 'полностью': 399,\n",
       " 'города': 400,\n",
       " 'находится': 401,\n",
       " 'обычно': 402,\n",
       " 'решения': 403,\n",
       " 'нью': 404,\n",
       " 'представьте': 405,\n",
       " 'слова': 406,\n",
       " 'помочь': 407,\n",
       " 'возможности': 408,\n",
       " 'говорю': 409,\n",
       " 'правда': 410,\n",
       " 'точно': 411,\n",
       " 'будем': 412,\n",
       " 'начала': 413,\n",
       " 'стало': 414,\n",
       " 'гораздо': 415,\n",
       " 'которого': 416,\n",
       " 'музыка': 417,\n",
       " 'куда': 418,\n",
       " 'представить': 419,\n",
       " 'места': 420,\n",
       " 'сделал': 421,\n",
       " 'воды': 422,\n",
       " 'особенно': 423,\n",
       " 'сказала': 424,\n",
       " 'называется': 425,\n",
       " 'мере': 426,\n",
       " 'должна': 427,\n",
       " 'точки': 428,\n",
       " 'зрения': 429,\n",
       " 'начать': 430,\n",
       " 'должно': 431,\n",
       " 'спустя': 432,\n",
       " 'идеи': 433,\n",
       " 'сможем': 434,\n",
       " 'выглядит': 435,\n",
       " 'посмотрите': 436,\n",
       " 'иметь': 437,\n",
       " 'информацию': 438,\n",
       " 'интересно': 439,\n",
       " 'вещей': 440,\n",
       " 'начал': 441,\n",
       " 'сами': 442,\n",
       " 'среди': 443,\n",
       " 'мной': 444,\n",
       " 'весь': 445,\n",
       " 'однажды': 446,\n",
       " 'наконец': 447,\n",
       " 'туда': 448,\n",
       " 'жить': 449,\n",
       " 'исследования': 450,\n",
       " 'ними': 451,\n",
       " 'нечто': 452,\n",
       " 'стране': 453,\n",
       " 'новый': 454,\n",
       " 'решение': 455,\n",
       " 'намного': 456,\n",
       " 'систему': 457,\n",
       " 'четыре': 458,\n",
       " 'множество': 459,\n",
       " 'первых': 460,\n",
       " 'нравится': 461,\n",
       " 'той': 462,\n",
       " 'произошло': 463,\n",
       " 'делаем': 464,\n",
       " 'говорил': 465,\n",
       " 'думать': 466,\n",
       " 'любой': 467,\n",
       " 'хотели': 468,\n",
       " 'странах': 469,\n",
       " 'дальше': 470,\n",
       " 'посмотреть': 471,\n",
       " 'взять': 472,\n",
       " 'против': 473,\n",
       " 'уровень': 474,\n",
       " 'клетки': 475,\n",
       " 'дать': 476,\n",
       " 'сильно': 477,\n",
       " 'тебя': 478,\n",
       " 'другом': 479,\n",
       " 'второй': 480,\n",
       " 'впервые': 481,\n",
       " 'постоянно': 482,\n",
       " 'являются': 483,\n",
       " 'другими': 484,\n",
       " 'стала': 485,\n",
       " 'земле': 486,\n",
       " 'помощи': 487,\n",
       " 'одним': 488,\n",
       " 'нашу': 489,\n",
       " 'земли': 490,\n",
       " 'друга': 491,\n",
       " 'изменения': 492,\n",
       " 'такая': 493,\n",
       " 'проблем': 494,\n",
       " 'пять': 495,\n",
       " 'буду': 496,\n",
       " 'пути': 497,\n",
       " 'считаю': 498,\n",
       " 'хотят': 499,\n",
       " 'вопросы': 500,\n",
       " 'хотите': 501,\n",
       " 'пару': 502,\n",
       " 'т': 503,\n",
       " 'своё': 504,\n",
       " 'каждого': 505,\n",
       " 'поскольку': 506,\n",
       " 'своим': 507,\n",
       " 'месте': 508,\n",
       " 'кое': 509,\n",
       " 'сам': 510,\n",
       " 'месяцев': 511,\n",
       " 'видели': 512,\n",
       " 'области': 513,\n",
       " 'интернет': 514,\n",
       " 'ко': 515,\n",
       " 'свет': 516,\n",
       " 'слово': 517,\n",
       " 'животных': 518,\n",
       " 'минут': 519,\n",
       " 'модель': 520,\n",
       " 'имеют': 521,\n",
       " 'сказали': 522,\n",
       " 'знать': 523,\n",
       " 'говорили': 524,\n",
       " 'ваш': 525,\n",
       " 'моё': 526,\n",
       " 'путь': 527,\n",
       " 'денег': 528,\n",
       " 'населения': 529,\n",
       " 'кроме': 530,\n",
       " 'зале': 531,\n",
       " 'нужны': 532,\n",
       " 'дня': 533,\n",
       " 'будущее': 534,\n",
       " 'развития': 535,\n",
       " 'решили': 536,\n",
       " 'вид': 537,\n",
       " 'индии': 538,\n",
       " 'надеюсь': 539,\n",
       " 'смогли': 540,\n",
       " 'будто': 541,\n",
       " 'разные': 542,\n",
       " 'легко': 543,\n",
       " 'говоря': 544,\n",
       " 'новых': 545,\n",
       " 'рода': 546,\n",
       " 'днк': 547,\n",
       " 'работают': 548,\n",
       " 'нашем': 549,\n",
       " 'вещь': 550,\n",
       " 'прежде': 551,\n",
       " 'тела': 552,\n",
       " 'нельзя': 553,\n",
       " 'наиболее': 554,\n",
       " 'виду': 555,\n",
       " 'скорее': 556,\n",
       " 'другим': 557,\n",
       " 'позвольте': 558,\n",
       " 'стран': 559,\n",
       " 'большая': 560,\n",
       " 'связи': 561,\n",
       " 'про': 562,\n",
       " 'африке': 563,\n",
       " 'каким': 564,\n",
       " 'миллиардов': 565,\n",
       " 'сети': 566,\n",
       " 'будущем': 567,\n",
       " 'самые': 568,\n",
       " 'узнать': 569,\n",
       " 'рядом': 570,\n",
       " 'руки': 571,\n",
       " 'создания': 572,\n",
       " 'частью': 573,\n",
       " 'итоге': 574,\n",
       " 'некоторых': 575,\n",
       " 'сложно': 576,\n",
       " 'речь': 577,\n",
       " 'сразу': 578,\n",
       " 'знают': 579,\n",
       " 'посмотрим': 580,\n",
       " 'тысячи': 581,\n",
       " 'прошлом': 582,\n",
       " 'несмотря': 583,\n",
       " 'тебе': 584,\n",
       " 'женщина': 585,\n",
       " 'дней': 586,\n",
       " 'причина': 587,\n",
       " 'работе': 588,\n",
       " 'решил': 589,\n",
       " 'могла': 590,\n",
       " 'машины': 591,\n",
       " 'этими': 592,\n",
       " 'знали': 593,\n",
       " 'которыми': 594,\n",
       " 'сначала': 595,\n",
       " 'абсолютно': 596,\n",
       " 'смысле': 597,\n",
       " 'самым': 598,\n",
       " 'процентов': 599,\n",
       " 'пришлось': 600,\n",
       " 'знает': 601,\n",
       " 'далее': 602,\n",
       " 'часов': 603,\n",
       " 'одном': 604,\n",
       " 'недавно': 605,\n",
       " 'фотографии': 606,\n",
       " 'используя': 607,\n",
       " 'понял': 608,\n",
       " 'число': 609,\n",
       " 'создали': 610,\n",
       " 'школу': 611,\n",
       " 'идёт': 612,\n",
       " 'проблему': 613,\n",
       " 'состоит': 614,\n",
       " 'общества': 615,\n",
       " 'позволяет': 616,\n",
       " 'та': 617,\n",
       " 'следует': 618,\n",
       " 'первая': 619,\n",
       " 'чему': 620,\n",
       " 'глаза': 621,\n",
       " 'видов': 622,\n",
       " 'домой': 623,\n",
       " 'м': 624,\n",
       " 'решить': 625,\n",
       " 'создавать': 626,\n",
       " 'войны': 627,\n",
       " 'результате': 628,\n",
       " 'вся': 629,\n",
       " 'имею': 630,\n",
       " 'первое': 631,\n",
       " 'невозможно': 632,\n",
       " 'группы': 633,\n",
       " 'самая': 634,\n",
       " 'результат': 635,\n",
       " 'покажу': 636,\n",
       " 'мою': 637,\n",
       " 'подумал': 638,\n",
       " 'город': 639,\n",
       " 'заключается': 640,\n",
       " 'хотела': 641,\n",
       " 'сюда': 642,\n",
       " 'самой': 643,\n",
       " 'протяжении': 644,\n",
       " 'семьи': 645,\n",
       " 'невероятно': 646,\n",
       " 'образование': 647,\n",
       " 'факт': 648,\n",
       " 'отец': 649,\n",
       " 'ваши': 650,\n",
       " 'технологий': 651,\n",
       " 'следующий': 652,\n",
       " 'уровне': 653,\n",
       " 'типа': 654,\n",
       " 'нём': 655,\n",
       " 'планеты': 656,\n",
       " 'случилось': 657,\n",
       " 'ка': 658,\n",
       " 'опять': 659,\n",
       " 'качестве': 660,\n",
       " 'названием': 661,\n",
       " 'далеко': 662,\n",
       " 'трудно': 663,\n",
       " 'говорим': 664,\n",
       " 'результаты': 665,\n",
       " 'мало': 666,\n",
       " 'станет': 667,\n",
       " 'бизнес': 668,\n",
       " 'кем': 669,\n",
       " 'модели': 670,\n",
       " 'вселенной': 671,\n",
       " 'книги': 672,\n",
       " 'уж': 673,\n",
       " 'сотни': 674,\n",
       " 'знал': 675,\n",
       " 'безопасности': 676,\n",
       " 'увидите': 677,\n",
       " 'практически': 678,\n",
       " 'большие': 679,\n",
       " 'другая': 680,\n",
       " 'сих': 681,\n",
       " 'другому': 682,\n",
       " 'кстати': 683,\n",
       " 'похоже': 684,\n",
       " 'выбор': 685,\n",
       " 'оказывается': 686,\n",
       " 'звук': 687,\n",
       " 'идею': 688,\n",
       " 'дом': 689,\n",
       " 'весьма': 690,\n",
       " 'иначе': 691,\n",
       " 'хотелось': 692,\n",
       " 'движения': 693,\n",
       " 'думаете': 694,\n",
       " 'вероятно': 695,\n",
       " 'равно': 696,\n",
       " 'воду': 697,\n",
       " 'огромное': 698,\n",
       " 'фактически': 699,\n",
       " 'движение': 700,\n",
       " 'язык': 701,\n",
       " 'опыт': 702,\n",
       " 'роль': 703,\n",
       " 'связь': 704,\n",
       " 'правильно': 705,\n",
       " 'планете': 706,\n",
       " 'компьютер': 707,\n",
       " 'столько': 708,\n",
       " 'света': 709,\n",
       " 'другое': 710,\n",
       " 'новое': 711,\n",
       " 'чувство': 712,\n",
       " 'поговорить': 713,\n",
       " 'используют': 714,\n",
       " 'цель': 715,\n",
       " 'концов': 716,\n",
       " 'организации': 717,\n",
       " 'обнаружили': 718,\n",
       " 'даёт': 719,\n",
       " 'очевидно': 720,\n",
       " 'сути': 721,\n",
       " 'годы': 722,\n",
       " 'помощь': 723,\n",
       " 'хочет': 724,\n",
       " 'нужна': 725,\n",
       " 'местах': 726,\n",
       " 'нескольких': 727,\n",
       " 'нашим': 728,\n",
       " 'удалось': 729,\n",
       " 'самого': 730,\n",
       " 'рост': 731,\n",
       " 'живут': 732,\n",
       " 'смысл': 733,\n",
       " 'голос': 734,\n",
       " 'различных': 735,\n",
       " 'существуют': 736,\n",
       " 'взгляд': 737,\n",
       " 'шесть': 738,\n",
       " 'вашей': 739,\n",
       " 'понимаете': 740,\n",
       " 'группа': 741,\n",
       " 'правительство': 742,\n",
       " 'своими': 743,\n",
       " 'получается': 744,\n",
       " 'наверное': 745,\n",
       " 'исследований': 746,\n",
       " 'расскажу': 747,\n",
       " 'технология': 748,\n",
       " 'значение': 749,\n",
       " 'думал': 750,\n",
       " 'вижу': 751,\n",
       " 'используем': 752,\n",
       " 'права': 753,\n",
       " 'смерти': 754,\n",
       " 'оказалось': 755,\n",
       " 'делали': 756,\n",
       " 'видно': 757,\n",
       " 'мама': 758,\n",
       " 'чуть': 759,\n",
       " 'школе': 760,\n",
       " 'основном': 761,\n",
       " 'общество': 762,\n",
       " 'какое': 763,\n",
       " 'вдруг': 764,\n",
       " 'видел': 765,\n",
       " 'тело': 766,\n",
       " 'быстрее': 767,\n",
       " 'программы': 768,\n",
       " 'люблю': 769,\n",
       " 'находятся': 770,\n",
       " 'другого': 771,\n",
       " 'такую': 772,\n",
       " 'образования': 773,\n",
       " 'слышали': 774,\n",
       " 'смотреть': 775,\n",
       " 'выше': 776,\n",
       " 'века': 777,\n",
       " 'кому': 778,\n",
       " 'новой': 779,\n",
       " 'игры': 780,\n",
       " 'отношения': 781,\n",
       " 'исследование': 782,\n",
       " 'уровня': 783,\n",
       " 'настоящему': 784,\n",
       " 'е': 785,\n",
       " 'позже': 786,\n",
       " 'откуда': 787,\n",
       " 'школы': 788,\n",
       " 'силы': 789,\n",
       " 'буквально': 790,\n",
       " 'маленькие': 791,\n",
       " 'показывает': 792,\n",
       " 'моим': 793,\n",
       " 'последний': 794,\n",
       " 'работал': 795,\n",
       " 'подход': 796,\n",
       " 'начале': 797,\n",
       " 'мест': 798,\n",
       " 'нашли': 799,\n",
       " 'поверхности': 800,\n",
       " 'вода': 801,\n",
       " 'системе': 802,\n",
       " 'какая': 803,\n",
       " 'трёх': 804,\n",
       " 'приходится': 805,\n",
       " 'лаборатории': 806,\n",
       " 'основе': 807,\n",
       " 'природы': 808,\n",
       " 'словами': 809,\n",
       " 'сможет': 810,\n",
       " 'фильм': 811,\n",
       " 'клеток': 812,\n",
       " 'уверен': 813,\n",
       " 'сторону': 814,\n",
       " 'пространство': 815,\n",
       " 'метров': 816,\n",
       " 'степени': 817,\n",
       " 'поняли': 818,\n",
       " 'мужчины': 819,\n",
       " 'ситуации': 820,\n",
       " 'способность': 821,\n",
       " 'становятся': 822,\n",
       " 'обратно': 823,\n",
       " 'известно': 824,\n",
       " 'подумать': 825,\n",
       " 'вернуться': 826,\n",
       " 'получили': 827,\n",
       " 'спросил': 828,\n",
       " 'увидел': 829,\n",
       " 'дела': 830,\n",
       " 'здания': 831,\n",
       " 'понимать': 832,\n",
       " 'окружающей': 833,\n",
       " 'болезни': 834,\n",
       " 'задача': 835,\n",
       " 'америке': 836,\n",
       " 'раза': 837,\n",
       " 'верю': 838,\n",
       " 'пришли': 839,\n",
       " 'одновременно': 840,\n",
       " 'нужен': 841,\n",
       " 'начинает': 842,\n",
       " 'человеком': 843,\n",
       " 'отлично': 844,\n",
       " 'формы': 845,\n",
       " 'среды': 846,\n",
       " 'суть': 847,\n",
       " 'африки': 848,\n",
       " 'центре': 849,\n",
       " 'ребёнка': 850,\n",
       " 'думают': 851,\n",
       " 'идей': 852,\n",
       " 'первой': 853,\n",
       " 'каждая': 854,\n",
       " 'моему': 855,\n",
       " 'получил': 856,\n",
       " 'большую': 857,\n",
       " 'каждой': 858,\n",
       " 'вашего': 859,\n",
       " 'общем': 860,\n",
       " 'крайней': 861,\n",
       " 'период': 862,\n",
       " 'состоянии': 863,\n",
       " 'будете': 864,\n",
       " 'думаем': 865,\n",
       " 'рака': 866,\n",
       " 'маленький': 867,\n",
       " 'имени': 868,\n",
       " 'поведение': 869,\n",
       " 'зачем': 870,\n",
       " 'цели': 871,\n",
       " 'виде': 872,\n",
       " 'готовы': 873,\n",
       " 'сеть': 874,\n",
       " 'называем': 875,\n",
       " 'собираюсь': 876,\n",
       " 'дизайн': 877,\n",
       " 'сама': 878,\n",
       " 'помню': 879,\n",
       " 'верно': 880,\n",
       " 'ту': 881,\n",
       " 'компания': 882,\n",
       " 'миллиона': 883,\n",
       " 'доступ': 884,\n",
       " 'построить': 885,\n",
       " 'своём': 886,\n",
       " 'родители': 887,\n",
       " 'плохо': 888,\n",
       " 'го': 889,\n",
       " 'удивительно': 890,\n",
       " 'мужчин': 891,\n",
       " 'информация': 892,\n",
       " 'руку': 893,\n",
       " 'изображение': 894,\n",
       " 'начало': 895,\n",
       " 'смотрите': 896,\n",
       " 'действительности': 897,\n",
       " 'вич': 898,\n",
       " 'такими': 899,\n",
       " 'здание': 900,\n",
       " 'климата': 901,\n",
       " 'поделиться': 902,\n",
       " 'касается': 903,\n",
       " 'ответил': 904,\n",
       " 'друзей': 905,\n",
       " 'начинают': 906,\n",
       " 'ваше': 907,\n",
       " 'всём': 908,\n",
       " 'землю': 909,\n",
       " 'смог': 910,\n",
       " 'первые': 911,\n",
       " 'проекта': 912,\n",
       " 'виды': 913,\n",
       " 'мысль': 914,\n",
       " 'мужчина': 915,\n",
       " 'долго': 916,\n",
       " 'работали': 917,\n",
       " 'шаг': 918,\n",
       " 'ради': 919,\n",
       " 'угодно': 920,\n",
       " 'объяснить': 921,\n",
       " 'вниз': 922,\n",
       " 'новую': 923,\n",
       " 'принять': 924,\n",
       " 'пожалуйста': 925,\n",
       " 'парень': 926,\n",
       " 'ранее': 927,\n",
       " 'больших': 928,\n",
       " 'здорово': 929,\n",
       " 'действия': 930,\n",
       " 'ребёнок': 931,\n",
       " 'таком': 932,\n",
       " 'силу': 933,\n",
       " 'процессе': 934,\n",
       " 'сожалению': 935,\n",
       " 'смогут': 936,\n",
       " 'первую': 937,\n",
       " 'часа': 938,\n",
       " 'думали': 939,\n",
       " 'будучи': 940,\n",
       " 'страна': 941,\n",
       " 'устройство': 942,\n",
       " 'развитие': 943,\n",
       " 'голову': 944,\n",
       " 'вверх': 945,\n",
       " 'считают': 946,\n",
       " 'проблемой': 947,\n",
       " 'здравоохранения': 948,\n",
       " 'играть': 949,\n",
       " 'неделю': 950,\n",
       " 'вполне': 951,\n",
       " 'ваша': 952,\n",
       " 'написал': 953,\n",
       " 'мать': 954,\n",
       " 'недель': 955,\n",
       " 'заниматься': 956,\n",
       " 'делаю': 957,\n",
       " 'возможностей': 958,\n",
       " 'пациентов': 959,\n",
       " 'каждом': 960,\n",
       " 'д': 961,\n",
       " 'мысли': 962,\n",
       " 'власти': 963,\n",
       " 'центр': 964,\n",
       " 'многое': 965,\n",
       " 'пытаемся': 966,\n",
       " 'правительства': 967,\n",
       " 'китай': 968,\n",
       " 'вперёд': 969,\n",
       " 'другую': 970,\n",
       " 'эксперимент': 971,\n",
       " 'искусства': 972,\n",
       " 'имя': 973,\n",
       " 'фотография': 974,\n",
       " 'придётся': 975,\n",
       " 'слева': 976,\n",
       " 'сможете': 977,\n",
       " 'прекрасно': 978,\n",
       " 'целью': 979,\n",
       " 'условиях': 980,\n",
       " 'новая': 981,\n",
       " 'детям': 982,\n",
       " 'реальность': 983,\n",
       " 'таки': 984,\n",
       " 'сила': 985,\n",
       " 'машина': 986,\n",
       " 'простой': 987,\n",
       " 'лица': 988,\n",
       " 'книгу': 989,\n",
       " 'включая': 990,\n",
       " 'использования': 991,\n",
       " 'помните': 992,\n",
       " 'сердце': 993,\n",
       " 'искусство': 994,\n",
       " 'миллионы': 995,\n",
       " 'казалось': 996,\n",
       " 'экономики': 997,\n",
       " 'рак': 998,\n",
       " 'заставить': 999,\n",
       " 'хорошие': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Here is an example for a samples number 1:\")\n",
    "print(\"Source('en') example:\", id_to_text(dataset['x'][0], dataset['x_tk']))\n",
    "print(\"Target('ru') example:\", id_to_text(dataset['y'][0], dataset['y_tk']))\n",
    "print(\" \")\n",
    "print(\"Samples number 2:\")\n",
    "print(\"Source('en') example:\", id_to_text(dataset['x'][1], dataset['x_tk']))\n",
    "print(\"Target('ru') example:\", id_to_text(dataset['y'][1], dataset['y_tk']))\n",
    "print(\"source vocabulary size:\", dataset['x'].max())\n",
    "print(\"target vocabulary size:\", dataset['y'].max())\n",
    "dataset['y_tk'].word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models\n",
    "\n",
    "The models are implemented with a similar set of parameters. The main idea is to keep models as small and simple as possible to quickly train them and validate the difference the primarely derived from model architectures. The summary of main hyper parameters presented below:\n",
    "\n",
    "* Mapping:\n",
    "    - Embeddings - word indices will be mapped into a 16-dimentional space\n",
    "    - Dense mapping - recurrence outputs mapped into the target-language space, represented with OHE, via Dense mapping\n",
    "* Layers:\n",
    "    - GRU - number of units 128\n",
    "    - Bidirectional GRU - number of untis set up to 64 in order to keep the total number of units the same (128)\n",
    "    - Batch Normalization - To speed up the training batch normalization is inserted after embeddings and before dense mapping\n",
    "* Optimization:\n",
    "    - Adam - all models trained with Adam optimizer and the same learning rate (1e-3)\n",
    "* Loss function:\n",
    "    - sparse_categorical_crossentropy_from_logits - keras.losses.sparse_categorical_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = int(dataset['x'].shape[0] * 0.30)\n",
    "learning_rate = 1e-3\n",
    "embeddings_units = 16\n",
    "gru_units = 128\n",
    "epochs = 10\n",
    "validation_split = 0.1\n",
    "sparse_categorical_crossentropy_from_logits = partial(keras.losses.sparse_categorical_crossentropy, from_logits=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model list:**\n",
    "\n",
    "1. Embedded GRU\n",
    "2. Embedded Bidirectional GRU\n",
    "3. Embedded GRU encoder-decoder model\n",
    "4. Embedded GRU encoder-decoder model with Multiplicative Attention\n",
    "\n",
    "#### Model 1 - Embedded GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model summary:\n",
      "Model: \"model_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_9 (InputLayer)         [(None, 113)]             0         \n",
      "_________________________________________________________________\n",
      "embedding_7 (Embedding)      (None, 113, 16)           816640    \n",
      "_________________________________________________________________\n",
      "unified_gru_7 (UnifiedGRU)   (None, 113, 128)          56064     \n",
      "_________________________________________________________________\n",
      "time_distributed_6 (TimeDist (None, 113, 155245)       20026605  \n",
      "=================================================================\n",
      "Total params: 20,899,309\n",
      "Trainable params: 20,899,309\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 58964 samples, validate on 6552 samples\n",
      "Epoch 1/10\n",
      "  128/58964 [..............................] - ETA: 4:28:25 - loss: 5.6606 - accuracy: 0.6659   "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-124-a8cff5b8db55>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Model summary:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0membed_rnn_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0membed_rnn_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'x'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZeroPadding1D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'x'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'y'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'y'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;31m# Print prediction(s)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits_to_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membed_rnn_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'x'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'y_tk'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/env/translation-tf2/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    871\u001b[0m           \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m           \u001b[0mvalidation_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m           steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m    874\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/env/translation-tf2/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/env/translation-tf2/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3215\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3216\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3217\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3218\u001b[0m     return nest.pack_sequence_as(self._outputs_structure,\n\u001b[1;32m   3219\u001b[0m                                  [x.numpy() for x in outputs])\n",
      "\u001b[0;32m~/env/translation-tf2/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    556\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[1;32m    557\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[0;32m--> 558\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    559\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/env/translation-tf2/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    625\u001b[0m     \u001b[0;31m# Only need to override the gradient in graph mode and when we have outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 627\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    628\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_register_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/env/translation-tf2/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args)\u001b[0m\n\u001b[1;32m    413\u001b[0m             attrs=(\"executor_type\", executor_type,\n\u001b[1;32m    414\u001b[0m                    \"config_proto\", config),\n\u001b[0;32m--> 415\u001b[0;31m             ctx=ctx)\n\u001b[0m\u001b[1;32m    416\u001b[0m       \u001b[0;31m# Replace empty list with None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/env/translation-tf2/lib/python3.6/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     59\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def embedded_gru_model(input_shape, output_sequence_length, source_vocab_size, target_vocab_size):\n",
    "    \"\"\"\n",
    "    Build and train a RNN model using word embedding on x and y\n",
    "    :param input_shape: Tuple of input shape\n",
    "    :param output_sequence_length: Length of output sequence\n",
    "    :param english_vocab_size: Number of unique English words in the dataset\n",
    "    :param french_vocab_size: Number of unique French words in the dataset\n",
    "    :return: Keras model built, but not trained\n",
    "    \"\"\"\n",
    "    input_seq = keras.Input(input_shape[1:])\n",
    "    embedded_seq = keras.layers.Embedding(source_vocab_size, embeddings_units, input_length=output_sequence_length)(input_seq)\n",
    "    rnn = keras.layers.GRU(gru_units, return_sequences=True)(embedded_seq)\n",
    "    logits = keras.layers.TimeDistributed(keras.layers.Dense(target_vocab_size))(rnn)\n",
    "    model = keras.Model(input_seq, logits)\n",
    "    model.compile(loss=keras.losses.sparse_categorical_crossentropy,\n",
    "                  optimizer=keras.optimizers.Adam(learning_rate),\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Train the neural network\n",
    "embed_rnn_model = embedded_gru_model(\n",
    "    dataset['x'].shape,\n",
    "    dataset['y'].shape[1],\n",
    "    len(dataset['x_tk'].word_index)+1,\n",
    "    len(dataset['y_tk'].word_index)+1)\n",
    "print(\"Model summary:\")\n",
    "embed_rnn_model.summary()\n",
    "embed_rnn_model.fit(dataset['x'][:n_samples], \n",
    "                    keras.layers.ZeroPadding1D((0, dataset['x'].shape[1]-dataset['y'].shape[1]))(dataset['y'][:n_samples][:,:,None]), \n",
    "                    batch_size=512, \n",
    "                    epochs=epochs, \n",
    "                    validation_split=validation_split)\n",
    "# Print prediction(s)\n",
    "print(logits_to_text(embed_rnn_model.predict(dataset['x'][-2:])[0], dataset['y_tk']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
