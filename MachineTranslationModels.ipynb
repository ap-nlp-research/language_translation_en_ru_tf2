{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MachineTranslationModels.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ap-nlp-research/language_translation_en_ru_tf2/blob/master/MachineTranslationModels.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17OD4xTkRxeL",
        "colab_type": "text"
      },
      "source": [
        "# Machine Translation Project\n",
        "\n",
        "\n",
        "The goal of the project is to compare the strength of the following recurrent models:\n",
        "\n",
        "1. Embedded GRU\n",
        "2. Embedded Bidirectional GRU\n",
        "3. Embedded GRU encoder-decoder model\n",
        "4. Embedded GRU encoder-decoder model with Multiplicative Attention\n",
        "\n",
        "The models implemented in Tensorflow 2.0 with Keras as a high-level API. Models are trained and analyzed based on EN-RU [wmt19_translate dataset](https://www.tensorflow.org/datasets/datasets#wmt19_translate) ([ACL 2019 FOURTH CONFERENCE ON MACHINE TRANSLATION (WMT19)](http://www.statmt.org/wmt19/translation-task.html))."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j3gfCUyDRxeN",
        "colab_type": "code",
        "outputId": "d79afe55-9df4-4ab9-f9b0-a0f2d4823046",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        }
      },
      "source": [
        "!pip install tensorflow-gpu==2.0.0-alpha0"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow-gpu==2.0.0-alpha0 in /usr/local/lib/python3.6/dist-packages (2.0.0a0)\n",
            "Requirement already satisfied: tb-nightly<1.14.0a20190302,>=1.14.0a20190301 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.14.0a20190301)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.12.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.0.9)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.1.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (0.1.6)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (0.7.1)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.0.7)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (0.7.1)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.16.3)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (0.33.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.15.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (3.7.1)\n",
            "Requirement already satisfied: tf-estimator-nightly<1.14.0.dev2019030116,>=1.14.0.dev2019030115 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.14.0.dev2019030115)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (0.2.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190302,>=1.14.0a20190301->tensorflow-gpu==2.0.0-alpha0) (0.15.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190302,>=1.14.0a20190301->tensorflow-gpu==2.0.0-alpha0) (3.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow-gpu==2.0.0-alpha0) (2.8.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow-gpu==2.0.0-alpha0) (41.0.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ecoqx4K1RxeT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import subprocess\n",
        "import re\n",
        "import numpy as np\n",
        "from functools import partial\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "myYPpU6bRxeX",
        "colab_type": "text"
      },
      "source": [
        "## Data ETL\n",
        "\n",
        "The data load, extraction, and transformation is done with data_etl() method. This method returns a dictionary containing source data stored under 'x' label. Target data is stored under 'y' label. In addition to the source and target data, the dictionary contains x and y tockenizers (stored as 'x_tk' and 'y_tk'):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S6MZuTfdRxeY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def data_etl(download_dir: str = \".\", file_name: str = \"en_ru.tgz\", n_lines: int = 3000,\n",
        "             lenght_lim_lower: int = 4, length_lim_upper: int = None, num_words: int = 5000) -> dict:\n",
        "    path = os.path.join(download_dir, file_name)\n",
        "    en_path = os.path.join(download_dir, \"paracrawl-release1.en-ru.zipporah0-dedup-clean.en\")\n",
        "    ru_path = os.path.join(download_dir, \"paracrawl-release1.en-ru.zipporah0-dedup-clean.ru\")\n",
        "\n",
        "    print(\"Start data ETL\")\n",
        "\n",
        "    if os.path.isfile(path):\n",
        "        print(\"Reuse pre-downloaded \" + path)\n",
        "        # extract\n",
        "        subprocess.run(\"tar --extract --file\".split() + [path])\n",
        "    else:\n",
        "        # download a dataset\n",
        "        print(\"Start downloading\")\n",
        "        completed = subprocess.run(\n",
        "                [\"curl\",\n",
        "                 \"https://s3.amazonaws.com/web-language-models/paracrawl/release1/paracrawl-release1.en-ru.zipporah0-dedup-clean.tgz\",\n",
        "                 \"--output\", \n",
        "                 path]\n",
        "        )\n",
        "        if completed.check_returncode() is not None:\n",
        "            print(\"Downloading returned error code:\", completed.check_returncode())\n",
        "    # extract\n",
        "    if not (os.path.isfile(en_path) and os.path.isfile(ru_path)):\n",
        "        subprocess.run(\"tar --extract --file\".split() + [path])\n",
        "    print(\"Data downloaded and extracted\")\n",
        "\n",
        "    # read data\n",
        "    en = read_lines(en_path, n_lines)\n",
        "    ru = read_lines(ru_path, n_lines)\n",
        "\n",
        "    print(\"Extracted successfully\")\n",
        "    # slit numbers\n",
        "    en = [re.sub(\"[0-9]\", \" \\g<0>\", s) for s in en]\n",
        "    ru = [re.sub(\"[0-9]\", \" \\g<0>\", s) for s in ru]\n",
        "    \n",
        "    if length_lim_upper is None:\n",
        "      # remove extra long sequence as they will skew the loss towards 0\n",
        "      lengths = np.array([len(s.split()) for s in en])\n",
        "      l_mean = lengths.mean()\n",
        "      l_std = lengths.mean()\n",
        "      length_lim_upper = int(l_mean + l_std*2)\n",
        "      \n",
        "\n",
        "    # filter out small samples\n",
        "    indices = list(filter(lambda idx: length_lim_upper > len(en[idx].split()) >=lenght_lim_lower, range(len(en))))\n",
        "    en = [en[idx] for idx in indices]\n",
        "    ru = [ru[idx] for idx in indices]\n",
        "\n",
        "    # Tockenize\n",
        "    x, x_tk = tokenize(en, num_words=num_words, filters_regex=None)\n",
        "    y, y_tk = tokenize(ru, num_words=num_words * 3, filters_regex=None)\n",
        "\n",
        "    x = pad(x)\n",
        "    y_length = max([len(sentence) for sentence in y])\n",
        "    y = pad(y, length=x.shape[1] if x.shape[1]>y_length else y_length)\n",
        "\n",
        "    print(\"Transformed successfully\")\n",
        "\n",
        "    return {'x': x, 'y': y, 'x_tk': x_tk, 'y_tk': y_tk}\n",
        "\n",
        "\n",
        "def read_lines(from_file: str, n_lines: int) -> list:\n",
        "    if not os.path.isfile(from_file):\n",
        "        raise FileNotFoundError(from_file)\n",
        "\n",
        "    lines: list = []\n",
        "    with open(from_file, \"r\") as f:\n",
        "        for idx, line in enumerate(f):\n",
        "            if idx < n_lines:\n",
        "                lines.append(line)\n",
        "            else:\n",
        "                break\n",
        "\n",
        "    return lines\n",
        "\n",
        "\n",
        "def tokenize(x: list, num_words: int = 5000, filters_regex: str = None):\n",
        "    \"\"\"\n",
        "    Tokenize x\n",
        "    :param x: List of sentences/strings to be tokenized\n",
        "    :param num_words: maximum number of words in the vocabulary - rare words will be replaced with a [rare] token\n",
        "    :param filters_regex: regular expression filter string such as [^a-zA-Z0-9/-]\n",
        "    :return: Tuple of (tokenized x data, tokenizer used to tokenize x)\n",
        "    \"\"\"\n",
        "    if filters_regex is not None:\n",
        "        x_tk = keras.preprocessing.text.Tokenizer(num_words=num_words, filters=filters_regex, oov_token=\"[rare]\")\n",
        "    else:\n",
        "        x_tk = keras.preprocessing.text.Tokenizer(num_words=num_words, oov_token=\"[rare]\")\n",
        "\n",
        "    x_tk.fit_on_texts(x)\n",
        "    return x_tk.texts_to_sequences(x), x_tk\n",
        "\n",
        "\n",
        "def pad(x, length=None) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Pad x\n",
        "    :param x: List of sequences.\n",
        "    :param length: Length to pad the sequence to.  If None, use length of longest sequence in x.\n",
        "    :return: Padded numpy array of sequences\n",
        "    \"\"\"\n",
        "    if length is None:\n",
        "        length = max([len(sentence) for sentence in x])\n",
        "\n",
        "    return keras.preprocessing.sequence.pad_sequences(x, maxlen=length, padding='post')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6-C7QMFRxea",
        "colab_type": "text"
      },
      "source": [
        "dataset = data_etl()\n",
        "\n",
        "{\n",
        "    'x': np.ndarray,\n",
        "    'y': np.ndarray,\n",
        "    'x_tk': keras.preprocessing.text.Tokenizer,\n",
        "    'y_tk': keras.preprocessing.text.Tokenizer\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hc1RXHOURxeb",
        "colab_type": "code",
        "outputId": "98cde311-3cf0-4342-f5b0-8cb2f737152f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "dataset = data_etl(download_dir=\".\", n_lines=5000, num_words=3000)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start data ETL\n",
            "Reuse pre-downloaded ./en_ru.tgz\n",
            "Data downloaded and extracted\n",
            "Extracted successfully\n",
            "Transformed successfully\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tj5lJy93Rxeg",
        "colab_type": "text"
      },
      "source": [
        "## Utility Functions\n",
        "\n",
        "In addition to the data ETL, the code below provides two additional functions for converting logits into word indicies and converting word indicies into text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tjrUMAOPRxeh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def logits_to_id(logits):\n",
        "    \"\"\"\n",
        "    Turns logits into word ids\n",
        "    :param logits: Logits from a neural network\n",
        "    \"\"\"\n",
        "    return np.argmax(logits, 1)\n",
        "\n",
        "def id_to_text(idx, tokenizer):\n",
        "    \"\"\"\n",
        "    Turns id into text using the tokenizer\n",
        "    :param idx: word id\n",
        "    :param tokenizer: Keras Tokenizer fit on the labels\n",
        "    :return: String that represents the text of the logits\n",
        "    \"\"\"\n",
        "    index_to_words = {id: word for word, id in tokenizer.word_index.items()}\n",
        "    index_to_words[0] = '<PAD>'\n",
        "\n",
        "    return ' '.join([index_to_words[prediction] for prediction in idx]).replace(\" <PAD>\", \"\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VE45jscXRxek",
        "colab_type": "code",
        "outputId": "9ff8edde-1669-4c0f-92e5-83ac9e92b1b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        }
      },
      "source": [
        "print(\"Here is an example for a samples number 1:\")\n",
        "print(\"Source('en') example:\", id_to_text(dataset['x'][0], dataset['x_tk']))\n",
        "print(\"Target('ru') example:\", id_to_text(dataset['y'][0], dataset['y_tk']))\n",
        "print(\" \")\n",
        "print(\"Samples number 2:\")\n",
        "print(\"Source('en') example:\", id_to_text(dataset['x'][-10], dataset['x_tk']))\n",
        "print(\"Target('ru') example:\", id_to_text(dataset['y'][-10], dataset['y_tk']))\n",
        "print(\"source vocabulary size:\", dataset['x'].max())\n",
        "print(\"target vocabulary size:\", dataset['y'].max())\n",
        "print(\"Source shape:\", dataset['x'].shape)\n",
        "print(\"Target shape:\", dataset['y'].shape)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Here is an example for a samples number 1:\n",
            "Source('en') example: the company has been registered with the [rare] court in prague in section b file 1 4 8 5 7\n",
            "Target('ru') example: фирма зарегистрирована в городском суде в г праге раздел б вкладыш 1 4 8 5 7\n",
            " \n",
            "Samples number 2:\n",
            "Source('en') example: pontoon floating platforms are quick to [rare] resistant to [rare] maintenance free and a good value and safe alternative to traditional wooden platforms\n",
            "Target('ru') example: понтонные плавучие пирсы [rare] [rare] установки [rare] к природным воздействиям [rare] и [rare] эксплуатации являются [rare] и безопасной [rare] традиционным деревянным [rare]\n",
            "source vocabulary size: 2999\n",
            "target vocabulary size: 8999\n",
            "Source shape: (3602, 65)\n",
            "Target shape: (3602, 99)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2b8qgl5Rxen",
        "colab_type": "text"
      },
      "source": [
        "## Models\n",
        "\n",
        "The models are implemented with a similar set of parameters. The main idea is to keep models as small and simple as possible to quickly train them and validate the difference the primarely derived from model architectures. The summary of main hyper parameters presented below:\n",
        "\n",
        "* Mapping:\n",
        "    - Embeddings - word indices will be mapped into a 16-dimentional space\n",
        "    - Dense mapping - recurrence outputs mapped into the target-language space, represented with OHE, via Dense mapping\n",
        "* Layers:\n",
        "    - GRU - number of units 256\n",
        "    - Bidirectional GRU - number of untis set up to 128 in order to keep the total number of units the same (256)\n",
        "    - Batch Normalization - To speed up the training batch normalization is inserted after embeddings and before dense mapping\n",
        "* Optimization:\n",
        "    - Adam - all models trained with Adam optimizer and the same learning rate (1e-3)\n",
        "* Loss function:\n",
        "    - sparse_categorical_crossentropy_from_logits - keras.losses.sparse_categorical_crossentropy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ZKvPDCFRxen",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learning_rate = 1e-3\n",
        "embeddings_units = 32\n",
        "gru_units = 256\n",
        "epochs = 60\n",
        "validation_split = 0.1\n",
        "batch_size = 256\n",
        "loss = keras.losses.sparse_categorical_crossentropy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1d7e-xH1Rxeq",
        "colab_type": "text"
      },
      "source": [
        "**Model list:**\n",
        "\n",
        "1. Embedded GRU\n",
        "2. Embedded Bidirectional GRU\n",
        "3. Embedded GRU encoder-decoder model\n",
        "4. Embedded GRU encoder-decoder model with Multiplicative Attention\n",
        "\n",
        "#### Model 1 - Embedded GRU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l4QhEHNaRxer",
        "colab_type": "code",
        "outputId": "11c96c20-85d0-43ac-93a9-bf9c4b43753a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        }
      },
      "source": [
        "def embedded_gru_model(input_shape, output_sequence_length, source_vocab_size, target_vocab_size):\n",
        "    \"\"\"\n",
        "    Build and train a RNN model using word embedding on x and y\n",
        "    :param input_shape: Tuple of input shape\n",
        "    :param output_sequence_length: Length of output sequence\n",
        "    :param english_vocab_size: Number of unique English words in the dataset\n",
        "    :param french_vocab_size: Number of unique French words in the dataset\n",
        "    :return: Keras model built, but not trained\n",
        "    \"\"\"\n",
        "    input_seq = keras.Input(input_shape[1:])\n",
        "    if output_sequence_length>input_shape[1]:\n",
        "        expanded_seq = keras.backend.squeeze(\n",
        "            keras.layers.ZeroPadding1D((0, output_sequence_length-input_shape[1]))(\n",
        "                keras.layers.Reshape((input_shape[1], 1))(input_seq)\n",
        "            ),\n",
        "            axis = -1\n",
        "        )\n",
        "    else:\n",
        "        expanded_seq = input_seq\n",
        "    embedded_seq = keras.layers.TimeDistributed(keras.layers.BatchNormalization(axis=-1))(\n",
        "        keras.layers.Embedding(source_vocab_size, embeddings_units, input_length=output_sequence_length)(expanded_seq)\n",
        "    )\n",
        "    rnn = keras.layers.TimeDistributed(keras.layers.BatchNormalization(axis=-1))(\n",
        "        keras.layers.GRU(int(gru_units), return_sequences=True)(embedded_seq)\n",
        "    )\n",
        "    logits = keras.layers.TimeDistributed(keras.layers.BatchNormalization(axis=-1))(\n",
        "        keras.layers.TimeDistributed(keras.layers.Dense(4*gru_units, activation='elu'))(rnn)\n",
        "    )\n",
        "    probabilities = keras.layers.TimeDistributed(keras.layers.Dense(target_vocab_size, activation='softmax'))(logits)\n",
        "    \n",
        "    model = keras.Model(input_seq, probabilities)\n",
        "    \n",
        "    model.compile(loss=loss,\n",
        "                  optimizer=keras.optimizers.Adam(learning_rate, clipnorm=3.0),\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "  \n",
        "# Train the neural network\n",
        "embed_rnn_model = embedded_gru_model(\n",
        "    dataset['x'].shape,\n",
        "    dataset['y'].shape[1],\n",
        "    dataset['x'].max()+1,\n",
        "    dataset['y'].max()+1)\n",
        "\n",
        "\n",
        "\n",
        "print(\"Model summary:\")\n",
        "embed_rnn_model.summary()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model summary:\n",
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 65)]              0         \n",
            "_________________________________________________________________\n",
            "reshape (Reshape)            (None, 65, 1)             0         \n",
            "_________________________________________________________________\n",
            "zero_padding1d (ZeroPadding1 (None, 99, 1)             0         \n",
            "_________________________________________________________________\n",
            "Squeeze (TensorFlowOpLayer)  [(None, 99)]              0         \n",
            "_________________________________________________________________\n",
            "embedding (Embedding)        (None, 99, 32)            96000     \n",
            "_________________________________________________________________\n",
            "time_distributed (TimeDistri (None, 99, 32)            128       \n",
            "_________________________________________________________________\n",
            "unified_gru (UnifiedGRU)     (None, 99, 256)           222720    \n",
            "_________________________________________________________________\n",
            "time_distributed_1 (TimeDist (None, 99, 256)           1024      \n",
            "_________________________________________________________________\n",
            "time_distributed_3 (TimeDist (None, 99, 1024)          263168    \n",
            "_________________________________________________________________\n",
            "time_distributed_2 (TimeDist (None, 99, 1024)          4096      \n",
            "_________________________________________________________________\n",
            "time_distributed_4 (TimeDist (None, 99, 9000)          9225000   \n",
            "=================================================================\n",
            "Total params: 9,812,136\n",
            "Trainable params: 9,809,512\n",
            "Non-trainable params: 2,624\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48rVQw-ch2ek",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2091
        },
        "outputId": "e4d3b214-16d5-44b4-df77-dad02310deb0"
      },
      "source": [
        "embed_rnn_model.fit(\n",
        "    dataset['x'], \n",
        "    dataset['y'][:,:, None], \n",
        "    batch_size=batch_size, \n",
        "    epochs=epochs, \n",
        "    validation_split=validation_split)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 3241 samples, validate on 361 samples\n",
            "Epoch 1/60\n",
            "3241/3241 [==============================] - 11s 3ms/sample - loss: 4.4764 - accuracy: 0.7343 - val_loss: 7.6642 - val_accuracy: 0.8467\n",
            "Epoch 2/60\n",
            "3241/3241 [==============================] - 8s 3ms/sample - loss: 1.6774 - accuracy: 0.8367 - val_loss: 6.6394 - val_accuracy: 0.8467\n",
            "Epoch 3/60\n",
            "3241/3241 [==============================] - 9s 3ms/sample - loss: 1.5308 - accuracy: 0.8473 - val_loss: 6.4128 - val_accuracy: 0.8467\n",
            "Epoch 4/60\n",
            "3241/3241 [==============================] - 9s 3ms/sample - loss: 1.4444 - accuracy: 0.8512 - val_loss: 6.7235 - val_accuracy: 0.8467\n",
            "Epoch 5/60\n",
            "3241/3241 [==============================] - 8s 3ms/sample - loss: 1.3551 - accuracy: 0.8553 - val_loss: 6.9678 - val_accuracy: 0.8467\n",
            "Epoch 6/60\n",
            "3241/3241 [==============================] - 8s 3ms/sample - loss: 1.2603 - accuracy: 0.8598 - val_loss: 7.2673 - val_accuracy: 0.8467\n",
            "Epoch 7/60\n",
            "3241/3241 [==============================] - 8s 3ms/sample - loss: 1.1726 - accuracy: 0.8645 - val_loss: 7.4931 - val_accuracy: 0.8466\n",
            "Epoch 8/60\n",
            "3241/3241 [==============================] - 8s 3ms/sample - loss: 1.1031 - accuracy: 0.8695 - val_loss: 7.6578 - val_accuracy: 0.8456\n",
            "Epoch 9/60\n",
            "3241/3241 [==============================] - 8s 3ms/sample - loss: 1.0358 - accuracy: 0.8755 - val_loss: 7.8123 - val_accuracy: 0.8302\n",
            "Epoch 10/60\n",
            "3241/3241 [==============================] - 8s 3ms/sample - loss: 0.9812 - accuracy: 0.8811 - val_loss: 8.0435 - val_accuracy: 0.8171\n",
            "Epoch 11/60\n",
            "3241/3241 [==============================] - 8s 3ms/sample - loss: 0.9369 - accuracy: 0.8866 - val_loss: 8.3805 - val_accuracy: 0.8146\n",
            "Epoch 12/60\n",
            "3241/3241 [==============================] - 8s 3ms/sample - loss: 0.9011 - accuracy: 0.8915 - val_loss: 8.7816 - val_accuracy: 3.0779e-04\n",
            "Epoch 13/60\n",
            "3241/3241 [==============================] - 8s 3ms/sample - loss: 0.8667 - accuracy: 0.8958 - val_loss: 9.2693 - val_accuracy: 9.7932e-04\n",
            "Epoch 14/60\n",
            "3241/3241 [==============================] - 8s 3ms/sample - loss: 0.8388 - accuracy: 0.8995 - val_loss: 9.7789 - val_accuracy: 0.0019\n",
            "Epoch 15/60\n",
            "3241/3241 [==============================] - 9s 3ms/sample - loss: 0.8184 - accuracy: 0.9020 - val_loss: 10.3864 - val_accuracy: 0.0018\n",
            "Epoch 16/60\n",
            "3241/3241 [==============================] - 9s 3ms/sample - loss: 0.8008 - accuracy: 0.9047 - val_loss: 10.8504 - val_accuracy: 0.0022\n",
            "Epoch 17/60\n",
            "3241/3241 [==============================] - 9s 3ms/sample - loss: 0.7813 - accuracy: 0.9067 - val_loss: 11.4091 - val_accuracy: 0.0022\n",
            "Epoch 18/60\n",
            "3241/3241 [==============================] - 9s 3ms/sample - loss: 0.7640 - accuracy: 0.9092 - val_loss: 11.8132 - val_accuracy: 0.0022\n",
            "Epoch 19/60\n",
            "3241/3241 [==============================] - 9s 3ms/sample - loss: 0.7493 - accuracy: 0.9116 - val_loss: 12.2142 - val_accuracy: 0.0035\n",
            "Epoch 20/60\n",
            "3241/3241 [==============================] - 9s 3ms/sample - loss: 0.7334 - accuracy: 0.9141 - val_loss: 12.5494 - val_accuracy: 0.0031\n",
            "Epoch 21/60\n",
            "3241/3241 [==============================] - 9s 3ms/sample - loss: 0.7261 - accuracy: 0.9160 - val_loss: 12.8201 - val_accuracy: 0.0067\n",
            "Epoch 22/60\n",
            "3241/3241 [==============================] - 9s 3ms/sample - loss: 0.7172 - accuracy: 0.9171 - val_loss: 13.1221 - val_accuracy: 0.0044\n",
            "Epoch 23/60\n",
            "3241/3241 [==============================] - 9s 3ms/sample - loss: 0.7060 - accuracy: 0.9190 - val_loss: 13.2498 - val_accuracy: 0.0088\n",
            "Epoch 24/60\n",
            "3241/3241 [==============================] - 9s 3ms/sample - loss: 0.6990 - accuracy: 0.9206 - val_loss: 13.4659 - val_accuracy: 0.0153\n",
            "Epoch 25/60\n",
            "3241/3241 [==============================] - 9s 3ms/sample - loss: 0.6893 - accuracy: 0.9221 - val_loss: 13.6924 - val_accuracy: 0.0125\n",
            "Epoch 26/60\n",
            "3241/3241 [==============================] - 9s 3ms/sample - loss: 0.6817 - accuracy: 0.9236 - val_loss: 13.7441 - val_accuracy: 0.0182\n",
            "Epoch 27/60\n",
            "3241/3241 [==============================] - 9s 3ms/sample - loss: 0.6735 - accuracy: 0.9250 - val_loss: 13.6919 - val_accuracy: 0.0186\n",
            "Epoch 28/60\n",
            "3241/3241 [==============================] - 9s 3ms/sample - loss: 0.6660 - accuracy: 0.9267 - val_loss: 13.9505 - val_accuracy: 0.0149\n",
            "Epoch 29/60\n",
            "3241/3241 [==============================] - 9s 3ms/sample - loss: 0.6607 - accuracy: 0.9277 - val_loss: 14.0606 - val_accuracy: 0.0199\n",
            "Epoch 30/60\n",
            "3241/3241 [==============================] - 9s 3ms/sample - loss: 0.6582 - accuracy: 0.9288 - val_loss: 14.1313 - val_accuracy: 0.0204\n",
            "Epoch 31/60\n",
            "3241/3241 [==============================] - 9s 3ms/sample - loss: 0.6525 - accuracy: 0.9299 - val_loss: 14.4286 - val_accuracy: 0.0198\n",
            "Epoch 32/60\n",
            "3241/3241 [==============================] - 9s 3ms/sample - loss: 0.6440 - accuracy: 0.9315 - val_loss: 13.7612 - val_accuracy: 0.0201\n",
            "Epoch 33/60\n",
            "3241/3241 [==============================] - 9s 3ms/sample - loss: 0.6382 - accuracy: 0.9322 - val_loss: 13.8231 - val_accuracy: 0.0191\n",
            "Epoch 34/60\n",
            "3241/3241 [==============================] - 9s 3ms/sample - loss: 0.6318 - accuracy: 0.9335 - val_loss: 13.7766 - val_accuracy: 0.0204\n",
            "Epoch 35/60\n",
            "3241/3241 [==============================] - 9s 3ms/sample - loss: 0.6264 - accuracy: 0.9347 - val_loss: 13.5736 - val_accuracy: 0.0200\n",
            "Epoch 36/60\n",
            "3241/3241 [==============================] - 9s 3ms/sample - loss: 0.6240 - accuracy: 0.9349 - val_loss: 13.4347 - val_accuracy: 0.0196\n",
            "Epoch 37/60\n",
            "3241/3241 [==============================] - 9s 3ms/sample - loss: 0.6211 - accuracy: 0.9358 - val_loss: 12.9251 - val_accuracy: 0.0217\n",
            "Epoch 38/60\n",
            "3241/3241 [==============================] - 9s 3ms/sample - loss: 0.6176 - accuracy: 0.9364 - val_loss: 12.7211 - val_accuracy: 0.0201\n",
            "Epoch 39/60\n",
            "3241/3241 [==============================] - 9s 3ms/sample - loss: 0.6148 - accuracy: 0.9370 - val_loss: 12.3790 - val_accuracy: 0.0205\n",
            "Epoch 40/60\n",
            "3241/3241 [==============================] - 9s 3ms/sample - loss: 0.6101 - accuracy: 0.9378 - val_loss: 11.9251 - val_accuracy: 0.0203\n",
            "Epoch 41/60\n",
            "3241/3241 [==============================] - 9s 3ms/sample - loss: 0.6078 - accuracy: 0.9379 - val_loss: 11.7599 - val_accuracy: 0.0206\n",
            "Epoch 42/60\n",
            "3241/3241 [==============================] - 9s 3ms/sample - loss: 0.6056 - accuracy: 0.9387 - val_loss: 11.0029 - val_accuracy: 0.0189\n",
            "Epoch 43/60\n",
            "3241/3241 [==============================] - 9s 3ms/sample - loss: 0.6020 - accuracy: 0.9394 - val_loss: 10.9247 - val_accuracy: 0.0213\n",
            "Epoch 44/60\n",
            "3241/3241 [==============================] - 9s 3ms/sample - loss: 0.5989 - accuracy: 0.9400 - val_loss: 10.2439 - val_accuracy: 0.0208\n",
            "Epoch 45/60\n",
            "3241/3241 [==============================] - 9s 3ms/sample - loss: 0.5972 - accuracy: 0.9406 - val_loss: 9.4921 - val_accuracy: 0.0232\n",
            "Epoch 46/60\n",
            "3241/3241 [==============================] - 9s 3ms/sample - loss: 0.5963 - accuracy: 0.9411 - val_loss: 9.8227 - val_accuracy: 0.0195\n",
            "Epoch 47/60\n",
            "3241/3241 [==============================] - 9s 3ms/sample - loss: 0.5929 - accuracy: 0.9414 - val_loss: 8.9261 - val_accuracy: 0.0281\n",
            "Epoch 48/60\n",
            "3241/3241 [==============================] - 9s 3ms/sample - loss: 0.5918 - accuracy: 0.9414 - val_loss: 8.2997 - val_accuracy: 0.0510\n",
            "Epoch 49/60\n",
            "3241/3241 [==============================] - 9s 3ms/sample - loss: 0.5906 - accuracy: 0.9421 - val_loss: 7.6495 - val_accuracy: 0.7422\n",
            "Epoch 50/60\n",
            "3241/3241 [==============================] - 9s 3ms/sample - loss: 0.5873 - accuracy: 0.9425 - val_loss: 6.9904 - val_accuracy: 0.7658\n",
            "Epoch 51/60\n",
            "3241/3241 [==============================] - 9s 3ms/sample - loss: 0.5863 - accuracy: 0.9425 - val_loss: 6.6762 - val_accuracy: 0.7697\n",
            "Epoch 52/60\n",
            "3241/3241 [==============================] - 9s 3ms/sample - loss: 0.5841 - accuracy: 0.9430 - val_loss: 6.0160 - val_accuracy: 0.7960\n",
            "Epoch 53/60\n",
            "3241/3241 [==============================] - 9s 3ms/sample - loss: 0.5821 - accuracy: 0.9436 - val_loss: 5.7355 - val_accuracy: 0.7976\n",
            "Epoch 54/60\n",
            "3241/3241 [==============================] - 9s 3ms/sample - loss: 0.5813 - accuracy: 0.9438 - val_loss: 4.0150 - val_accuracy: 0.8051\n",
            "Epoch 55/60\n",
            "3241/3241 [==============================] - 9s 3ms/sample - loss: 0.5797 - accuracy: 0.9441 - val_loss: 4.1584 - val_accuracy: 0.8083\n",
            "Epoch 56/60\n",
            "3241/3241 [==============================] - 9s 3ms/sample - loss: 0.5764 - accuracy: 0.9447 - val_loss: 3.4637 - val_accuracy: 0.8082\n",
            "Epoch 57/60\n",
            "3241/3241 [==============================] - 8s 3ms/sample - loss: 0.5757 - accuracy: 0.9447 - val_loss: 3.1871 - val_accuracy: 0.8176\n",
            "Epoch 58/60\n",
            "3241/3241 [==============================] - 8s 3ms/sample - loss: 0.5744 - accuracy: 0.9453 - val_loss: 3.0794 - val_accuracy: 0.8159\n",
            "Epoch 59/60\n",
            "3241/3241 [==============================] - 9s 3ms/sample - loss: 0.5741 - accuracy: 0.9453 - val_loss: 2.6152 - val_accuracy: 0.8128\n",
            "Epoch 60/60\n",
            "3241/3241 [==============================] - 8s 3ms/sample - loss: 0.5734 - accuracy: 0.9457 - val_loss: 2.6849 - val_accuracy: 0.8204\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f2bd0126ac8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u1Y2tPv5Rxev",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "outputId": "d16ca536-b179-40c1-e1d7-c3314e25dd1d"
      },
      "source": [
        "# Print prediction(s)\n",
        "sentense_id = 2\n",
        "x_sample = dataset['x'][sentense_id]\n",
        "y_sample = dataset['y'][sentense_id]\n",
        "print(\"Source('en') example:\", id_to_text( x_sample, dataset['x_tk'] ))\n",
        "print(\"Source('ru') example:\", id_to_text( y_sample, dataset['y_tk'] ))\n",
        "prediction = embed_rnn_model.predict(x_sample[None, :], verbose=1).squeeze()\n",
        "print(\"Translation(en_ru) example:\", id_to_text( logits_to_id(prediction), dataset['y_tk'] ))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Source('en') example: since january 2 0 0 9 new office premises in the street v [rare] 3 prague 1 have been opened these premises are situated in a close [rare] of all our partner banks\n",
            "Source('ru') example: с января месяца 2 0 0 9 г открыты новые офисные помещения на улице v kolkovně 3 в колковне д 3 прага 1 данные помещения находятся поблизости всех банков партнеров\n",
            "\r1/1 [==============================] - 0s 13ms/sample\n",
            "Translation(en_ru) example: с января месяца 0 0 0 9 г открыты новые офисные помещения на улице v kolkovně 3 в колковне д 3 прага 1 данные помещения находятся поблизости всех банков партнеров вам проекта нужно\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ORh1vXotA9t8",
        "colab_type": "text"
      },
      "source": [
        "#### Model 2 - BiDirectional GRU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZTraW66N41zP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        },
        "outputId": "2a21fff9-2abf-4059-9b4b-fb376041fb31"
      },
      "source": [
        "def bd_gru_model(input_shape, output_sequence_length, source_vocab_size, target_vocab_size):\n",
        "    \"\"\"\n",
        "    Build and train a RNN model using word embedding on x and y\n",
        "    :param input_shape: Tuple of input shape\n",
        "    :param output_sequence_length: Length of output sequence\n",
        "    :param english_vocab_size: Number of unique English words in the dataset\n",
        "    :param french_vocab_size: Number of unique French words in the dataset\n",
        "    :return: Keras model built, but not trained\n",
        "    \"\"\"\n",
        "    input_seq = keras.Input(input_shape[1:])\n",
        "    if output_sequence_length>input_shape[1]:\n",
        "        expanded_seq = keras.backend.squeeze(\n",
        "            keras.layers.ZeroPadding1D((0, output_sequence_length-input_shape[1]))(\n",
        "                keras.layers.Reshape((input_shape[1], 1))(input_seq)\n",
        "            ),\n",
        "            axis = -1\n",
        "        )\n",
        "    else:\n",
        "        expanded_seq = input_seq\n",
        "    embedded_seq = keras.layers.TimeDistributed(keras.layers.BatchNormalization(axis=-1))(\n",
        "        keras.layers.Embedding(source_vocab_size, embeddings_units, input_length=output_sequence_length)(expanded_seq)\n",
        "    )\n",
        "    rnn = keras.layers.TimeDistributed(keras.layers.BatchNormalization(axis=-1))(\n",
        "        keras.layers.Bidirectional(keras.layers.GRU(int(gru_units/2), return_sequences=True))(embedded_seq)\n",
        "    )\n",
        "    logits = keras.layers.TimeDistributed(keras.layers.BatchNormalization(axis=-1))(\n",
        "        keras.layers.TimeDistributed(keras.layers.Dense(4*gru_units, activation='elu'))(rnn)\n",
        "    )\n",
        "    probabilities = keras.layers.TimeDistributed(keras.layers.Dense(target_vocab_size, activation='softmax'))(logits)\n",
        "    \n",
        "    model = keras.Model(input_seq, probabilities)\n",
        "    \n",
        "    model.compile(loss=loss,\n",
        "                  optimizer=keras.optimizers.Adam(learning_rate, clipnorm=3.0),\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "  \n",
        "# Train the neural network\n",
        "bd_rnn_model = embedded_gru_model(\n",
        "    dataset['x'].shape,\n",
        "    dataset['y'].shape[1],\n",
        "    dataset['x'].max()+1,\n",
        "    dataset['y'].max()+1)\n",
        "\n",
        "\n",
        "\n",
        "print(\"Model summary:\")\n",
        "bd_rnn_model.summary()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model summary:\n",
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         [(None, 65)]              0         \n",
            "_________________________________________________________________\n",
            "reshape_1 (Reshape)          (None, 65, 1)             0         \n",
            "_________________________________________________________________\n",
            "zero_padding1d_1 (ZeroPaddin (None, 99, 1)             0         \n",
            "_________________________________________________________________\n",
            "Squeeze_1 (TensorFlowOpLayer [(None, 99)]              0         \n",
            "_________________________________________________________________\n",
            "embedding_1 (Embedding)      (None, 99, 32)            96000     \n",
            "_________________________________________________________________\n",
            "time_distributed_5 (TimeDist (None, 99, 32)            128       \n",
            "_________________________________________________________________\n",
            "unified_gru_1 (UnifiedGRU)   (None, 99, 256)           222720    \n",
            "_________________________________________________________________\n",
            "time_distributed_6 (TimeDist (None, 99, 256)           1024      \n",
            "_________________________________________________________________\n",
            "time_distributed_8 (TimeDist (None, 99, 1024)          263168    \n",
            "_________________________________________________________________\n",
            "time_distributed_7 (TimeDist (None, 99, 1024)          4096      \n",
            "_________________________________________________________________\n",
            "time_distributed_9 (TimeDist (None, 99, 9000)          9225000   \n",
            "=================================================================\n",
            "Total params: 9,812,136\n",
            "Trainable params: 9,809,512\n",
            "Non-trainable params: 2,624\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hFKX2UJHBmvh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2091
        },
        "outputId": "9c1b2043-f784-4214-b8ec-515e69896ffd"
      },
      "source": [
        "bd_rnn_model.fit(\n",
        "    dataset['x'], \n",
        "    dataset['y'][:,:, None], \n",
        "    batch_size=batch_size, \n",
        "    epochs=epochs, \n",
        "    validation_split=validation_split\n",
        ")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 3241 samples, validate on 361 samples\n",
            "Epoch 1/60\n",
            "3241/3241 [==============================] - 9s 3ms/sample - loss: 4.3171 - accuracy: 0.7334 - val_loss: 7.5948 - val_accuracy: 0.8467\n",
            "Epoch 2/60\n",
            "3241/3241 [==============================] - 8s 3ms/sample - loss: 1.6781 - accuracy: 0.8361 - val_loss: 6.5531 - val_accuracy: 0.8467\n",
            "Epoch 3/60\n",
            "3241/3241 [==============================] - 8s 3ms/sample - loss: 1.5285 - accuracy: 0.8479 - val_loss: 6.2935 - val_accuracy: 0.8467\n",
            "Epoch 4/60\n",
            "3241/3241 [==============================] - 8s 3ms/sample - loss: 1.4378 - accuracy: 0.8517 - val_loss: 6.5556 - val_accuracy: 0.8467\n",
            "Epoch 5/60\n",
            "3241/3241 [==============================] - 8s 3ms/sample - loss: 1.3417 - accuracy: 0.8558 - val_loss: 6.8463 - val_accuracy: 0.8467\n",
            "Epoch 6/60\n",
            "3241/3241 [==============================] - 8s 3ms/sample - loss: 1.2515 - accuracy: 0.8601 - val_loss: 7.1699 - val_accuracy: 0.8467\n",
            "Epoch 7/60\n",
            "3241/3241 [==============================] - 8s 3ms/sample - loss: 1.1667 - accuracy: 0.8653 - val_loss: 7.3828 - val_accuracy: 0.8441\n",
            "Epoch 8/60\n",
            "3241/3241 [==============================] - 8s 3ms/sample - loss: 1.0922 - accuracy: 0.8707 - val_loss: 7.5842 - val_accuracy: 0.8258\n",
            "Epoch 9/60\n",
            "3241/3241 [==============================] - 8s 3ms/sample - loss: 1.0325 - accuracy: 0.8761 - val_loss: 7.8224 - val_accuracy: 0.8172\n",
            "Epoch 10/60\n",
            "3241/3241 [==============================] - 8s 3ms/sample - loss: 0.9911 - accuracy: 0.8814 - val_loss: 7.9476 - val_accuracy: 0.8165\n",
            "Epoch 11/60\n",
            "3241/3241 [==============================] - 8s 3ms/sample - loss: 0.9385 - accuracy: 0.8868 - val_loss: 8.1287 - val_accuracy: 0.8094\n",
            "Epoch 12/60\n",
            "3241/3241 [==============================] - 8s 3ms/sample - loss: 0.9042 - accuracy: 0.8915 - val_loss: 8.5453 - val_accuracy: 0.0000e+00\n",
            "Epoch 13/60\n",
            "3241/3241 [==============================] - 8s 3ms/sample - loss: 0.8751 - accuracy: 0.8954 - val_loss: 8.7789 - val_accuracy: 0.0000e+00\n",
            "Epoch 14/60\n",
            "3241/3241 [==============================] - 8s 3ms/sample - loss: 0.8434 - accuracy: 0.8992 - val_loss: 9.3183 - val_accuracy: 0.0000e+00\n",
            "Epoch 15/60\n",
            "3241/3241 [==============================] - 8s 3ms/sample - loss: 0.8189 - accuracy: 0.9028 - val_loss: 9.8455 - val_accuracy: 0.0000e+00\n",
            "Epoch 16/60\n",
            "3241/3241 [==============================] - 9s 3ms/sample - loss: 0.7965 - accuracy: 0.9057 - val_loss: 10.4441 - val_accuracy: 2.7981e-05\n",
            "Epoch 17/60\n",
            "3241/3241 [==============================] - 9s 3ms/sample - loss: 0.7789 - accuracy: 0.9081 - val_loss: 11.0071 - val_accuracy: 4.4769e-04\n",
            "Epoch 18/60\n",
            "3241/3241 [==============================] - 9s 3ms/sample - loss: 0.7638 - accuracy: 0.9102 - val_loss: 11.5197 - val_accuracy: 0.0019\n",
            "Epoch 19/60\n",
            "3241/3241 [==============================] - 9s 3ms/sample - loss: 0.7532 - accuracy: 0.9119 - val_loss: 11.6893 - val_accuracy: 0.0024\n",
            "Epoch 20/60\n",
            "3241/3241 [==============================] - 9s 3ms/sample - loss: 0.7385 - accuracy: 0.9141 - val_loss: 12.2231 - val_accuracy: 0.0043\n",
            "Epoch 21/60\n",
            "3241/3241 [==============================] - 9s 3ms/sample - loss: 0.7287 - accuracy: 0.9154 - val_loss: 12.4844 - val_accuracy: 0.0119\n",
            "Epoch 22/60\n",
            "3241/3241 [==============================] - 9s 3ms/sample - loss: 0.7148 - accuracy: 0.9175 - val_loss: 12.7875 - val_accuracy: 0.0094\n",
            "Epoch 23/60\n",
            "3241/3241 [==============================] - 9s 3ms/sample - loss: 0.7024 - accuracy: 0.9199 - val_loss: 13.0514 - val_accuracy: 0.0087\n",
            "Epoch 24/60\n",
            "3241/3241 [==============================] - 9s 3ms/sample - loss: 0.6924 - accuracy: 0.9215 - val_loss: 13.4084 - val_accuracy: 0.0079\n",
            "Epoch 25/60\n",
            "3241/3241 [==============================] - 9s 3ms/sample - loss: 0.6834 - accuracy: 0.9234 - val_loss: 13.5736 - val_accuracy: 0.0121\n",
            "Epoch 26/60\n",
            "3241/3241 [==============================] - 9s 3ms/sample - loss: 0.6767 - accuracy: 0.9247 - val_loss: 13.7570 - val_accuracy: 0.0133\n",
            "Epoch 27/60\n",
            "3241/3241 [==============================] - 9s 3ms/sample - loss: 0.6682 - accuracy: 0.9261 - val_loss: 14.0252 - val_accuracy: 0.0154\n",
            "Epoch 28/60\n",
            "3241/3241 [==============================] - 9s 3ms/sample - loss: 0.6672 - accuracy: 0.9269 - val_loss: 14.1007 - val_accuracy: 0.0161\n",
            "Epoch 29/60\n",
            "3241/3241 [==============================] - 9s 3ms/sample - loss: 0.6569 - accuracy: 0.9285 - val_loss: 14.2517 - val_accuracy: 0.0144\n",
            "Epoch 30/60\n",
            "3241/3241 [==============================] - 9s 3ms/sample - loss: 0.6492 - accuracy: 0.9298 - val_loss: 14.2998 - val_accuracy: 0.0184\n",
            "Epoch 31/60\n",
            "3241/3241 [==============================] - 9s 3ms/sample - loss: 0.6432 - accuracy: 0.9313 - val_loss: 14.1550 - val_accuracy: 0.0180\n",
            "Epoch 32/60\n",
            "3241/3241 [==============================] - 9s 3ms/sample - loss: 0.6410 - accuracy: 0.9317 - val_loss: 14.1401 - val_accuracy: 0.0177\n",
            "Epoch 33/60\n",
            "3241/3241 [==============================] - 9s 3ms/sample - loss: 0.6341 - accuracy: 0.9329 - val_loss: 13.9135 - val_accuracy: 0.0162\n",
            "Epoch 34/60\n",
            "3241/3241 [==============================] - 9s 3ms/sample - loss: 0.6317 - accuracy: 0.9337 - val_loss: 13.9507 - val_accuracy: 0.0163\n",
            "Epoch 35/60\n",
            "3241/3241 [==============================] - 9s 3ms/sample - loss: 0.6284 - accuracy: 0.9344 - val_loss: 14.0875 - val_accuracy: 0.0190\n",
            "Epoch 36/60\n",
            "3241/3241 [==============================] - 9s 3ms/sample - loss: 0.6270 - accuracy: 0.9352 - val_loss: 13.4347 - val_accuracy: 0.0203\n",
            "Epoch 37/60\n",
            "3241/3241 [==============================] - 9s 3ms/sample - loss: 0.6209 - accuracy: 0.9357 - val_loss: 13.3602 - val_accuracy: 0.0205\n",
            "Epoch 38/60\n",
            "3241/3241 [==============================] - 9s 3ms/sample - loss: 0.6154 - accuracy: 0.9369 - val_loss: 12.9615 - val_accuracy: 0.0192\n",
            "Epoch 39/60\n",
            "3241/3241 [==============================] - 9s 3ms/sample - loss: 0.6178 - accuracy: 0.9370 - val_loss: 12.4036 - val_accuracy: 0.0209\n",
            "Epoch 40/60\n",
            "3241/3241 [==============================] - 9s 3ms/sample - loss: 0.6135 - accuracy: 0.9377 - val_loss: 12.2128 - val_accuracy: 0.0187\n",
            "Epoch 41/60\n",
            "3241/3241 [==============================] - 9s 3ms/sample - loss: 0.6079 - accuracy: 0.9382 - val_loss: 11.6454 - val_accuracy: 0.0198\n",
            "Epoch 42/60\n",
            "3241/3241 [==============================] - 9s 3ms/sample - loss: 0.6044 - accuracy: 0.9390 - val_loss: 11.5011 - val_accuracy: 0.0192\n",
            "Epoch 43/60\n",
            "3241/3241 [==============================] - 9s 3ms/sample - loss: 0.5996 - accuracy: 0.9398 - val_loss: 10.8007 - val_accuracy: 0.0199\n",
            "Epoch 44/60\n",
            "3241/3241 [==============================] - 9s 3ms/sample - loss: 0.5979 - accuracy: 0.9401 - val_loss: 10.3894 - val_accuracy: 0.0193\n",
            "Epoch 45/60\n",
            "3241/3241 [==============================] - 9s 3ms/sample - loss: 0.5945 - accuracy: 0.9410 - val_loss: 9.8376 - val_accuracy: 0.0185\n",
            "Epoch 46/60\n",
            "3241/3241 [==============================] - 9s 3ms/sample - loss: 0.5918 - accuracy: 0.9413 - val_loss: 9.3663 - val_accuracy: 0.0220\n",
            "Epoch 47/60\n",
            "3241/3241 [==============================] - 9s 3ms/sample - loss: 0.5900 - accuracy: 0.9421 - val_loss: 9.2755 - val_accuracy: 0.0200\n",
            "Epoch 48/60\n",
            "3241/3241 [==============================] - 9s 3ms/sample - loss: 0.5885 - accuracy: 0.9422 - val_loss: 8.9536 - val_accuracy: 0.0232\n",
            "Epoch 49/60\n",
            "3241/3241 [==============================] - 9s 3ms/sample - loss: 0.5864 - accuracy: 0.9425 - val_loss: 8.3491 - val_accuracy: 0.0229\n",
            "Epoch 50/60\n",
            "3241/3241 [==============================] - 9s 3ms/sample - loss: 0.5834 - accuracy: 0.9432 - val_loss: 7.0513 - val_accuracy: 0.7501\n",
            "Epoch 51/60\n",
            "3241/3241 [==============================] - 9s 3ms/sample - loss: 0.5813 - accuracy: 0.9434 - val_loss: 7.2384 - val_accuracy: 0.7521\n",
            "Epoch 52/60\n",
            "3241/3241 [==============================] - 9s 3ms/sample - loss: 0.5795 - accuracy: 0.9441 - val_loss: 5.7859 - val_accuracy: 0.7858\n",
            "Epoch 53/60\n",
            "3241/3241 [==============================] - 9s 3ms/sample - loss: 0.5783 - accuracy: 0.9441 - val_loss: 5.3978 - val_accuracy: 0.8060\n",
            "Epoch 54/60\n",
            "3241/3241 [==============================] - 9s 3ms/sample - loss: 0.5766 - accuracy: 0.9447 - val_loss: 5.4581 - val_accuracy: 0.8006\n",
            "Epoch 55/60\n",
            "3241/3241 [==============================] - 9s 3ms/sample - loss: 0.5736 - accuracy: 0.9452 - val_loss: 4.0479 - val_accuracy: 0.8121\n",
            "Epoch 56/60\n",
            "3241/3241 [==============================] - 9s 3ms/sample - loss: 0.5740 - accuracy: 0.9455 - val_loss: 4.7967 - val_accuracy: 0.7972\n",
            "Epoch 57/60\n",
            "3241/3241 [==============================] - 9s 3ms/sample - loss: 0.5733 - accuracy: 0.9453 - val_loss: 4.0537 - val_accuracy: 0.8194\n",
            "Epoch 58/60\n",
            "3241/3241 [==============================] - 9s 3ms/sample - loss: 0.5732 - accuracy: 0.9455 - val_loss: 3.0055 - val_accuracy: 0.8134\n",
            "Epoch 59/60\n",
            "3241/3241 [==============================] - 9s 3ms/sample - loss: 0.5701 - accuracy: 0.9459 - val_loss: 3.3069 - val_accuracy: 0.8145\n",
            "Epoch 60/60\n",
            "3241/3241 [==============================] - 9s 3ms/sample - loss: 0.5686 - accuracy: 0.9461 - val_loss: 2.5681 - val_accuracy: 0.8116\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f2be45f84a8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7OZpP5XBB6z4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "outputId": "5f2c3404-aec1-4347-f65c-f5e0554c3dc4"
      },
      "source": [
        "# Print prediction(s)\n",
        "sentense_id = 2\n",
        "x_sample = dataset['x'][sentense_id]\n",
        "y_sample = dataset['y'][sentense_id]\n",
        "print(\"Source('en') example:\", id_to_text( x_sample, dataset['x_tk'] ))\n",
        "print(\"Source('ru') example:\", id_to_text( y_sample, dataset['y_tk'] ))\n",
        "prediction = bd_rnn_model.predict(x_sample[None, :], verbose=1).squeeze()\n",
        "print(\"Translation(en_ru) example:\", id_to_text( logits_to_id(prediction), dataset['y_tk'] ))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Source('en') example: since january 2 0 0 9 new office premises in the street v [rare] 3 prague 1 have been opened these premises are situated in a close [rare] of all our partner banks\n",
            "Source('ru') example: с января месяца 2 0 0 9 г открыты новые офисные помещения на улице v kolkovně 3 в колковне д 3 прага 1 данные помещения находятся поблизости всех банков партнеров\n",
            "\r1/1 [==============================] - 0s 8ms/sample\n",
            "Translation(en_ru) example: в января месяца 2 0 0 9 г открыты новые офисные помещения на улице v kolkovně 3 в колковне д 3 прага 1 данные помещения находятся поблизости всех банков партнеров [rare] проекта был\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}