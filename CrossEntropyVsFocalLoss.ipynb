{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MachineTranslationModels.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ap-nlp-research/language_translation_en_ru_tf2/blob/master/CrossEntropyVsFocalLoss.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "17OD4xTkRxeL"
      },
      "source": [
        "# CrossEntropy vs Focal Loss\n",
        "\n",
        "\n",
        "The goal of the project is to compare the performance of two loss functions:\n",
        "1. Traditional Cross Entropy\n",
        "2. Focal Loss\n",
        "\n",
        "Cross Entropy evaluates entropy between classes, which could relatevely low and difficult to train in cases where the number of classes is large. In order to counteract this problem Facebook AI Research suggest to use [Focal Loss](https://arxiv.org/pdf/1708.02002.pdf) that in theory should focus the training on hard examples and therefore speed up the traing.\n",
        "\n",
        "The models implemented in Tensorflow 2.0 with Keras as a high-level API. Models are trained and analyzed based on EN-RU [wmt19_translate dataset](https://www.tensorflow.org/datasets/datasets#wmt19_translate) ([ACL 2019 FOURTH CONFERENCE ON MACHINE TRANSLATION (WMT19)](http://www.statmt.org/wmt19/translation-task.html))."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "j3gfCUyDRxeN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "outputId": "71aad76c-c36f-456a-c53e-9b1358220d7f"
      },
      "source": [
        "!pip install tensorflow-gpu==2.0.0-alpha0\n",
        "!git clone https://github.com/ap-nlp-research/language_translation_en_ru_tf2.git translation_en_ru"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow-gpu==2.0.0-alpha0 in /usr/local/lib/python3.6/dist-packages (2.0.0a0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.16.3)\n",
            "Requirement already satisfied: tb-nightly<1.14.0a20190302,>=1.14.0a20190301 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.14.0a20190301)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.15.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.0.7)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (0.7.1)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (3.7.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.12.0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (0.2.2)\n",
            "Requirement already satisfied: tf-estimator-nightly<1.14.0.dev2019030116,>=1.14.0.dev2019030115 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.14.0.dev2019030115)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.1.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.0.9)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (0.7.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (0.1.6)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (0.33.4)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190302,>=1.14.0a20190301->tensorflow-gpu==2.0.0-alpha0) (3.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190302,>=1.14.0a20190301->tensorflow-gpu==2.0.0-alpha0) (0.15.2)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow-gpu==2.0.0-alpha0) (2.8.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow-gpu==2.0.0-alpha0) (41.0.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ecoqx4K1RxeT",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import pickle as pk\n",
        "import subprocess\n",
        "import re\n",
        "import numpy as np\n",
        "from functools import partial\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import backend as K"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "myYPpU6bRxeX"
      },
      "source": [
        "## Data ETL\n",
        "\n",
        "The data load, extraction, and transformation was done beforehand with [create_dataset_en_ru.py](./create_dataset_en_ru.py) script. This script stores a dictionary containing source data under 'x' label. Target data is stored under 'y' label. In addition to the source and target data, the dictionary contains x and y tockenizers (stored as 'x_tk' and 'y_tk'):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "s6-C7QMFRxea"
      },
      "source": [
        "dataset: dict\n",
        "\n",
        "{\n",
        "    'x': np.ndarray,\n",
        "    'y': np.ndarray,\n",
        "    'x_tk': keras.preprocessing.text.Tokenizer,\n",
        "    'y_tk': keras.preprocessing.text.Tokenizer\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hc1RXHOURxeb",
        "colab": {}
      },
      "source": [
        "with open(\"./translation_en_ru/data/wlm_en_ru.pkl\", 'rb') as file:\n",
        "    dataset = pk.load(file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tj5lJy93Rxeg"
      },
      "source": [
        "## Utility Functions\n",
        "\n",
        "In addition to the data ETL, the code below provides two additional functions for converting logits into word indicies and converting word indicies into text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tjrUMAOPRxeh",
        "colab": {}
      },
      "source": [
        "def logits_to_id(logits):\n",
        "    \"\"\"\n",
        "    Turns logits into word ids\n",
        "    :param logits: Logits from a neural network\n",
        "    \"\"\"\n",
        "    return np.argmax(logits, 1)\n",
        "\n",
        "def id_to_text(idx, tokenizer):\n",
        "    \"\"\"\n",
        "    Turns id into text using the tokenizer\n",
        "    :param idx: word id\n",
        "    :param tokenizer: Keras Tokenizer fit on the labels\n",
        "    :return: String that represents the text of the logits\n",
        "    \"\"\"\n",
        "    index_to_words = {id: word for word, id in tokenizer.word_index.items()}\n",
        "    index_to_words[0] = '<PAD>'\n",
        "\n",
        "    return ' '.join([index_to_words[prediction] for prediction in idx]).replace(\" <PAD>\", \"\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VE45jscXRxek",
        "outputId": "19958320-6e99-4d1a-fb98-52b6bda806d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        }
      },
      "source": [
        "print(\"Here is an example for a sample number 1:\")\n",
        "print(\"Source('en') example:\", id_to_text(dataset['x'][0], dataset['x_tk']))\n",
        "print(\"Target('ru') example:\", id_to_text(dataset['y'][0], dataset['y_tk']))\n",
        "print(\" \")\n",
        "print(\"A sample number 2:\")\n",
        "print(\"Source('en') example:\", id_to_text(dataset['x'][-10], dataset['x_tk']))\n",
        "print(\"Target('ru') example:\", id_to_text(dataset['y'][-10], dataset['y_tk']))\n",
        "print(\"source vocabulary size:\", dataset['x'].max())\n",
        "print(\"target vocabulary size:\", dataset['y'].max())\n",
        "print(\"Source shape:\", dataset['x'].shape)\n",
        "print(\"Target shape:\", dataset['y'].shape)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Here is an example for a sample number 1:\n",
            "Source('en') example: the company has been registered with the municipal court in prague in section b file 1 4 8 5 7\n",
            "Target('ru') example: фирма зарегистрирована в городском суде в г праге раздел б [rare] 1 4 8 5 7\n",
            " \n",
            "A sample number 2:\n",
            "Source('en') example: six years ago i had a surgery and l 4 l 5 [rare] were [rare] now l 5 s 1 [rare] [rare] and i had a second surgery that went well\n",
            "Target('ru') example: шесть лет назад мне сделали операцию и на дисках l 4 l 5 сейчас [rare] [rare] диски l 5 s 1 и было необходимо второе хирургическое вмешательство которое произошло вчера и прошло хорошо\n",
            "source vocabulary size: 3499\n",
            "target vocabulary size: 14999\n",
            "Source shape: (14751, 148)\n",
            "Target shape: (14751, 148)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "g2b8qgl5Rxen"
      },
      "source": [
        "## Models\n",
        "\n",
        "The models are implemented with a similar set of parameters. The main idea is to keep models as small and simple as possible to quickly train them and validate the difference derived from the loss function. The summary of main hyper parameters presented below:\n",
        "\n",
        "* Mapping:\n",
        "    - Embeddings - word indices will be mapped into a 16-dimentional space\n",
        "    - Dense mapping - recurrence outputs mapped into the target-language space, represented with OHE, via Dense mapping\n",
        "* Layers:\n",
        "    - GRU - number of units 128\n",
        "    - Batch Normalization - To speed up the training batch normalization is inserted after embeddings and before dense mapping\n",
        "* Optimization:\n",
        "    - Adam - all models trained with Adam optimizer and the same learning rate (1e-3)\n",
        "* Loss function:\n",
        "    - sparse_categorical_crossentropy_from_logits - keras.losses.sparse_categorical_crossentropy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6ZKvPDCFRxen",
        "colab": {}
      },
      "source": [
        "learning_rate = 1e-3\n",
        "embeddings_units = 16\n",
        "gru_units = 64\n",
        "epochs = 3\n",
        "validation_split = 0.1\n",
        "batch_size = 64\n",
        "loss = keras.losses.sparse_categorical_crossentropy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ORh1vXotA9t8"
      },
      "source": [
        "#### Model 2 - Embedded GRU - Focal Loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ouKODleG0KxE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def focal_loss(gamma=2., alpha=.25):\n",
        "\n",
        "    def focal_loss_fixed(y_true, y_pred):\n",
        "        y_pred = tf.clip_by_value(y_pred, K.epsilon(), 1-K.epsilon())\n",
        "        vocab_size = y_pred.shape[-1]\n",
        "        length = tf.shape(y_pred)[1]\n",
        "        zeros = tf.zeros((tf.shape(y_pred)[0], vocab_size), dtype=y_pred.dtype)\n",
        "        ones = tf.ones_like(zeros)\n",
        "\n",
        "        indicies = tf.range(0, length, dtype=tf.int32)\n",
        "\n",
        "        def fn(idx):\n",
        "            el_ids = tf.reshape(tf.cast(y_true[:, idx, :], tf.int32), (-1,))\n",
        "            pred = y_pred[:, idx, :]\n",
        "            truth = K.one_hot(el_ids, num_classes=vocab_size)\n",
        "            truth = K.equal(truth, 1)\n",
        "            pt_1 = tf.where(truth, pred, ones)\n",
        "            pt_0 = tf.where(tf.logical_not(truth), pred, zeros)\n",
        "            return -alpha * K.sum(K.pow(1. - pt_1, gamma) * K.log(pt_1), axis=-1) \\\n",
        "                   - (1-alpha) * K.sum(K.pow(pt_0, gamma) * K.log(1. - pt_0), axis=-1)\n",
        "\n",
        "        return tf.map_fn(fn, indicies, dtype=tf.float32)\n",
        "\n",
        "    return focal_loss_fixed"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZTraW66N41zP",
        "outputId": "6ee03f7c-a009-4000-850f-7fad73dfd8c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "def FocalLoss_model(input_shape, output_sequence_length, source_vocab_size, target_vocab_size):\n",
        "    \"\"\"\n",
        "    Build and train a RNN model using word embedding on x and y\n",
        "    :param input_shape: Tuple of input shape\n",
        "    :param output_sequence_length: Length of output sequence\n",
        "    :param english_vocab_size: Number of unique English words in the dataset\n",
        "    :param french_vocab_size: Number of unique French words in the dataset\n",
        "    :return: Keras model built, but not trained\n",
        "    \"\"\"\n",
        "    input_seq = keras.Input(input_shape[1:])\n",
        "    if output_sequence_length>input_shape[1]:\n",
        "        expanded_seq = keras.backend.squeeze(\n",
        "            keras.layers.ZeroPadding1D((0, output_sequence_length-input_shape[1]))(\n",
        "                keras.layers.Reshape((input_shape[1], 1))(input_seq)\n",
        "            ),\n",
        "            axis = -1\n",
        "        )\n",
        "    else:\n",
        "        expanded_seq = input_seq\n",
        "        \n",
        "        \n",
        "    embedded_seq = keras.layers.TimeDistributed(keras.layers.BatchNormalization(axis=-1))(\n",
        "        keras.layers.Embedding(source_vocab_size, embeddings_units, input_length=output_sequence_length)(expanded_seq)\n",
        "    )\n",
        "    rnn = keras.layers.TimeDistributed(keras.layers.BatchNormalization(axis=-1))(\n",
        "        keras.layers.GRU(gru_units, return_sequences=True)(embedded_seq)\n",
        "    )\n",
        "    probabilities = keras.layers.TimeDistributed(keras.layers.Dense(target_vocab_size, activation='softmax'))(rnn)\n",
        "    \n",
        "    model = keras.Model(input_seq, probabilities)\n",
        "    \n",
        "    model.compile(loss=focal_loss(alpha=.25, gamma=2),\n",
        "                  optimizer=keras.optimizers.Adam(learning_rate, clipnorm=3.0),\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "  \n",
        "# Train the neural network\n",
        "keras.backend.clear_session()\n",
        "model = FocalLoss_model(\n",
        "    dataset['x'].shape,\n",
        "    dataset['y'].shape[1],\n",
        "    dataset['x'].max()+1,\n",
        "    dataset['y'].max()+1)\n",
        "\n",
        "\n",
        "\n",
        "print(\"Model summary:\")\n",
        "model.summary()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model summary:\n",
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 148)]             0         \n",
            "_________________________________________________________________\n",
            "embedding (Embedding)        (None, 148, 16)           56000     \n",
            "_________________________________________________________________\n",
            "time_distributed (TimeDistri (None, 148, 16)           64        \n",
            "_________________________________________________________________\n",
            "unified_gru (UnifiedGRU)     (None, 148, 64)           15744     \n",
            "_________________________________________________________________\n",
            "time_distributed_1 (TimeDist (None, 148, 64)           256       \n",
            "_________________________________________________________________\n",
            "time_distributed_2 (TimeDist (None, 148, 15000)        975000    \n",
            "=================================================================\n",
            "Total params: 1,047,064\n",
            "Trainable params: 1,046,904\n",
            "Non-trainable params: 160\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hFKX2UJHBmvh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "c3c5cc75-e1d7-4c76-b419-1dca0923b5bc"
      },
      "source": [
        "model.fit(\n",
        "    dataset['x'], \n",
        "    dataset['y'][:,:, None], \n",
        "    batch_size=batch_size, \n",
        "    epochs=epochs, \n",
        "    validation_split=validation_split\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 13275 samples, validate on 1476 samples\n",
            "Epoch 1/3\n",
            " 1920/13275 [===>..........................] - ETA: 5:49 - loss: 2.3035 - accuracy: 0.7513"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7OZpP5XBB6z4",
        "colab": {}
      },
      "source": [
        "# Print prediction(s)\n",
        "sentense_id = 2\n",
        "x_sample = dataset['x'][sentense_id]\n",
        "y_sample = dataset['y'][sentense_id]\n",
        "print(\"Source('en') example:\", id_to_text( x_sample, dataset['x_tk'] ))\n",
        "print(\"Source('ru') example:\", id_to_text( y_sample, dataset['y_tk'] ))\n",
        "prediction = model.predict(x_sample[None, :], verbose=1).squeeze()\n",
        "print(\"Translation(en_ru) example:\", id_to_text( logits_to_id(prediction), dataset['y_tk'] ))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1d7e-xH1Rxeq"
      },
      "source": [
        "**Model list:**\n",
        "\n",
        "1. Embedded GRU - CrossEntropy\n",
        "1. Embedded GRU - Focal Loss\n",
        "\n",
        "#### Model 1 - Embedded GRU - CrossEntropy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "l4QhEHNaRxer",
        "colab": {}
      },
      "source": [
        "def xEntropy_model(input_shape, output_sequence_length, source_vocab_size, target_vocab_size):\n",
        "    \"\"\"\n",
        "    Build and train a RNN model using word embedding on x and y\n",
        "    :param input_shape: Tuple of input shape\n",
        "    :param output_sequence_length: Length of output sequence\n",
        "    :param english_vocab_size: Number of unique English words in the dataset\n",
        "    :param french_vocab_size: Number of unique French words in the dataset\n",
        "    :return: Keras model built, but not trained\n",
        "    \"\"\"\n",
        "    input_seq = keras.Input(input_shape[1:])\n",
        "    if output_sequence_length>input_shape[1]:\n",
        "        expanded_seq = keras.backend.squeeze(\n",
        "            keras.layers.ZeroPadding1D((0, output_sequence_length-input_shape[1]))(\n",
        "                keras.layers.Reshape((input_shape[1], 1))(input_seq)\n",
        "            ),\n",
        "            axis = -1\n",
        "        )\n",
        "    else:\n",
        "        expanded_seq = input_seq\n",
        "        \n",
        "        \n",
        "    embedded_seq = keras.layers.TimeDistributed(keras.layers.BatchNormalization(axis=-1))(\n",
        "        keras.layers.Embedding(source_vocab_size, embeddings_units, input_length=output_sequence_length)(expanded_seq)\n",
        "    )\n",
        "    rnn = keras.layers.TimeDistributed(keras.layers.BatchNormalization(axis=-1))(\n",
        "        keras.layers.GRU(gru_units, return_sequences=True)(embedded_seq)\n",
        "    )\n",
        "    probabilities = keras.layers.TimeDistributed(keras.layers.Dense(target_vocab_size, activation='softmax'))(rnn)\n",
        "    \n",
        "    model = keras.Model(input_seq, probabilities)\n",
        "    \n",
        "    model.compile(loss=loss,\n",
        "                  optimizer=keras.optimizers.Adam(learning_rate, clipnorm=3.0),\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "  \n",
        "# Train the neural network\n",
        "keras.backend.clear_session()\n",
        "model = xEntropy_model(\n",
        "    dataset['x'].shape,\n",
        "    dataset['y'].shape[1],\n",
        "    dataset['x'].max()+1,\n",
        "    dataset['y'].max()+1)\n",
        "\n",
        "\n",
        "\n",
        "print(\"Model summary:\")\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "48rVQw-ch2ek",
        "colab": {}
      },
      "source": [
        "model.fit(\n",
        "    dataset['x'], \n",
        "    dataset['y'][:,:, None], \n",
        "    batch_size=batch_size, \n",
        "    epochs=epochs, \n",
        "    validation_split=validation_split)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "u1Y2tPv5Rxev",
        "colab": {}
      },
      "source": [
        "# Print prediction(s)\n",
        "sentense_id = 2\n",
        "x_sample = dataset['x'][sentense_id]\n",
        "y_sample = dataset['y'][sentense_id]\n",
        "print(\"Source('en') example:\", id_to_text( x_sample, dataset['x_tk'] ))\n",
        "print(\"Source('ru') example:\", id_to_text( y_sample, dataset['y_tk'] ))\n",
        "prediction = model.predict(x_sample[None, :], verbose=1).squeeze()\n",
        "print(\"Translation(en_ru) example:\", id_to_text( logits_to_id(prediction), dataset['y_tk'] ))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}